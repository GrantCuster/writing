<!DOCTYPE html><html><head><link rel="preload" href="/static/fonts/Inter-Regular.woff2?v=3.5" as="font" type="font/woff2" crossorigin="*"/><link rel="preload" href="/static/fonts/Inter-Italic.woff2?v=3.5" as="font" type="font/woff2" crossorigin="*"/><link href="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.css" rel="stylesheet"/><meta name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1" class="next-head"/><meta charSet="utf-8" class="next-head"/><style class="next-head">.js-no-flash { display: none }</style><noscript class="jsx-4059783939 jsx-3344216300 next-head"><style>.js-no-flash { display: block }</style></noscript><link rel="icon" type="image/x-icon" href="static/images/favicon.png" class="jsx-1135431938 jsx-2959859206 next-head"/><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css" integrity="sha384-yFRtMMDnQtDRO8rLpMIKrtPCD5jdktao2TV19YiZYWMDkUR5GQZR/NOVTdquEx1j" crossorigin="anonymous" class="jsx-1135431938 jsx-2959859206 next-head"/><title class="jsx-1135431938 jsx-2959859206 next-head">Interpretability in conversation with Patrick Hall and Sameer Singh - Cloudera Fast Forward</title><link rel="preload" href="/_next/static/Ybc3fb3eGTaHFGC_lgzB6/pages/posts/2017-09-11-interpretability-webinar.js" as="script"/><link rel="preload" href="/_next/static/Ybc3fb3eGTaHFGC_lgzB6/pages/_app.js" as="script"/><link rel="preload" href="/_next/static/runtime/webpack-fdce77a122c11e06ae50.js" as="script"/><link rel="preload" href="/_next/static/chunks/commons.f77b50de979bfbbb280b.js" as="script"/><link rel="preload" href="/_next/static/runtime/main-5ab107747766f1b4e0bf.js" as="script"/><style id="__jsx-4059783939">@font-face{font-family:'Inter';font-style:normal;font-weight:400;src:url('/static/fonts/Inter-Regular.woff2?v=3.5') format('woff2'), url('/static/fonts/Inter-Regular.woff?v=3.5') format('woff');}@font-face{font-family:'Inter';font-style:italic;font-weight:400;src:url('/static/fonts/Inter-Italic.woff2?v=3.5') format('woff2'), url('/static/fonts/Inter-Italic.woff?v=3.5') format('woff');}@font-face{font-family:'Inter';font-style:normal;font-weight:700;src:url('/static/fonts/Inter-Bold.woff2?v=3.5') format('woff2'), url('/static/fonts/Inter-Bold.woff?v=3.5') format('woff');}@font-face{font-family:'Inter';font-style:italic;font-weight:700;src:url('/static/fonts/Inter-BoldItalic.woff2?v=3.5') format('woff2'), url('/static/fonts/Inter-BoldItalic.woff?v=3.5') format('woff');}*{box-sizing:border-box;}html{font-family:'Inter',serif;font-size:18px;line-height:27px;text-rendering:optimizelegibility;font-feature-settings:'kern';font-kerning:normal;font-feature-settings:'ss02' 1;-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale;}pre{-webkit-font-smoothing:auto;-moz-osx-font-smoothing:auto;overflow-x:auto;font-family:SFMono-Regular,Consolas,Liberation Mono,Menlo, Monaco,Courier New,monospace;}body{margin:0;overflow-x:hidden;}a{color:inherit;-webkit-text-decoration:none;text-decoration:none;-webkit-transition:opacity 0.025s linear;transition:opacity 0.025s linear;}a:hover{opacity:0.75;}a.no-hover:hover{opacity:1;}.hover_box_overlay{opacity:0;-webkit-transition:opacity 0.025s linear;transition:opacity 0.025s linear;}.hover_box:hover .hover_box_overlay{opacity:1;}a.gray-backer{-webkit-transition:background 0.05s linear;transition:background 0.05s linear;}a.gray-backer:hover{background:#f3f3f3;}button:focus{outline:#999 auto 3px;}</style><style id="__jsx-3344216300">html{font-size:17.189999999999998px;line-height:25.784999999999997px;}a,.display-link{background-image:linear-gradient( to right, black 100%, transparent 0% );background-position:0em calc(1.07em);background-repeat:repeat-x;background-size:1em 0.07em;}a.no-underline{background-image:none;}a.no-hover{background-image:none;}a.no-hover:hover{background-image:none;opacity:1;}</style><style id="__jsx-2959859206">h1,h2,h3,h4,h5,h6{font-weight:400;font-style:normal;margin:0;}h1{font-size:51.56999999999999px;line-height:1.25;}h2{font-size:34.379999999999995px;line-height:1.25;padding-top:25.784999999999997px;margin-bottom:25.784999999999997px;}h3{font-size:25.784999999999997px;line-height:1.25;padding-top:25.784999999999997px;margin-bottom:25.784999999999997px;}h4{font-size:21.487499999999997px;line-height:1.25;padding-top:0px;margin-bottom:25.784999999999997px;}h5{font-size:12.892499999999998px;line-height:1.4375;margin-bottom:12.892499999999998px;padding-bottom:12.892499999999998px;margin-top:-12.892499999999998px;}p{margin:0 0 25.784999999999997px 0;}ol,ul{margin:0 0 25.784999999999997px 0;padding-left:25.784999999999997px;}blockquote{margin:0 0 25.784999999999997px 25.784999999999997px;}.html-video-holder{margin:0 0 25.784999999999997px 0;}video{max-width:100%;}code{background:#eaeaea;padding-right:3px;padding-left:3px;font-size:0.975em;word-break:break-word;}</style><style id="__jsx-1135431938">code[class*='language-'],pre[class*='language-']{color:black;background:none;font-family:Consolas,Monaco,'Andale Mono','Ubuntu Mono', monospace;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;}pre[class*='language-']::-moz-selection,pre[class*='language-']::-moz-selection,code[class*='language-']::-moz-selection,code[class*='language-']::-moz-selection{text-shadow:none;}pre[class*='language-']::selection,pre[class*='language-']::selection,code[class*='language-']::selection,code[class*='language-']::selection{text-shadow:none;}@media print{code[class*='language-'],pre[class*='language-']{text-shadow:none;}}:not(pre)>code[class*='language-']{white-space:normal;}.token.comment,.token.prolog,.token.doctype,.token.cdata{color:slategray;}.token.punctuation{color:#999;}.namespace{opacity:0.7;}.token.property,.token.tag,.token.boolean,.token.number,.token.constant,.token.symbol,.token.deleted{color:#905;}.token.selector,.token.attr-name,.token.string,.token.char,.token.builtin,.token.inserted{color:#690;}.token.operator,.token.entity,.token.url,.language-css .token.string,.style .token.string{color:#9a6e3a;}.token.atrule,.token.attr-value,.token.keyword{color:#07a;}.token.function,.token.class-name{color:#dd4a68;}.token.regex,.token.important,.token.variable{color:#e90;}.token.important,.token.bold{font-weight:bold;}.token.italic{font-style:italic;}.token.entity{cursor:help;}</style></head><body><div id="__next"><div class="jsx-4059783939 jsx-3344216300 js-no-flash"><div style="padding-bottom:12.892499999999998px"><div style="position:relative"><div style="position:relative;padding:6.446249999999999px 12.892499999999998px 6.446249999999999px 12.892499999999998px;background-image:url(&quot;/static/images/dataline.png&quot;)"><div style="display:flex;justify-content:space-between"><div style="display:flex;align-items:center;height:25.784999999999997px;padding-top:1px"><a href="https://www.cloudera.com/products/fast-forward-labs-research.html" style="display:block;line-height:0" class="no-underline no-hover"><img style="height:12.892499999999998px" src="/static/images/cloudera.png"/></a></div><div style="display:flex;align-items:center;padding-top:1px;font-size:17.189999999999998px;line-height:1.5"><a class="no-underline" href="https://www.cloudera.com/products/fast-forward-labs-research.html" style="color:#333e47;font-size:12.892499999999998px;line-height:1.5">About Us â†’</a></div></div><div style="position:absolute;left:0;width:100%;height:2px;transform:scaleY(0.60165);transform-origin:center center;background:rgba(0,0,0,0.125);bottom:-1px"></div></div><div style="padding:12.892499999999998px 12.892499999999998px 12.892499999999998px 12.892499999999998px;display:flex;justify-content:space-between;font-size:17.189999999999998px;line-height:1.5"><div style="display:flex;align-items:center"><a class="no-hover no-underline" style="display:flex;align-items:center" href="/"><img style="height:18.75272727272727px;width:18.75272727272727px;margin-right:9.669374999999999px;position:relative;top:-0.5860227272727272px" src="/static/images/ff.png"/><div>Fast Forward Labs </div></a></div><div style="display:flex;align-items:center;height:25.784999999999997px"></div><div style="display:flex"><div style="margin-right:12.892499999999998px"><a href="/">Blog</a></div><div><a href="https://experiments.fastforwardlabs.com">AI Experiments</a></div></div></div><div style="position:absolute;left:0;width:100%;height:2px;transform:scaleY(0.60165);transform-origin:center center;background:black;bottom:-1px"></div></div></div><div class="jsx-1135431938 jsx-2959859206"><div style="position:relative" class="jsx-1135431938 jsx-2959859206"><div style="padding-left:6.446249999999999px;padding-right:6.446249999999999px;padding-top:25.784999999999997px" class="jsx-1135431938 jsx-2959859206"><div class="jsx-1135431938 jsx-2959859206"><div style="display:flex;margin-bottom:12.892499999999998px;width:587.1075px;margin-left:0" class="jsx-1135431938 jsx-2959859206"><div style="width:293.55375px;padding:0px 12.892499999999998px;position:relative;font-size:12.892499999999998px;letter-spacing:0.03em;text-transform:uppercase;line-height:1" class="jsx-1135431938 jsx-2959859206">Sep 01 2017</div><div style="width:293.55375px;padding:0px 12.892499999999998px;position:relative;font-size:12.892499999999998px;letter-spacing:0.03em;text-transform:uppercase;line-height:1" class="jsx-1135431938 jsx-2959859206">post</div></div><div style="margin-bottom:0" class="jsx-1135431938 jsx-2959859206"><div style="font-size:42.974999999999994px;line-height:1.25;padding:0px 12.892499999999998px;width:587.1075px;margin-left:0" class="jsx-1135431938 jsx-2959859206">Interpretability in conversation with Patrick Hall and Sameer Singh</div></div><div style="display:flex;flex-wrap:wrap;width:587.1075px;margin-left:0" class="jsx-1135431938 jsx-2959859206"><div style="padding:12.892499999999998px 12.892499999999998px 0px 12.892499999999998px;width:587.1075px" class="jsx-1135431938 jsx-2959859206"><div class="jsx-1135431938 jsx-2959859206">by<!-- --> <a href="http://twitter.com/mikepqr" class="jsx-1135431938 jsx-2959859206">Mike</a></div></div><div style="width:587.1075px;padding:25.784999999999997px 12.892499999999998px 25.784999999999997px 12.892499999999998px" class="jsx-1135431938 jsx-2959859206"><div style="position:relative;padding-bottom:56.25%;padding-top:25px;height:0;margin-bottom:1.5em"><iframe style="position:absolute;top:0;left:0;width:100%;height:100%" src="https://www.youtube.com/embed/NxYCY8-Qfx0" frameborder="0"></iframe></div><p>We&#x27;re pleased to share the recording of our recent webinar on machine learning
interpretability and accompanying resources.</p><p>We were joined by guests Patrick Hall (Senior Director for Data Science
Products at H2o.ai, co-author of <a href="https://www.oreilly.com/ideas/ideas-on-interpreting-machine-learning">Ideas on Interpreting Machine
Learning</a>)
and Sameer Singh (Assistant Professor of Computer Science at UC Irvine,
co-creator of <a href="https://github.com/marcotcr/lime">LIME</a>).</p><p>We spoke for an hour and got lots of fantastic questions during that time. We
didn&#x27;t have time to answer them all, so Patrick and Sameer have been kind
enough to <a href="#questions">answer many of them below</a>.</p><p>We&#x27;re also glad to share contact information for all the participants and links
to code and further reading. Please get in touch with any of us if you&#x27;re
interested in working together.</p><h3>Contact</h3><ul><li>Fast Forward Labs, <a href="mailto:contact@fastforwardlabs.com">contact@fastforwardlabs.com</a>,
<a href="https://twitter.com/FastForwardLabs">@fastforwardlabs</a></li><li>Mike Lee Williams, <a href="https://twitter.com/mikepqr">@mikepqr</a></li><li>Patrick Hall, <a href="https://twitter.com/jpatrickhall">@jpatrickhall</a>,
<a href="mailto:phall@h2o.ai">phall@h2o.ai</a></li><li>Sameer Singh, <a href="http://sameersingh.org/">sameersingh.org</a> or
<a href="https://twitter.com/sameer_">@sameer<!-- -->_</a>, <a href="mailto:sameer@uci.edu">sameer@uci.edu</a></li></ul><h3>Code, demos and applications</h3><ul><li><a href="https://github.com/marcotcr/lime">Open source LIME implementation</a></li><li><a href="https://youtu.be/3_gm00kBwEw">Machine Learning Interpretability with H2O.aiâ€™s Driverless
AI</a></li><li><a href="https://github.com/jphall663/GWU_data_mining/blob/master/10_model_interpretability/10_model_interpretability.md">Practical Model
Interpretability</a>
(Patrickâ€™s teaching resources)</li><li><a href="http://blog.fastforwardlabs.com/2017/09/01/LIME-for-couples.html">Why your relationship is likely to last (or not): using
LIME</a> by
Fast Forward Labs</li></ul><h3>Reading</h3><ul><li><a href="https://www.oreilly.com/ideas/ideas-on-interpreting-machine-learning">Ideas on Machine
Learning</a>
by Patrick Hall</li><li><a href="https://arxiv.org/abs/1602.04938">&quot;Why Should I Trust You?&quot;: Explaining the Predictions of Any
Classifier</a> (or <a href="https://www.oreilly.com/learning/introduction-to-local-interpretable-model-agnostic-explanations-lime">Oâ€™Reilly version of
paper</a>)
by Marco Tulio Ribero, Sameer Singh and Carlos Guestrin</li><li><a href="http://people.dbmi.columbia.edu/noemie/papers/15kdd.pdf">Intelligible Models for HealthCare: Predicting Pneumonia Risk and Hospital
30-day Readmission</a>
by Rich Caruana et al. (the source for the pneumonia/asthma story Mike told)</li><li><a href="http://blog.fastforwardlabs.com/2017/08/02/business-interpretability.html">The business case for
interpretability</a>
by Fast Forward Labs</li><li><a href="https://arxiv.org/abs/1606.05386">A Case for Model-Agnostic
Interpretability</a> by Marco Tulio Ribero,
Sameer Singh and Carlos Guestrin</li><li><a href="http://www.fatml.org/">Fairness, Accountability and Transparency in Machine
Learning</a> and <a href="https://fatconference.org/">FAT
conference</a> (NYC, February 2018)</li><li><a href="https://logicmag.io/01-intelligence/">Logic Magazine Issue 1</a>, which
features the interview with an anonymous data scientist</li></ul><h3>Talks</h3><ul><li><a href="https://www.youtube.com/watch?v=LAm4QmVaf0E">Explaining Black-box Machine Learning
Predictions</a>, talk by Sameer on
LIME and related ideas at the Orange County ACM chapter.</li><li><a href="https://conferences.oreilly.com/strata/strata-ny/public/schedule/detail/59747">Interpretable AI: not just for
regulators</a>
a forthcoming talk by SriSatish Ambati and Patrick Hall at Strata NYC, Sep
27 2017</li></ul><h2>Audience questions we didn&#x27;t address during the webinar</h2><p><strong>Is there a standard way to measure model complexity?</strong></p><p>Patrick: Not that I am aware of, but we use and have <a href="https://www.oreilly.com/ideas/ideas-on-interpreting-machine-learning">put forward this simple
heuristic</a>:</p><ul><li>Linear, monotonic models - easily interpretable</li><li>Nonlinear, monotonic models - interpretable</li><li>Nonlinear, non-monotonic models - difficult to interpret</li></ul><p>Mike: one option, when comparing like with like, is simply the number of
parameters. This is a common metric in the deep learning community. It glosses
over some of what we really mean when we say &quot;complex&quot;, but it gets at
something.</p><p>Sameer: Complexity is very subjective, and in different contexts, different
definitions are useful. I also agree that number of parameters are often quite
a useful proxy to complexity. One other metric I like is running time or energy
consumed for each prediction. Of course, there is some theoretical work on this
as well, such as <a href="https://en.wikipedia.org/wiki/VC_dimension">VC
dimensionality</a> or even <a href="https://en.wikipedia.org/wiki/Kolmogorov_complexity">Kolmogorov
complexity</a>. The open
question is which of these measures of complexity correlates with a userâ€™s
capacity to interpret it.</p><p><strong>Is there really a trade-off in call cases between interpretability and accuracy? There are certainly cases where there isn&#x27;t, e.g Rich Caruana&#x27;s pneumonia model. Can you characterize where this trade-off exists and doesn&#x27;t?</strong></p><p>Patrick: I think we are making an assumption that greater accuracy requires
greater complexity -- which it often does for predictive modeling. So, maybe
itâ€™s more accurate to say there is a trade-off between interpretability and
complexity. Humans cannot, in general, understand models with thousands, let
alone millions, of rules or parameters -- this level of complexity is common in
machine learning -- and this level of complexity is often required to model
real-life, nonlinear phenomena. For a linear model, I probably agree with the
questioner that the trade-off may not be as impactful, as long as the number of
parameters in the linear model is relatively small.</p><p>Mike: This may be stating the obvious, but Iâ€™d also add that, in situations
where you can get high enough accuracy for your use case with a model so simple
itâ€™s interpretable by inspection (which does happen!), there is of course no
trade-off. You can have it all!</p><p><strong>Is the black box aspect of machine learning programming only an early AI development issue? Will it eventually be possible to program in &quot;check points&quot; where programmed models will reveal key points or factors that appear within levels of neural network calculations?</strong></p><p>Patrick: I donâ€™t think this is an early AI issue. In my opinion, itâ€™s about the
fundamental complexity of the generated models. Again, the sheer volume of
information is not interpretable to humans -- not even touching on more subtle
complications. I donâ€™t mean big data either -- even though that often doesnâ€™t
help make things any clearer -- I mean that I donâ€™t think anyone can understand
a mathematical formula that requires 500 MB just to store itâ€™s rules and
parameters. (Which is not uncommon in practice.) I do like the idea of training
checkpoints, but what if at the checkpoint, the model says: &quot;these are the
10,000 most important extracted features which represent 100+ degree
combinations of the original model inputs&quot;? So perhaps the combination of
training checkpoints plus constraints on complexity could be very useful.</p><p><strong>Conversations in data science center around the latest/greatest models, not interpretability. Do you have any recommendations for building a company culture that values interpretability.</strong></p><p>Mike: send your colleagues our blog post <a href="http://blog.fastforwardlabs.com/2017/08/02/business-interpretability.html">The Business Case for Machine
Learning
Interpretability</a>!
Interpretability models are profitable, safer and more intellectually
rewarding. Hopefully every one of your colleagues is interested in at least one
of those things.</p><p>Patrick: In my opinion, Iâ€™d also say this is part of customer-focus in an
analytics tool providerâ€™s culture. Itâ€™s usually us data-nerds who want to use
our new toys. I almost never hear a customer say, &quot;give me a model using the
latest and greatest algorithm, oh, and itâ€™s fine if itâ€™s super complex and not
interpretable.&quot;</p><p>Sameer: Partly, it comes from the fact that accuracy provides a single number,
which appeals to the human strive for competition and for sports, and for
engineering things that beat other things. Interpretability is, almost by
definition, much more fuzzier to define and evaluate, making us a little
nervous as empiricists, I think.</p><p><strong>How does interpretability varies across industry, e.g. aviation v media v financial services?</strong></p><p>Patrick: I can only say that the regulations for predictive models are probably
most mature in credit lending <em>in the U.S.</em>, and that I see machine learning
being used more prominently in verticals outside credit lending, i.e.
e-commerce, marketing, anti-fraud, anti-money-laundering.</p><p>Mike: Iâ€™d say that, of the particular three mentioned, the need for
interpretability is most acute in aviation. In fact, it goes beyond
interpretability into <a href="http://composition.al/blog/2017/05/30/proving-that-safety-critical-neural-networks-do-what-theyre-supposed-to-where-we-are-where-were-going-part-1-of-2/">formal verifiability of the properties of an
algorithm</a>,
which is a whole different ball of wax. The acknowledged need is least in
media, because thereâ€™s relatively little regulation. Which is not to say it
wouldnâ€™t be profitable to apply these techniques in this or any other industry
where itâ€™s important to understand your customers. Financial services is
interesting. The need for interpretability is well-understood (and hopefully
well-enforced) there. Thereâ€™s no question, however, that more accurate models
would make more money. People are <a href="https://insight.equifax.com/approve-business-customers/">starting to build neural network-based
trading and lending
models</a> that satisfy
applicable regulations, e.g. <a href="https://www.federalreserve.gov/supervisionreg/srletters/sr1107.htm">SR
11-7</a>, and
the <a href="https://en.wikipedia.org/wiki/Fair_Credit_Reporting_Act">Fair Credit Reporting
Act</a>. There&#x27;s a huge
first-to-market advantage in deploying these more accurate models.</p><p><strong>Model governance and model reviews are standard for financial models as are stress tests. Do you see something similar in the future of industry ML models?</strong></p><p>Patrick: I donâ€™t know why so few machine learning practitioners stress-test
their models. Itâ€™s easy to do with simple sensitivity analysis, and the
financial risks of out-of-range predictions on new data are staggering! I do
also hope that machine learning models that make serious impacts on peopleâ€™s
lives will be better regulated in the future, and the EU is taking steps toward
this with the GDPR. In the meantime, you can keep up with research in this area
at <a href="http://www.fatml.org/">FATML</a>.</p><p>Mike: I also recommend <a href="https://research.google.com/pubs/pub45742.html">Whatâ€™s your ML test score? A rubric for ML production
systems</a>, which mentions a
bunch of really basic stuff that far too few of us do.</p><p><strong>What effect will interpretability have on feature selection?</strong></p><p>Mike: Anecdotally, we spotted a bunch of problems with our model of customer
churn using LIME. In particular, as non-experts in the domain, weâ€™d left in
features that were leaking the target variable. These lit up like Christmas
trees in our web interface thanks to LIME.</p><p>Patrick: I think it will prevent people from using overly-complicated,
difficult to explain features in their models as well. Itâ€™s no good to say
<code>CV_SVD23_Cluster99_INCOME_RANGE</code> is the most important variable in the
prediction if you canâ€™t also say directly what that variable is exactly and how
it was derived.</p><p><strong>I&#x27;m a graduate DS student who just sent some ML research to a group of people in industry who I thought would be interested. In response I got the question &quot;will your research replace my job&quot;. What are some ways to overcome the fear of ML and convince people that AI won&#x27;t replace the creativity in decision making of humans.</strong></p><p>Patrick: Well it might one day -- and we all need to be realistic about that.
But for today, and likely for many years, most of us can rest easy. Today,
machine learning is only good at specific tasks: tasks where there is a large
amount of labeled, easy-to-use &quot;clean&quot; data that has also been labeled.</p><p>Sameer: For now, you can use the explanations almost as a way to show that
machine learning is not a magical black-box. Without an explanation, a natural
reaction is to say &quot;how could it predict this? It must be super-intelligent!&quot;,
but with an explanation and demystifies this, even if it is doing the right
thing for right reasons, the perception of machine learning will not be of an
adversary.</p><p><strong>Why is it that some models are seen as interpretable and others aren&#x27;t? There are large tomes on the theory of linear models, yet they&#x27;re seen as interpretable. Could part of this be due to how long they&#x27;ve been taught?</strong></p><p>Mike: this is a great point. I donâ€™t think itâ€™s simply due to our relative
familiarity with linear models. Itâ€™s that a trained linear model really is
simple to describe (and interpret). Trained neural networks are, in a relative
sense, not even simple to describe. The big linear modeling textbooks are about the long textbooks&#x27; deep domain-specific implications, difficulties like
causality, and the <a href="https://arxiv.org/abs/1008.4686">real numerical/engineering
subtleties</a>.</p><p>Patrick: I 100% agree with the questioner&#x27;s sentiment. Essentially linear model
interpretations are exact and stable, which is great, <em>but</em> the models are
approximate. Machine learning explanations take a different mindset: machine
learning explanations are less stable and exact, but the model itself is
usually much less approximate. So, do you prefer an exact explanation for an
approximate model? Or an approximate explanation for an exact model? In my
opinion, both are useful.</p><p>Sameer: Interpretability is relative. I donâ€™t think we should hold linear
models as the ideal in interpretability, because it is not, especially with
large number of variables. One of the known problems with linear models is
correlated features, i.e. the importance of a feature can get distributed to
correlated features, making features that are less important, but uncorrelated,
have a higher weight. We tried to get around this somewhat in LIME by
restricting the number of features chosen as an explanation (L1 regularization
or Lasso), and normalizing the regression variables over our samples (to reduce
the effect of the bias).</p><p><strong>Once we identify biases, how do we address them?</strong></p><p>Patrick: Problematic features -- such as those correlated to race, gender,
marital status, disability status, etc. -- can be removed from the input data
and the model can be retrained. Or features can be intentionally corrupted to
remove problematic information with techniques like differential privacy during
model training. Another method Iâ€™m personally interested is determining the
local contribution of problematic features using something like LOCO or LIME
and subtracting out the different contributions of problematic features
row-by-row when predictions are made.</p><p><strong>Aren&#x27;t we reducing interpretability to visual analytics of sensitivity?</strong></p><p>Patrick: In some cases yes, but I would argue this is a good thing. In my
opinion, explanations themselves have to be simple. However, Iâ€™m more
interested in fostering the understanding of someone who was just denied parole
or a credit card (both of which are happening today) based on the decision of a
predictive model. For the mass-consumer audience, itâ€™s not an effective
strategy to provide explanations that are just as mathematically complex as the
original model.</p><p><strong>How is LIME different than variable importance, which we get from different algorithms (e.g. RFs?)</strong></p><p>Patrick: The key is locality. LIME essentially provides local variable
importance, meaning that you often get a different variable importance value
for each input variable for each row of the data set. This opens up the
possibility of describing why a machine learning model made the prediction it
made for each customer, patient, transaction, etc. in the data set.</p><p>Sameer: To add to that, I would say the difference between global and local
dependence can sometimes be quite important. Aggregations used to compute
global dependence, like variable importance, can sometimes drown signals. For
example, if race is being used to make a decision for a really small number of
individuals, it might not show up in the global aggregations. Similarly, local
explanations are also useful in showing the sign of the dependence in context,
i.e. age might be important overall, but for some individuals age might act as
a negative factor, and for a positive, and global explanations will not be able
to capture that. That said, itâ€™s much easier to look at only the big picture,
instead of many small pictures.</p><p><strong>Which bootstrapping algorithm is used by LIME generate the perturbed samples</strong></p><p>Sameer: This is often domain dependent, and you can plug in your own. We tried
to stick with pretty simple techniques for each domain, such as removing tokens
in text, patches in images, etc. More details are in the paper/code.</p><p><strong>In the case of adversarial attacks, can LIME detect what causes the deviation from correct detection.</strong></p><p>Sameer: (excerpt from an email thread with Adam) This is quite an interesting
idea, but unfortunately, I believe LIME will get quite stumped in this case,
especially for images, either proposing the whole image as the explanation
(assuming the adversarial noise is spread out, as it often is), or find a &quot;low
confidence&quot; explanation, i.e. it&#x27;ll find the subset of the image that is most
adversarial, but with sufficient uncertainty to say &quot;don&#x27;t take this
explanation too seriously&quot;.</p><p><strong>Can you explain the significance of the clusters in the H2O interpretability interface?</strong></p><p>Patrick: We chose to use clusters in the training data, instead of bootstrapped
or simulated samples around a row of data, to construct local regions on which
to build explanatory linear models. This has two primary advantages:</p><ul><li>We donâ€™t need a new/different sample for every point we want to explain</li><li>It allows us to present the (hopefully helpful) diagnostic plot of the
training data, complex model, and explanatory model that you saw in the
webinar.</li></ul><p>The main drawback is that sometimes clusters are large and the fit of the
explanatory model can degrade in this case. If youâ€™re curious, we choose the
number clusters by maximizing the R-squared between all the linear model
predictions and the complex modelâ€™s predictions.</p><p><strong>LIME makes accurate models more interpretable. Also mentioned was the related idea of making interpretable models more accurate. Which is more promising?</strong></p><p>Patrick: Most research I see is towards making accurate models more
interpretable. One nice practical approach for going the other direction --
making interpretable models more accurate -- are the monotonicity constraints
in XGBoost.</p><p>Sameer: Personally, I like the former, since I do believe an inaccurate model
is not a useful model. I also donâ€™t want to restrict the architecture or the
algorithms that people want to use, nor do I want to constrain them to certain
types of interpretations that an interpretable model provides.</p><h3>Mailing list</h3><p>Our public mailing list is a great way of getting a taste of what Fast Forward
Labs is interested in and working on right now. <a href="https://fastforwardlabs.us8.list-manage.com/subscribe/post?u=bdb368b9a389b010c19dbcd54&amp;id=1d8c97a0bd">We hope you&#x27;ll sign up</a>!</p><div style="display:none;justify-content:center;padding:12.892499999999998px" class="jsx-1135431938 jsx-2959859206"><img style="height:18.75272727272727px;width:18.75272727272727px;position:relative;display:block" src="/static/images/ff.png" class="jsx-1135431938 jsx-2959859206"/></div></div></div></div><div style="position:relative;display:block;width:612.8924999999999px;grid-template-columns:repeat(2, 1fr);margin-left:-12.892499999999998px;margin-right:-12.892499999999998px"><div style="display:grid"><div style="position:relative;display:grid;grid-template-rows:auto 1fr;margin-bottom:12.892499999999998px"><div style="text-transform:uppercase;font-size:12.892499999999998px;letter-spacing:0.03em;line-height:1.5;padding-bottom:6.446249999999999px;padding-left:25.784999999999997px">Newer</div><div style="position:relative;display:grid"><a class="hover-box no-underline no-hover gray-backer hover-extend" style="display:block;position:relative;width:612.8925px;padding-left:12.892499999999998px;padding-right:12.892499999999998px;margin-left:0;margin-right:0" href="/2017/09/26/the-product-possibilities-of-interpretability.html"><div style="display:flex;flex-wrap:wrap"><div style="display:flex;width:587.1075px;font-size:12.892499999999998px;letter-spacing:0.03em;text-transform:uppercase;line-height:1.5;margin-left:0"><div style="width:293.55375px;padding:12.892499999999998px 12.892499999999998px 0px 12.892499999999998px"><div>Sep 05 2017</div></div><div style="width:293.55375px;padding:12.892499999999998px 12.892499999999998px 0px 12.892499999999998px"><div>post</div></div></div><div style="position:relative;width:587.1075px;padding:12.892499999999998px 0px"><div style="font-size:34.379999999999995px;line-height:1.25;padding:0px 12.892499999999998px;margin-top:-6.446249999999999px"><div style="padding-left:0">The Product Possibilities of Interpretability</div></div><div style="padding-top:6.446249999999999px;position:relative"><div style="font-size:15.041249999999998px;line-height:1.5em;display:flex;flex-wrap:wrap;padding-left:0"><div style="padding:0px 12.892499999999998px;width:293.55375px"><div style="height:90.24749999999999px;overflow:hidden;display:-webkit-box;-webkit-line-clamp:4;hyphens:auto;-webkit-box-orient:vertical"><span>by <!-- -->Grant<!-- --> â—¦ </span>
This post is part of a series highlighting the importance of interpretability. Previous posts include a video conversation on interpretability, a guide to using the LIME technique to predict whether couples will stay together, and a look at the business rationale. In our post on...</div><div style="overflow:hidden;width:100%;text-overflow:ellipsis;white-space:nowrap"><span class="display-link"><span>View<!-- --> <span style="text-transform:lowercase">post</span></span></span></div></div><div style="position:relative;width:293.55375px"><div style="width:calc(100% - 25.784999999999997px);height:calc(100% - 12.892499999999998px);margin-left:12.892499999999998px;margin-top:6.446249999999999px;background-image:url(/static/images/2017/09/refractor-local.gif);background-size:contain;background-position:center center;background-repeat:no-repeat"></div></div></div></div></div></div></a><div style="position:absolute;left:-1px;top:-1px;height:calc(158.7301587301587% + 2px);width:calc(158.7301587301587% + 2px);transform:scale(0.6300000000000001);transform-origin:left top;border:solid 2px black;pointer-events:none"></div></div></div></div><div style="display:grid"><div style="position:relative;display:grid;grid-template-rows:auto 1fr;margin-bottom:12.892499999999998px"><div style="text-transform:uppercase;font-size:12.892499999999998px;letter-spacing:0.03em;line-height:1.5;padding-bottom:6.446249999999999px;padding-left:25.784999999999997px">Older</div><div style="position:relative;display:grid"><a class="hover-box no-underline no-hover gray-backer hover-extend" style="display:block;position:relative;width:612.8925px;padding-left:12.892499999999998px;padding-right:12.892499999999998px;margin-left:0;margin-right:0" href="/2017/09/07/to-the-future.html"><div style="display:flex;flex-wrap:wrap"><div style="display:flex;width:587.1075px;font-size:12.892499999999998px;letter-spacing:0.03em;text-transform:uppercase;line-height:1.5;margin-left:0"><div style="width:293.55375px;padding:12.892499999999998px 12.892499999999998px 0px 12.892499999999998px"><div>Sep 05 2017</div></div><div style="width:293.55375px;padding:12.892499999999998px 12.892499999999998px 0px 12.892499999999998px"><div>post</div></div></div><div style="position:relative;width:587.1075px;padding:12.892499999999998px 0px"><div style="font-size:34.379999999999995px;line-height:1.25;padding:0px 12.892499999999998px;margin-top:-6.446249999999999px"><div style="padding-left:0">To the Future...</div></div><div style="padding-top:6.446249999999999px;position:relative"><div style="font-size:15.041249999999998px;line-height:1.5em;display:flex;flex-wrap:wrap;padding-left:0"><div style="padding:0px 12.892499999999998px;width:293.55375px"><div style="height:90.24749999999999px;overflow:hidden;display:-webkit-box;-webkit-line-clamp:4;hyphens:auto;-webkit-box-orient:vertical"><span>by <!-- -->Hilary<!-- --> â—¦ </span>
I started Fast Forward Labs more than three years ago with the vision of creating a new mechanism for applied research, helping businesses large and small understand how recently possible machine learning and applied artificial intelligence technologies could be useful for...</div><div style="overflow:hidden;width:100%;text-overflow:ellipsis;white-space:nowrap"><span class="display-link"><span>View<!-- --> <span style="text-transform:lowercase">post</span></span></span></div></div><div style="position:relative;width:293.55375px"><div style="width:calc(100% - 25.784999999999997px);height:calc(100% - 12.892499999999998px);margin-left:12.892499999999998px;margin-top:6.446249999999999px;background-image:url(/static/images/2017/09/gradient-1504809772766.jpg);background-size:contain;background-position:center center;background-repeat:no-repeat"></div></div></div></div></div></div></a><div style="position:absolute;left:-1px;top:-1px;height:calc(158.7301587301587% + 2px);width:calc(158.7301587301587% + 2px);transform:scale(0.6300000000000001);transform-origin:left top;border:solid 2px black;pointer-events:none"></div></div></div></div></div><div style="padding-bottom:25.784999999999997px;padding-top:25.784999999999997px" class="jsx-1135431938 jsx-2959859206"><div style="font-size:34.379999999999995px;line-height:1.25;padding:0px 12.892499999999998px 0px 12.892499999999998px;margin-bottom:12.892499999999998px;width:587.1075px;margin-left:0" class="jsx-1135431938 jsx-2959859206">About</div><div style="padding:0px 12.892499999999998px;margin-bottom:12.892499999999998px;width:587.1075px;margin-left:0;font-size:17.189999999999998px;line-height:1.5" class="jsx-1135431938 jsx-2959859206">Cloudera Fast Forward Labs is an applied machine learning research group. We help organizations recognize and develop new product and business opportunities through emerging technologies.<!-- --> </div><div style="padding:0px 12.892499999999998px;margin-bottom:12.892499999999998px;width:587.1075px;margin-left:0;font-size:17.189999999999998px;line-height:1.5" class="jsx-1135431938 jsx-2959859206"><a href="https://www.cloudera.com/products/fast-forward-labs-research.html" class="jsx-1135431938 jsx-2959859206">Learn more about working with us.</a></div></div></div><div class="jsx-1135431938 jsx-2959859206"><div style="position:relative" class="jsx-1135431938 jsx-2959859206"><div style="position:absolute;left:0;width:100%;height:2px;transform:scaleY(0.60165);transform-origin:center center;background:black;top:-1px"></div><div style="padding:12.892499999999998px;display:flex;justify-content:space-between;flex-wrap:wrap;font-size:17.189999999999998px;line-height:1.5" class="jsx-1135431938 jsx-2959859206"><div class="jsx-1135431938 jsx-2959859206"><a href="/" class="jsx-1135431938 jsx-2959859206">Blog</a></div><div style="display:flex;flex-wrap:wrap" class="jsx-1135431938 jsx-2959859206"><div style="margin-right:12.892499999999998px" class="jsx-1135431938 jsx-2959859206"><a href="https://www.cloudera.com/products/fast-forward-labs-research.html" class="jsx-1135431938 jsx-2959859206">Cloudera</a></div><div style="margin-right:12.892499999999998px" class="jsx-1135431938 jsx-2959859206"><a href="https://blog.fastforwardlabs.com/" class="jsx-1135431938 jsx-2959859206">AI Experiments</a></div><div class="jsx-1135431938 jsx-2959859206"><a href="https://twitter.com/fastforwardlabs" class="jsx-1135431938 jsx-2959859206">Twitter</a></div></div></div></div></div></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{}},"page":"/posts/2017-09-11-interpretability-webinar","query":{},"buildId":"Ybc3fb3eGTaHFGC_lgzB6","nextExport":true}</script><script async="" id="__NEXT_PAGE__/posts/2017-09-11-interpretability-webinar" src="/_next/static/Ybc3fb3eGTaHFGC_lgzB6/pages/posts/2017-09-11-interpretability-webinar.js"></script><script async="" id="__NEXT_PAGE__/_app" src="/_next/static/Ybc3fb3eGTaHFGC_lgzB6/pages/_app.js"></script><script src="/_next/static/runtime/webpack-fdce77a122c11e06ae50.js" async=""></script><script src="/_next/static/chunks/commons.f77b50de979bfbbb280b.js" async=""></script><script src="/_next/static/runtime/main-5ab107747766f1b4e0bf.js" async=""></script></body></html>