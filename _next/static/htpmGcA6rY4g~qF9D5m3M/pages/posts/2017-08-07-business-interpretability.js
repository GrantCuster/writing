(window.webpackJsonp=window.webpackJsonp||[]).push([["5253"],{"Le+B":function(e,t,n){"use strict";n.r(t),n.d(t,"frontMatter",function(){return r}),n.d(t,"default",function(){return c});var a=n("kOwS"),i=n("qNsG"),o=(n("q1tI"),n("E/Ix")),r={layout:"post",title:"The Business Case for Machine Learning Interpretability",date:"2017-08-02 09:00",preview_image:"/images/2017/08/actions.png",author:"Manny",author_link:"https://twitter.com/mannymoss",feature:!1,published:!0},s={frontMatter:r},l="wrapper";function c(e){var t=e.components,n=Object(i.a)(e,["components"]);return Object(o.b)(l,Object(a.a)({},s,n,{components:t,mdxType:"MDXLayout"}),Object(o.b)("p",null,"Last week we launched the latest prototype and report from our machine\nintelligence R&D team:\n",Object(o.b)("a",Object(a.a)({parentName:"p"},{href:"http://blog.fastforwardlabs.com/2017/08/02/interpretability.html"}),Object(o.b)("em",{parentName:"a"},"Interpretability")),"."),Object(o.b)("p",null,"Our prototype shows how new ideas in interpretability research can be used to\nextract actionable insights from black-box machine learning models; our report\ndescribes breakthroughs in interpretability research and places them in a\ncommercial, legal and ethical context. This research is relevant to anyone who\ndesigns systems using machine learning, from engineers and data scientists to\nbusiness leaders and executives who are considering new product opportunities."),Object(o.b)("p",null,"We will host a ",Object(o.b)("a",Object(a.a)({parentName:"p"},{href:"https://mlinterpretability.splashthat.com/"}),"public webinar on\ninterpretability")," on September 6\n2017, where we'll be joined by guests Patrick Hall (Senior Data Scientist at\nH2O, co-author of ",Object(o.b)("a",Object(a.a)({parentName:"p"},{href:"https://www.oreilly.com/ideas/ideas-on-interpreting-machine-learning"}),"Ideas on Interpreting Machine\nLearning"),")\nand Sameer Singh (Assistant Professor of Computer Science at UC Irvine,\nco-creator of ",Object(o.b)("a",Object(a.a)({parentName:"p"},{href:"https://github.com/marcotcr/lime"}),"LIME"),", a model-agnostic tool\nfor extracting explanations from black box machine learning models). There will\nbe lots of opportunities for the audience to ask questions, so we hope you'll\n",Object(o.b)("a",Object(a.a)({parentName:"p"},{href:"https://mlinterpretability.splashthat.com/"}),"join us"),"!"),Object(o.b)("p",null,"In this post we're going to look at the business case for interpretability.\nWhat advantages are there to building interpretable machine learning systems?"),Object(o.b)("h2",null,"The Power of Interpretability"),Object(o.b)("p",null,"A model you can interpret is one you, regulators, and society can more easily\ntrust to be safe and nondiscriminatory. A model whose decisions can be\nexplained opens up the possibility of new kinds of intelligent products. An\naccurate model that is also interpretable can offer insights that can be used\nto change real-world outcomes for the better. And a model you can interpret and\nunderstand is one you can more easily improve. Let's look at these in more\ndetail."),Object(o.b)("h2",null,"Enhancing Trust"),Object(o.b)("p",null,"If we are to trust a machine learning model to perform accurately, and to be\nsafe and non-discriminatory, it is important that we understand it."),Object(o.b)("p",null,Object(o.b)("img",Object(a.a)({parentName:"p"},{src:"/images/2017/08/asthma.png",alt:"Asthma and pneumonia"}))),Object(o.b)("p",null,"A ",Object(o.b)("a",Object(a.a)({parentName:"p"},{href:"http://people.dbmi.columbia.edu/noemie/papers/15kdd.pdf"}),"paper by Rich Caruana and\ncolleagues")," gives a\npowerful example of the danger of deploying a model you do not understand. They\ndescribe a model that recommended whether patients with pneumonia should be\nadmitted to hospital or treated as outpatients. This model was interpretable,\nand it was immediately obvious by inspection that the model had acquired a\nlethal tendency to view pneumonia patients who also have asthma as low-risk.\nThis was wrong, of course. Asthma patients with a pulmonary infection\nshould be admitted to hospital. In fact they often are, and this was the\nproblem. Such treatment makes their prognosis better, and this pattern was\npresent in the historical training data."),Object(o.b)("p",null,"This problem was only spotted because the model was interpretable. We discuss\nwhat makes a model interpretable in more detail in the report. Similar risks\nlurk in any model applied in the real world."),Object(o.b)("h2",null,"Satisfying Regulations"),Object(o.b)("p",null,"In many industries and jurisdictions, the application of algorithms is\nconstrained by legal regulations. Even when it's not, it should be constrained\nby ethical concerns. "),Object(o.b)("p",null,Object(o.b)("img",Object(a.a)({parentName:"p"},{src:"/images/2017/08/regulations.png",alt:"Asthma and pneumonia"}))),Object(o.b)("p",null,'It is extremely difficult to satisfy regulations or ethical concerns if the\nmodel is uninterpretable. For example, the US Fair Credit Reporting Act\nrequires that agencies disclose "all of the key factors that adversely affected the credit score\nof the consumer in the model used, the total number of which shall not exceed\n4." It\'s difficult to satisfy this regulation if your credit model is a deep\nneural network. Difficult, but as we show in the report, thanks to new\nresearch, not impossible!'),Object(o.b)("h2",null,"Explaining Decisions"),Object(o.b)("p",null,"Some kinds of interpretability allow you to offer automated explanations for\nindividual decisions. A model that makes a prediction is useful, but a model\nthat tells you why the prediction was made is even more powerful. Our prototype\nproduct, Refractor, demonstrates this. It offers explanations for the\nreasons customers are predicted to leave a subscription business. This allows\nthe user to identify both global weaknesses in the product, and individual\ncomplaints. Most excitingly, it raises the possibility of intervening to change\noutcomes."),Object(o.b)("p",null,Object(o.b)("img",Object(a.a)({parentName:"p"},{src:"/images/2017/08/actions.png",alt:"Explaining predictions"}))),Object(o.b)("p",null,"Refractor is built on top of ",Object(o.b)("a",Object(a.a)({parentName:"p"},{href:"https://github.com/marcotcr/lime"}),"LIME"),", a\nmodel-agnostic tool that can be applied to a trained black-box model, so data\nscientists don't need to change the way they build their models to apply this\ntechnique. Our report describes our application in conceptual and technical\nterms, and looks closely at how it could be deployed in a business."),Object(o.b)("h2",null,"Improving the Model"),Object(o.b)("p",null,"Finally, interpretability makes for models that simply work better. Debugging\nan uninterpretable, black-box model is time consuming, and relies at least in\npart on trial-and-error. Debugging an interpretable model, however, is easier\nbecause glaring problems stand out. Having insight into the attributes that are\nlikely causing trouble can motivate a theory about how the model works, which\nsaves time when tracking down problems."),Object(o.b)("h2",null,"How to Access our Reports and Prototypes"),Object(o.b)("p",null,"The future is algorithmic. White-box models and techniques for making black-box\nmodels interpretable offer a safer, more productive, and ultimately more\ncollaborative relationship between humans and intelligent machines. We are just\nat the beginning of the conversation about interpretability and will see the\nimpact over the coming years."),Object(o.b)("p",null,"We offer our research on interpretability in a few ways:"),Object(o.b)("ul",null,Object(o.b)("li",{parentName:"ul"},"Annual research subscription (for individuals and corporate members)"),Object(o.b)("li",{parentName:"ul"},"Subscription and advising (research and time with our team)"),Object(o.b)("li",{parentName:"ul"},"Special projects and workshops (help to build a great data product or\nstrategy)")),Object(o.b)("p",null,"Email us at ",Object(o.b)("a",Object(a.a)({parentName:"p"},{href:"mailto:contact@fastforwardlabs.com"}),"contact@fastforwardlabs.com")," if youâ€™d like to learn more!"))}c.isMDXComponent=!0},alD0:function(e,t,n){(window.__NEXT_P=window.__NEXT_P||[]).push(["/posts/2017-08-07-business-interpretability",function(){var e=n("Le+B");return{page:e.default||e}}])}},[["alD0","5d41","9da1"]]]);