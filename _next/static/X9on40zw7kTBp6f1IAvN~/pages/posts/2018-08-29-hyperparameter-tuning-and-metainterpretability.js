(window.webpackJsonp=window.webpackJsonp||[]).push([["84e0"],{"/DNd":function(e,t,a){(window.__NEXT_P=window.__NEXT_P||[]).push(["/posts/2018-08-29-hyperparameter-tuning-and-metainterpretability",function(){var e=a("rOJB");return{page:e.default||e}}])},rOJB:function(e,t,a){"use strict";a.r(t),a.d(t,"frontMatter",function(){return i}),a.d(t,"default",function(){return m});var r=a("kOwS"),n=a("qNsG"),o=(a("q1tI"),a("E/Ix")),i={title:"Hyperparameter Tuning and Meta-Interpretability: Track All Your Experiments!",date:"Tue Aug 28 2018 20:00:00 GMT-0400 (Eastern Daylight Time)",preview_image:"/images/2018/07/Screen_Shot_2018_07_31_at_11_01_41_AM-1533049344437.png",feature:!1,published:!0,author:"Friederike",author_link:"https://www.linkedin.com/in/friederikeschueuer/",post_type:"newsletter"},s={frontMatter:i},p="wrapper";function m(e){var t=e.components,a=Object(n.a)(e,["components"]);return Object(o.b)(p,Object(r.a)({},s,a,{components:t,mdxType:"MDXLayout"}),Object(o.b)("p",null,"From random forest to neural networks, many modern machine learning algorithms involve a number of parameters that have to be fixed before training the\nalgorithm. These parameters, in contrast to the ones learned by the algorithm\nduring training, are called hyperparameters. The performance of a model on a\ntask given data depends on the specific values of these hyperparameters."),Object(o.b)("p",null,"Hyperparamter tuning is the process of determining the hyperparameter values\nthat maximize model performance on a task given data. The tuning of\nhyperparameters is done by machine learning experts, or increasingly, software\npackages (e.g., ",Object(o.b)("a",Object(r.a)({parentName:"p"},{href:"http://hyperopt.github.io/hyperopt/"}),"HyperOpt"),",\n",Object(o.b)("a",Object(r.a)({parentName:"p"},{href:"https://automl.github.io/auto-sklearn/stable/"}),"auto-sklearn"),",\n",Object(o.b)("a",Object(r.a)({parentName:"p"},{href:"https://github.com/automl/SMAC3"}),"SMAC"),'). The aim of these libraries is to turn\nhyperparameter tuning from a "black art", requiring expert expertise and\nsometimes brute-force search, into a reproducible science - reducing the need for\nexpert knowledge, whilst keeping computational complexity at bay (e.g.,',Object(o.b)("a",Object(r.a)({parentName:"p"},{href:"http://papers.nips.cc/paper/4522-practical-bayesian-optimization"}),"Snoek,\nLarochelle, & Adams;\n2012"),")."),Object(o.b)("p",null,"Traditionally, hyperparameter tuning is done using grid search. Grid search\nrequires that we choose a set of values for each hyperparameter and evaluate\nevery possible combination of hyperparameter values. Grid search suffers from\nthe ",Object(o.b)("em",{parentName:"p"},"curse of dimensionality"),"; the number of joint values grows exponentially\nwith the number of hyperparameters."),Object(o.b)("p",null,"In 2012, ",Object(o.b)("a",Object(r.a)({parentName:"p"},{href:"http://www.jmlr.org/papers/v13/bergstra12a.html"}),"Bergstra and Bengio"),"\nshowed that random hyperparameter search is more efficient than grid search, a\nperhaps counter-intuitive result. This is because only a few hyperparameters\ntend to really matter for the performance of a model on a task given data.\nGrid search tends to spend more time in regions of the hyperparameter space\nthat are low-performing compared to random search. What's more, random search\nallows one to easily add more experiments that explore even more sets of\nhyperparameter values without (expensive) adjustment of the grid (most\nrecently, ",Object(o.b)("a",Object(r.a)({parentName:"p"},{href:"https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf"}),"sequential\napproaches"),"\nhave shown great promise)."),Object(o.b)("p",null,Object(o.b)("img",Object(r.a)({parentName:"p"},{src:"/images/2018/07/Screen_Shot_2018_07_31_at_11_01_41_AM-1533049344437.png",alt:null}))),Object(o.b)("h5",null,"Grid (left) and random (right) search for nine experiments. With random search, all nine trials explore distinct values of the hyperparameters. Random search is more efficient. (Picture taken from Bergstra and Bengio, 2012)"),Object(o.b)("p",null,"If only a few hyperparameter values really matter, for a given model on a task\ngiven data, what are those parameters and what are their values? Current\nsoftware libraries for hyperparameter tuning do not tend to discriminate\nimportant from unimportant hyperparameters and/or do not expose important\nparameters and their values. This limits insights into the workings of a\nmodel - which is important for a variety of reasons, as we explain in depth in\nour report on ",Object(o.b)("a",Object(r.a)({parentName:"p"},{href:"http://blog.fastforwardlabs.com/2017/08/02/interpretability.html"}),"model\ninterpretability"),";\ninterpretability allows us to verify, for example, that a model gives high\nquality predictions for the right, and not the wrong, reasons. "),Object(o.b)("p",null,"A series of recent papers tackles this ",Object(o.b)("em",{parentName:"p"},'"meta-interpretability" problem'),": what\nhyperparameters matter for model performance on a task given data? In\n",Object(o.b)("a",Object(r.a)({parentName:"p"},{href:"https://arxiv.org/abs/1710.04725"}),Object(o.b)("em",{parentName:"a"},"Hyperparameter Importance Across\nDatasets")),", Jan van Rijn and Frank Hutter\nfirst evaluate a model on a task given data and a set of randomly chosen\nhyperparameters to assess model performance. They then use these\nhyperparameters as inputs to a model, a so-called surrogate model, that they\ntrain to predict the oberved performance. Given a trained surrogate model, they\npredict model performance for hyperparameters not previously included in their\nexperiments. Finally, they conduct an analysis of variance (ANOVA) to determine\nhow much of the predicted model performance by the surrogate model can be\nexplained by each hyperparameter or combination of hyperparameters. To draw\nconclusions across data sets, a more generalizable result, the authors repeat\nthis procedure across several data sets. For random forests, for example, they\nfind that only the ",Object(o.b)("inlineCode",{parentName:"p"},"minimum samples per leaf")," and the ",Object(o.b)("inlineCode",{parentName:"p"},"maximum numbers of\nfeatures")," really matter. This finding is consistent with expert knowledge,\nwhich is great: it validates the method; we can use it to study more\ncomplex models for which we have no such intuition yet while it helps beginners\nto get started."),Object(o.b)("p",null,"Using a related approach also based on surrogate models, Philipp Probst, Bernd\nBischl, and Anne-Laure Boulesteix\n",Object(o.b)("a",Object(r.a)({parentName:"p"},{href:"https://arxiv.org/abs/1802.09596"}),"demonstrate"),' that some default values of\nhyperparameters as set by software packages (e.g., scikit-learn) lie\noutside the range of hyperparameter values that tend to yield optimal model\nperformance across tasks and data; we can use solutions to the\nmeta-interpretability problem to define better default values, or to define\nprior distributions for even more efficient random hyperparameter search\n(Bergtra and Bengio sample from a uniform distribution which we can replace by\na "more informed" distribution).'),Object(o.b)("p",null,"Within organizations, these results suggest that one should track and store the\nresults of hyperparameter tuning - not only the set of parameters that result in\nthe best performing model, but ",Object(o.b)("em",{parentName:"p"},"all")," results. These results can be used to\ntrain surrogate models that allow us insight into the importance of\nhyperparameter values and increase the efficiency of hyperparameter tuning by\ndefining sensible default values (or distributions) for the classes of problems\ntackled by data teams at these organizations.\n"))}m.isMDXComponent=!0}},[["/DNd","5d41","9da1"]]]);