(window.webpackJsonp=window.webpackJsonp||[]).push([["5dcf"],{"3Yly":function(e,t,a){"use strict";a.r(t),a.d(t,"frontMatter",function(){return l}),a.d(t,"default",function(){return c});var n=a("kOwS"),i=a("qNsG"),o=(a("q1tI"),a("E/Ix")),l={title:"An Invitation to Active Learning",date:"2019-04-03",preview_image:"/images/editor_uploads/2019-04-04-171937-uncertainty_sampling_observable_fast.gif",feature:!1,published:!0,author_link:"https://twitter.com/_cjwallace",author:"Chris"},r={frontMatter:l},s="wrapper";function c(e){var t=e.components,a=Object(i.a)(e,["components"]);return Object(o.b)(s,Object(n.a)({},r,a,{components:t,mdxType:"MDXLayout"}),Object(o.b)("p",null,"Many interesting learning problems exist in places where labeled data is limited.\nAs such, much thought has been spent on how best to learn from limited labeled data.\nOne obvious answer is simply to collect more data.\nThat is valid, but for some applications, data is difficult or expensive to collect.\nIf we will collect more data, we ought at least be smart about the data we collect.\nThis motivates ",Object(o.b)("em",{parentName:"p"},"active learning"),", which provides strategies for learning in this scenario."),Object(o.b)("p",null,"The ideal setting for active learning is that in which we have a small amount of labeled data with which to build a model and access to a large pool of unlabeled data.\nWe must also have the means to label some of that data, but it's OK for the labeling process to be costly (for instance, a human hand-labeling an image).\nThe active learning process forms a loop:"),Object(o.b)("ol",null,Object(o.b)("li",{parentName:"ol"},"build a model based on the labeled data available"),Object(o.b)("li",{parentName:"ol"},"use the model to predict labels for the unlabeled points"),Object(o.b)("li",{parentName:"ol"},"use an active learning ",Object(o.b)("em",{parentName:"li"},"strategy")," to decide which point to label next"),Object(o.b)("li",{parentName:"ol"},"label that point"),Object(o.b)("li",{parentName:"ol"},"GOTO 1.")),Object(o.b)("p",null,Object(o.b)("img",Object(n.a)({parentName:"p"},{src:"/images/editor_uploads/2019-04-04-171937-uncertainty_sampling_observable_fast.gif",alt:"The active learning loop in action"})),"\n",Object(o.b)("em",{parentName:"p"},"The active learning loop in action - try out the ",Object(o.b)("a",Object(n.a)({parentName:"em"},{href:"https://observablehq.com/@cjwallace/an-invitation-to-active-learning"}),"demo"),"!")),Object(o.b)("p",null,"The essence of active learning is in the ",Object(o.b)("em",{parentName:"p"},"strategy")," we choose in the loop above.\nThree broad families of strategy are:"),Object(o.b)("ul",null,Object(o.b)("li",{parentName:"ul"},Object(o.b)("p",{parentName:"li"},Object(o.b)("em",{parentName:"p"},"Random sampling"),". In the default case, we sample unlabeled data from the pool randomly. This is a passive approach where we don't use the output of the current model to inform the next data point to be labeled. As such, it isn't really active learning.")),Object(o.b)("li",{parentName:"ul"},Object(o.b)("p",{parentName:"li"},Object(o.b)("em",{parentName:"p"},"Uncertainty sampling"),". In uncertainty sampling, we choose the data point about which the algorithm is least certain to label next. This could be the point closest to the decision boundary (the least confident prediction), or it could be the point with highest entropy, or other measure of uncertainty. Choosing points as such helps our learning algorithm refine the decision boundary.")),Object(o.b)("li",{parentName:"ul"},Object(o.b)("p",{parentName:"li"},Object(o.b)("em",{parentName:"p"},"Density sampling"),". Uncertainty sampling works much better than random sampling, but by definition it causes the data points we choose to label to cluster around the decision boundary. This data may be very informative, but not necessarily representative. In density sampling, we try to sample from regions where there are many data points. The trade off between informativeness and representativeness is fundamental to active learning, and there are many approaches that address it."))),Object(o.b)("p",null,"To illustrate the difference between passive and active learning, we created an Observable notebook with a toy problem, which you can explore ",Object(o.b)("a",Object(n.a)({parentName:"p"},{href:"https://observablehq.com/@cjwallace/an-invitation-to-active-learning"}),"here"),".\nIn the notebook, the goal is to find a good separation of red and blue points on a two dimensional chart, and we train a logistic regression model live in the browser to do so.\nOne can see how the decision boundary separating the points evolves as more data is labeled with a random sampling strategy, and also with an uncertainty sampling strategy.\nIn the case of two classes with a linear decision boundary, all the uncertainty sampling strategies (least confidence and highest entropy) give the same result.\nThis is an extremely simplified example, but we think it shows some of the intuition behind active learning."),Object(o.b)("p",null,"We explore active learning in much more detail in our report Learning with Limited Labeled Data, and you can get a high level overview in our ",Object(o.b)("a",Object(n.a)({parentName:"p"},{href:"https://blog.fastforwardlabs.com/2019/04/02/a-guide-to-learning-with-limited-labeled-data.html"}),"previous post"),".\n"))}c.isMDXComponent=!0},sP11:function(e,t,a){(window.__NEXT_P=window.__NEXT_P||[]).push(["/posts/2019-04-03-an-invitation-to-active-learning",function(){var e=a("3Yly");return{page:e.default||e}}])}},[["sP11","5d41","9da1"]]]);