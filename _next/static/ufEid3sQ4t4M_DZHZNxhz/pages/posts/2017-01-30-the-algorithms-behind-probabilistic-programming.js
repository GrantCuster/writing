(window.webpackJsonp=window.webpackJsonp||[]).push([["e2b5"],{"64RF":function(e,t,a){"use strict";a.r(t),a.d(t,"frontMatter",function(){return o}),a.d(t,"default",function(){return c});var n=a("kOwS"),i=a("qNsG"),r=(a("q1tI"),a("E/Ix")),o={layout:"post",title:"The Algorithms Behind Probabilistic Programming",date:"2017-01-30 13:52",preview_image:"http://fastforwardlabs.github.io/report_images/ff05/3-08.png",feature:!1,author:"Mike",author_link:"https://twitter.com/mikepqr"},s={frontMatter:o},l="wrapper";function c(e){var t=e.components,a=Object(i.a)(e,["components"]);return Object(r.b)(l,Object(n.a)({},s,a,{components:t,mdxType:"MDXLayout"}),Object(r.b)("p",null,"We recently ",Object(r.b)("a",Object(n.a)({parentName:"p"},{href:"http://blog.fastforwardlabs.com/2017/01/18/new-research-on-probabilistic-programming.html"}),"introduced our report on probabilistic\nprogramming"),". The accompanying prototype allows you to explore the ",Object(r.b)("a",Object(n.a)({parentName:"p"},{href:"http://fastforwardlabs.github.io/pre/"}),"past and\nfuture of the New York residential real estate\nmarket"),"."),Object(r.b)("p",null,"This post gives a feel for the content in our report by introducing the algorithms and technology that make probabilistic programming possible. We'll dive even deeper into these algorithms in conversation with ",Object(r.b)("a",Object(n.a)({parentName:"p"},{href:"http://stan.fit/"}),"the Stan Group")," Tuesday, February 7 at 1 pm ET/10am PT. Please ",Object(r.b)("a",Object(n.a)({parentName:"p"},{href:"https://www.eventbrite.com/e/introduction-to-probabilistic-programming-tickets-31160610224"}),"join us"),"!"),Object(r.b)("h2",null,"Bayesian Inference"),Object(r.b)("p",null,"Probabilistic programming enables us to construct and fit ",Object(r.b)("a",Object(n.a)({parentName:"p"},{href:"http://www.kdnuggets.com/2016/11/how-bayesian-inference-works.html"}),"probabilistic models")," in code. At its essence, Bayesian inference is a principled way to draw conclusions from incomplete or\nimperfect data, by interpreting data in light of prior knowledge of probabilities. As pretty much all real-world data is incomplete or imperfect in\nsome way, it's an important (and old!) idea."),Object(r.b)("p",null,"Bayesian inference might be the way to go if you:"),Object(r.b)("ul",null,Object(r.b)("li",{parentName:"ul"},"want to make use of institutional knowledge (suspicions, beliefs, logical\ncertainties) about the quantities you want to measure and predict, rather\nthan learn solely from the data"),Object(r.b)("li",{parentName:"ul"},"have several different datasets that you want to learn from"),Object(r.b)("li",{parentName:"ul"},"need to quantify the probability of all possibilities, not just determine\nwhich is most likely"),Object(r.b)("li",{parentName:"ul"},"want to do online learning (i.e., continually update your model as data\narrives)"),Object(r.b)("li",{parentName:"ul"},"want to do active learning (i.e., gather more information until your\npredictions reach some threshold of confidence)"),Object(r.b)("li",{parentName:"ul"},"want to use data to decide if a more complicated model is justified"),Object(r.b)("li",{parentName:"ul"},"need to explain your decisions to customers or regulators"),Object(r.b)("li",{parentName:"ul"},"want to use a single model to answer several questions"),Object(r.b)("li",{parentName:"ul"},"have sparse data with a shared or hierarchical structure")),Object(r.b)("p",null,"Many — perhaps most — analytics and product problems are like this. And the\ncentral idea of Bayesian inference is centuries old. Why, then, do relatively few\ndata analysts, data scientists, and machine learning engineers use the\napproach?"),Object(r.b)("h2",null,"The Algorithmic Building Blocks"),Object(r.b)("p",null,"The problem is that, until recently, the algorithms that make product and business problems\ntractable using Bayesian methods have been difficult to implement and computationally\nexpensive to run. Probabilistic programming systems abstract away many of these difficulties by baking inference algorithms in as building blocks of the language. Morever, these algorithms are robust, so don't require\nproblem-specific hand-tuning."),Object(r.b)("p",null,"One powerful example is sampling from an arbitrary probability distribution, which we need to do often (and efficiently!) when doing inference. The brute force approach, rejection sampling, is problematic because acceptance rates are low: as only a tiny fraction of attempts generate successful samples, the algorithms are slow and inefficient. See ",Object(r.b)("a",Object(n.a)({parentName:"p"},{href:"https://jeremykun.com/2015/04/06/markov-chain-monte-carlo-without-all-the-bullshit/"}),"this post by Jeremey Kun")," for further details."),Object(r.b)("p",null,"Until recently, the main alternative to this naive approach was ",Object(r.b)("a",Object(n.a)({parentName:"p"},{href:"https://jeremykun.com/2015/04/06/markov-chain-monte-carlo-without-all-the-bullshit/"}),"Markov Chain\nMonte Carlo\nsampling"),"\n(of which ",Object(r.b)("a",Object(n.a)({parentName:"p"},{href:"http://michaeljflynn.net/2015/06/01/my-favorite-algorithm-metropolis-hastings/"}),"Metropolis\nHastings"),"\nand Gibbs sampling are well-known examples). If you used Bayesian inference in\nthe 90s or early 2000s, you may remember BUGS (and WinBUGS) or JAGS, which used\nthese methods. These remain popular teaching tools (see e.g. ",Object(r.b)("a",Object(n.a)({parentName:"p"},{href:"https://www.stat.auckland.ac.nz/~brewer/stats331.pdf"}),"STATS331 by\nBrendon Brewer"),", our\nfavorite elementary introduction to Bayesian data analysis). But MCMC samplers\nare often too slow for problems with rich structure or internet-scale data."),Object(r.b)("p",null,"Bayesian inference research in the last 5-10 years has therefore focused on\ntwo new approaches that use clever ideas to make sure the sampler spends more\ntime in regions of high probability, raising efficiency. Those are\n",Object(r.b)("strong",{parentName:"p"},"Hamiltonian Monte Carlo")," and ",Object(r.b)("strong",{parentName:"p"},"Variational inference"),"."),Object(r.b)("h2",null,"Hamiltonian Monte Carlo and the No U-Turn Sampler"),Object(r.b)("p",null,Object(r.b)("a",Object(n.a)({parentName:"p"},{href:"https://arxiv.org/abs/1701.02434"}),"Hamiltonian Monte Carlo")," (HMC) treats the\nprobability distribution as a physical surface. It uses an elegant and\ncomputationally efficient ",Object(r.b)("a",Object(n.a)({parentName:"p"},{href:"https://en.wikipedia.org/wiki/Hamiltonian_mechanics"}),"idea from 19th-century physics")," to explore that\nsurface using calculus, as if under the influence of gravity. This method doesn't work\nfor discrete parameters, but the user doesn't need to differentiate functions by hand."),Object(r.b)("p",null,Object(r.b)("img",Object(n.a)({parentName:"p"},{src:"http://fastforwardlabs.github.io/report_images/ff05/3-06.png",alt:null}))),Object(r.b)("p",null,"To use HMC, you need to tune a sensitive hyperparameter, which makes its\napplication expensive and error-prone. The invention of the ",Object(r.b)("a",Object(n.a)({parentName:"p"},{href:"https://arxiv.org/abs/1111.4246"}),"No U-Turn\nSampler")," (NUTS), a robust algorithm that tunes\nthis parameter automatically, was crucial for making probabilistic\nprogramming useful and practical."),Object(r.b)("h2",null,"Variational Inference and Automatic Differentation"),Object(r.b)("p",null,Object(r.b)("a",Object(n.a)({parentName:"p"},{href:"https://arxiv.org/abs/1601.00670"}),"Variational inference")," (VI) samples from a\ndistribution by building a simple approximation of the distribution. That\napproximation is so simple that it can be sampled from directly, entirely\ncircumventing the need for approximate sampling algorithms like MCMC or HMC."),Object(r.b)("p",null,Object(r.b)("img",Object(n.a)({parentName:"p"},{src:"http://fastforwardlabs.github.io/report_images/ff05/3-08.png",alt:null}))),Object(r.b)("p",null,"To do this, we start with simple distributions that we understand well (e.g.,\nGaussians) and perturb them until they match the real distribution from which\nwe want to sample. The bulk of this work is done by ",Object(r.b)("a",Object(n.a)({parentName:"p"},{href:"http://sebastianruder.com/optimizing-gradient-descent/"}),"stochastic gradient descent (SGD)"),". Given its widespread use in machine learning (e.g., it's used to train deep neural networks), SGD is well\nunderstood and optimized."),Object(r.b)("p",null,"Converting a probabilistic model from a sampling approach to VI used to\nrequire complex math, making it hard for non-experts. ",Object(r.b)("a",Object(n.a)({parentName:"p"},{href:"https://arxiv.org/abs/1603.00788"}),"Automatic\nDifferentiation Variational Inference")," solves\nthis problem by using ",Object(r.b)("strong",{parentName:"p"},"automatic differentation")," (having computers take exact derivatives of arbitrary functions) to differentiate functions\nat the CPU instruction level, allowing a probability distribution to be\nexplored efficiently."),Object(r.b)("h2",null,"Probabilistic Programming Languages"),Object(r.b)("p",null,"ADVI and HMC with NUTS are the two fundamental algorithmic innovations that\nhave made ",Object(r.b)("strong",{parentName:"p"},"probabilistic programming")," possible. Their inclusion in leading\nprobabilistic programming environments (which can be traced back to\n",Object(r.b)("a",Object(n.a)({parentName:"p"},{href:"https://goodmorningeconomics.wordpress.com/2010/11/16/the-promise-of-bayesian-statistics-pt-2/"}),"these"),"\n",Object(r.b)("a",Object(n.a)({parentName:"p"},{href:"http://andrewgelman.com/2010/11/18/derivative-base/"}),"two"),"\nblog posts from 2010) makes inference a one-liner. The end user doesn't need to\ntune parameters or differentiate functions by hand. And because they're built\non Hamiltonian Monte Carlo or variational inference, they're fast."),Object(r.b)("p",null,"But fast and robust algorithms are not all that's required to make Bayesian\ninference a practical proposition. Probabilistic programming languages also\nmake life simpler by providing a concise syntax to define generative\nmodels using a library of built-in probability distributions. Probabilistic\nconcepts are primitive objects defined in the core language. Specifying the\nmodel is therefore a ",Object(r.b)("em",{parentName:"p"},"declarative")," problem for the user, who declares what is\nknown to be true and lets the language figure out how to derive conclusions."),Object(r.b)("p",null,"The most popular probabilistic programming tools are ",Object(r.b)("a",Object(n.a)({parentName:"p"},{href:"http://mc-stan.org/"}),"Stan")," and\n",Object(r.b)("a",Object(n.a)({parentName:"p"},{href:"http://pymc-devs.github.io/pymc3/"}),"PyMC3"),". ",Object(r.b)("a",Object(n.a)({parentName:"p"},{href:"http://edwardlib.org/"}),"Edward")," is a newcomer gaining\na lot of attention. Stan experts ",Object(r.b)("a",Object(n.a)({parentName:"p"},{href:"https://about.me/ericnovik"}),"Eric Novik")," and ",Object(r.b)("a",Object(n.a)({parentName:"p"},{href:"http://syclik.com/"}),"Daniel Lee")," will walk us through how Stan works and what problems they've used it to solve in our ",Object(r.b)("a",Object(n.a)({parentName:"p"},{href:"https://www.eventbrite.com/e/introduction-to-probabilistic-programming-tickets-31160610224"}),"online event February 7"),"."),Object(r.b)("p",null,"For more on PyMC3 see ",Object(r.b)("a",Object(n.a)({parentName:"p"},{href:"http://blog.fastforwardlabs.com/2017/01/11/thomas-wiecki-on-probabilistic-programming-with.html"}),"our interview with Thomas\nWiecki"),"\na PyMC3 core developer (and Director of Data Science at Quantopian). We also\nenjoyed ",Object(r.b)("a",Object(n.a)({parentName:"p"},{href:"http://stronginference.com/pymc3-release.html"}),"Chris Fonnesbeck's blog\npost")," that accompanied the recent\nofficial release of PyMC3."),Object(r.b)("p",null,"In the report we go into more detail on the strengths and weaknesses of these\ntwo languages, and discuss some of the many other options. Whichever language you use, the claim of probabilistic programming - that it\nhides the complexity of Bayesian inference - is more true than ever."),Object(r.b)("p",null,"– ",Object(r.b)("a",Object(n.a)({parentName:"p"},{href:"https://twitter.com/mikepqr"}),"Mike")))}c.isMDXComponent=!0},w41L:function(e,t,a){(window.__NEXT_P=window.__NEXT_P||[]).push(["/posts/2017-01-30-the-algorithms-behind-probabilistic-programming",function(){var e=a("64RF");return{page:e.default||e}}])}},[["w41L","5d41","9da1"]]]);