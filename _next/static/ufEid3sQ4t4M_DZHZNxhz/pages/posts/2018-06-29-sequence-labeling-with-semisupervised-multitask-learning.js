(window.webpackJsonp=window.webpackJsonp||[]).push([["8977"],{"I/Gi":function(e,t,n){"use strict";n.r(t),n.d(t,"frontMatter",function(){return s}),n.d(t,"default",function(){return d});var a=n("kOwS"),o=n("qNsG"),r=(n("q1tI"),n("E/Ix")),s={title:"Sequence labeling with semi-supervised multi-task learning",date:"2018-06-29 12:08 -0400",preview_image:"/images/2018/06/Screen_Shot_2018_06_14_at_4_27_13_PM-1529008110241.png",feature:!1,published:!0,post_type:"Newsletter"},i={frontMatter:s},c="wrapper";function d(e){var t=e.components,n=Object(o.a)(e,["components"]);return Object(r.b)(c,Object(a.a)({},i,n,{components:t,mdxType:"MDXLayout"}),Object(r.b)("p",null,"Sequence labeling tasks attempt to assign a categorical label to each member in\nthe sequence. In natural language processing, where a sequence generally refers\nto a sentence, examples of sequence labeling include named entity\nrecognition (NER), part-of-speech tagging (POS) and error detection. NER, as the\nname implies, tries to recognize names in a sentence and classify them into\npre-defined labels such as ",Object(r.b)("em",{parentName:"p"},"Person")," and ",Object(r.b)("em",{parentName:"p"},"Organization"),". POS tagging assigns labels\nsuch as ",Object(r.b)("em",{parentName:"p"},"noun"),", ",Object(r.b)("em",{parentName:"p"},"verb"),", and ",Object(r.b)("em",{parentName:"p"},"adjective")," to each word, while error detection identifies\ngrammatical errors in sentences. In many of these tasks, the relevant labels in\nthe dataset are very sparse and most of the words contribute very little to the\ntraining process. But why let the data go to waste? "),Object(r.b)("p",null,Object(r.b)("a",Object(a.a)({parentName:"p"},{href:"https://arxiv.org/abs/1704.07156"}),"A recent\npaper"),' proposes using multitask learning to\nmake more use of the available data. In addition to assigning labels to each\ntoken (or words, loosely), the authors propose a model that also predicts the\nsurrounding words in the dataset. By adding the secondary unsupervised\nobjective, "',Object(r.b)("em",{parentName:"p"},"the model is required to learn more general patterns of semantic and\nsyntactic composition, which can be reused in order to predict individual labels\nmore accurately"),'".'),Object(r.b)("p",null,"For the sequence modeling neural network, the authors take one\nsentence as input and use a bidirectional Long Short Term Memory network (LSTM) to assign a label to every token in the\nsentence. Each sentence is first tokenized and the resulting tokens are mapped into a\nsequence of word embeddings before being fed into the LSTM. Two LSTM components,\nmoving in opposite directions (forward and backward) through the sentence, are\nthen used for constructing context-dependent representations for every word. The\nhidden representations from both LSTMs are concatenated in order to obtain a\ncontext-specific representation for each word. This concatenated representation\nis passed through a feed-forward layer, allowing the model to learn features\nbased on both context directions. To predict a label for each token, the authors\nuse either a softmax or conditional random field (CRF) output\narchitecture. Softmax predicts each label independently. CRF, on the other hand,\nhandles dependencies between subsequent labels by looking for the best label\nsequence."),Object(r.b)("p",null,"To predict the surrounding words, the authors cannot use the concatenated\n(forward and backward) representation because it contains information on both\nthe previous word and next word. Instead, they use the pre-concatenated\nversion. The hidden representation from the forward-moving LSTM is used to\npredict the next word; the hidden representation from the backward-moving LSTM\nis used to predict the previous word."),Object(r.b)("p",null,Object(r.b)("img",Object(a.a)({parentName:"p"},{src:"/images/2018/06/Screen_Shot_2018_06_14_at_4_27_13_PM-1529008110241.png",alt:"Architecture of the sequence labeling model with secondary task of predicting surrounding words. The input tokens are shown at the bottom; the expected output labels are at the top."}))),Object(r.b)("p",null,"The architecture was evaluated on a range of datasets, covering the tasks of\nerror detection, named entity recognition, chunking, and POS-tagging. Introducing\na secondary task resulted in consistent performance improvements on every\nbenchmark. The largest benefit was observed on the task of error detection -\nperhaps due to the very sparse and unbalanced label distribution in the dataset.\n"))}d.isMDXComponent=!0},VeG7:function(e,t,n){(window.__NEXT_P=window.__NEXT_P||[]).push(["/posts/2018-06-29-sequence-labeling-with-semisupervised-multitask-learning",function(){var e=n("I/Gi");return{page:e.default||e}}])}},[["VeG7","5d41","9da1"]]]);