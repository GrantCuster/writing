<!DOCTYPE html><html><head><link rel="preload" href="/static/fonts/Inter-Regular.woff2?v=3.5" as="font" type="font/woff2" crossorigin="*"/><link rel="preload" href="/static/fonts/Inter-Italic.woff2?v=3.5" as="font" type="font/woff2" crossorigin="*"/><link href="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.css" rel="stylesheet"/><meta name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1" class="next-head"/><meta charSet="utf-8" class="next-head"/><style class="next-head">.js-no-flash { display: none }</style><noscript class="jsx-4059783939 jsx-3344216300 next-head"><style>.js-no-flash { display: block }</style></noscript><link rel="icon" type="image/x-icon" href="static/images/favicon.png" class="jsx-1135431938 jsx-2959859206 next-head"/><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css" integrity="sha384-yFRtMMDnQtDRO8rLpMIKrtPCD5jdktao2TV19YiZYWMDkUR5GQZR/NOVTdquEx1j" crossorigin="anonymous" class="jsx-1135431938 jsx-2959859206 next-head"/><title class="jsx-1135431938 jsx-2959859206 next-head">Under the Hood of the Variational Autoencoder (in Prose and Code) - Cloudera Fast Forward</title><link rel="preload" href="/_next/static/EnU~FBvqy_55fWO5RkIyH/pages/posts/2016-08-22-under-the-hood-of-the-variational-autoencoder-in.js" as="script"/><link rel="preload" href="/_next/static/EnU~FBvqy_55fWO5RkIyH/pages/_app.js" as="script"/><link rel="preload" href="/_next/static/runtime/webpack-fdce77a122c11e06ae50.js" as="script"/><link rel="preload" href="/_next/static/chunks/commons.f77b50de979bfbbb280b.js" as="script"/><link rel="preload" href="/_next/static/runtime/main-5ab107747766f1b4e0bf.js" as="script"/><style id="__jsx-4059783939">@font-face{font-family:'Inter';font-style:normal;font-weight:400;src:url('/static/fonts/Inter-Regular.woff2?v=3.5') format('woff2'), url('/static/fonts/Inter-Regular.woff?v=3.5') format('woff');}@font-face{font-family:'Inter';font-style:italic;font-weight:400;src:url('/static/fonts/Inter-Italic.woff2?v=3.5') format('woff2'), url('/static/fonts/Inter-Italic.woff?v=3.5') format('woff');}@font-face{font-family:'Inter';font-style:normal;font-weight:700;src:url('/static/fonts/Inter-Bold.woff2?v=3.5') format('woff2'), url('/static/fonts/Inter-Bold.woff?v=3.5') format('woff');}@font-face{font-family:'Inter';font-style:italic;font-weight:700;src:url('/static/fonts/Inter-BoldItalic.woff2?v=3.5') format('woff2'), url('/static/fonts/Inter-BoldItalic.woff?v=3.5') format('woff');}*{box-sizing:border-box;}html{font-family:'Inter',serif;font-size:18px;line-height:27px;text-rendering:optimizelegibility;font-feature-settings:'kern';font-kerning:normal;font-feature-settings:'ss02' 1;-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale;}pre{-webkit-font-smoothing:auto;-moz-osx-font-smoothing:auto;overflow-x:auto;font-family:SFMono-Regular,Consolas,Liberation Mono,Menlo, Monaco,Courier New,monospace;}body{margin:0;overflow-x:hidden;}a{color:inherit;-webkit-text-decoration:none;text-decoration:none;-webkit-transition:opacity 0.025s linear;transition:opacity 0.025s linear;}a:hover{opacity:0.75;}a.no-hover:hover{opacity:1;}.hover_box_overlay{opacity:0;-webkit-transition:opacity 0.025s linear;transition:opacity 0.025s linear;}.hover_box:hover .hover_box_overlay{opacity:1;}a.gray-backer{-webkit-transition:background 0.05s linear;transition:background 0.05s linear;}a.gray-backer:hover{background:#f3f3f3;}button:focus{outline:#999 auto 3px;}</style><style id="__jsx-3344216300">html{font-size:17.189999999999998px;line-height:25.784999999999997px;}a,.display-link{background-image:linear-gradient( to right, black 100%, transparent 0% );background-position:0em calc(1.07em);background-repeat:repeat-x;background-size:1em 0.07em;}a.no-underline{background-image:none;}a.no-hover{background-image:none;}a.no-hover:hover{background-image:none;opacity:1;}</style><style id="__jsx-2959859206">h1,h2,h3,h4,h5,h6{font-weight:400;font-style:normal;margin:0;}h1{font-size:51.56999999999999px;line-height:1.25;}h2{font-size:34.379999999999995px;line-height:1.25;padding-top:25.784999999999997px;margin-bottom:25.784999999999997px;}h3{font-size:25.784999999999997px;line-height:1.25;padding-top:25.784999999999997px;margin-bottom:25.784999999999997px;}h4{font-size:21.487499999999997px;line-height:1.25;padding-top:0px;margin-bottom:25.784999999999997px;}h5{font-size:12.892499999999998px;line-height:1.4375;margin-bottom:12.892499999999998px;padding-bottom:12.892499999999998px;margin-top:-12.892499999999998px;}p{margin:0 0 25.784999999999997px 0;}ol,ul{margin:0 0 25.784999999999997px 0;padding-left:25.784999999999997px;}blockquote{margin:0 0 25.784999999999997px 25.784999999999997px;}.html-video-holder{margin:0 0 25.784999999999997px 0;}video{max-width:100%;}code{background:#eaeaea;padding-right:3px;padding-left:3px;font-size:0.975em;word-break:break-word;}</style><style id="__jsx-1135431938">code[class*='language-'],pre[class*='language-']{color:black;background:none;font-family:Consolas,Monaco,'Andale Mono','Ubuntu Mono', monospace;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;}pre[class*='language-']::-moz-selection,pre[class*='language-']::-moz-selection,code[class*='language-']::-moz-selection,code[class*='language-']::-moz-selection{text-shadow:none;}pre[class*='language-']::selection,pre[class*='language-']::selection,code[class*='language-']::selection,code[class*='language-']::selection{text-shadow:none;}@media print{code[class*='language-'],pre[class*='language-']{text-shadow:none;}}:not(pre)>code[class*='language-']{white-space:normal;}.token.comment,.token.prolog,.token.doctype,.token.cdata{color:slategray;}.token.punctuation{color:#999;}.namespace{opacity:0.7;}.token.property,.token.tag,.token.boolean,.token.number,.token.constant,.token.symbol,.token.deleted{color:#905;}.token.selector,.token.attr-name,.token.string,.token.char,.token.builtin,.token.inserted{color:#690;}.token.operator,.token.entity,.token.url,.language-css .token.string,.style .token.string{color:#9a6e3a;}.token.atrule,.token.attr-value,.token.keyword{color:#07a;}.token.function,.token.class-name{color:#dd4a68;}.token.regex,.token.important,.token.variable{color:#e90;}.token.important,.token.bold{font-weight:bold;}.token.italic{font-style:italic;}.token.entity{cursor:help;}</style></head><body><div id="__next"><div class="jsx-4059783939 jsx-3344216300 js-no-flash"><div style="padding-bottom:12.892499999999998px"><div style="position:relative"><div style="position:relative;padding:6.446249999999999px 12.892499999999998px 6.446249999999999px 12.892499999999998px;background-image:url(&quot;/static/images/dataline.png&quot;)"><div style="display:flex;justify-content:space-between"><div style="display:flex;align-items:center;height:25.784999999999997px;padding-top:1px"><a href="https://www.cloudera.com/products/fast-forward-labs-research.html" style="display:block;line-height:0" class="no-underline no-hover"><img style="height:12.892499999999998px" src="/static/images/cloudera.png"/></a></div><div style="display:flex;align-items:center;padding-top:1px;font-size:17.189999999999998px;line-height:1.5"><a class="no-underline" href="https://www.cloudera.com/products/fast-forward-labs-research.html" style="color:#333e47;font-size:12.892499999999998px;line-height:1.5">About Us →</a></div></div><div style="position:absolute;left:0;width:100%;height:2px;transform:scaleY(0.60165);transform-origin:center center;background:rgba(0,0,0,0.125);bottom:-1px"></div></div><div style="padding:12.892499999999998px 12.892499999999998px 12.892499999999998px 12.892499999999998px;display:flex;justify-content:space-between;font-size:17.189999999999998px;line-height:1.5"><div style="display:flex;align-items:center"><a class="no-hover no-underline" style="display:flex;align-items:center" href="/"><img style="height:18.75272727272727px;width:18.75272727272727px;margin-right:9.669374999999999px;position:relative;top:-0.5860227272727272px" src="/static/images/ff.png"/><div>Fast Forward Labs </div></a></div><div style="display:flex;align-items:center;height:25.784999999999997px"></div><div style="display:flex"><div style="margin-right:12.892499999999998px"><a href="/">Blog</a></div><div><a href="https://experiments.fastforwardlabs.com">AI Experiments</a></div></div></div><div style="position:absolute;left:0;width:100%;height:2px;transform:scaleY(0.60165);transform-origin:center center;background:black;bottom:-1px"></div></div></div><div class="jsx-1135431938 jsx-2959859206"><div style="position:relative" class="jsx-1135431938 jsx-2959859206"><div style="padding-left:6.446249999999999px;padding-right:6.446249999999999px;padding-top:25.784999999999997px" class="jsx-1135431938 jsx-2959859206"><div class="jsx-1135431938 jsx-2959859206"><div style="display:flex;margin-bottom:12.892499999999998px;width:587.1075px;margin-left:0" class="jsx-1135431938 jsx-2959859206"><div style="width:293.55375px;padding:0px 12.892499999999998px;position:relative;font-size:12.892499999999998px;letter-spacing:0.03em;text-transform:uppercase;line-height:1" class="jsx-1135431938 jsx-2959859206">Aug 02 2016</div><div style="width:293.55375px;padding:0px 12.892499999999998px;position:relative;font-size:12.892499999999998px;letter-spacing:0.03em;text-transform:uppercase;line-height:1" class="jsx-1135431938 jsx-2959859206">Whitepaper</div></div><div style="margin-bottom:0" class="jsx-1135431938 jsx-2959859206"><div style="font-size:42.974999999999994px;line-height:1.25;padding:0px 12.892499999999998px;width:587.1075px;margin-left:0" class="jsx-1135431938 jsx-2959859206">Under the Hood of the Variational Autoencoder (in Prose and Code)</div></div><div style="display:flex;flex-wrap:wrap;width:587.1075px;margin-left:0" class="jsx-1135431938 jsx-2959859206"><div style="padding:12.892499999999998px 12.892499999999998px 0px 12.892499999999998px;width:587.1075px" class="jsx-1135431938 jsx-2959859206"><div class="jsx-1135431938 jsx-2959859206">by<!-- --> <a href="https://twitter.com/meereve" class="jsx-1135431938 jsx-2959859206">Miriam</a></div></div><div style="width:587.1075px;padding:25.784999999999997px 12.892499999999998px 25.784999999999997px 12.892499999999998px" class="jsx-1135431938 jsx-2959859206"><h5>The <a href="https://arxiv.org/abs/1312.6114">Variational</a> <a href="https://arxiv.org/abs/1401.4082">Autoencoder</a> (VAE) neatly synthesizes unsupervised deep learning and variational Bayesian methods into one sleek package. In <a href="http://blog.fastforwardlabs.com/2016/08/12/introducing-variational-autoencoders-in-prose-and.html">Part I</a> of this series, we introduced the theory and intuition behind the VAE, an exciting development in machine learning for combined generative modeling and inference—<a href="http://shakirm.com/slides/DLSummerSchool_Aug2016_compress.pdf">&quot;machines that imagine and reason.&quot;</a></h5><p>To recap: VAEs put a probabilistic spin on the basic autoencoder paradigm—treating their inputs, hidden representations, and reconstructed outputs as probabilistic random variables within a directed graphical model. With this <a href="https://xkcd.com/1236/">Bayesian</a> perspective, the encoder becomes a variational <em>inference network</em>, mapping observed inputs to (approximate) posterior distributions over latent space, and the decoder becomes a <em>generative network</em>, capable of mapping arbitrary latent coordinates back to distributions over the original data space.</p><p><div style="position:relative"><img src="http://fastforwardlabs.github.io/blog-images/miriam/imgs_code/vae.4.png" alt="A variational autoencoder" style="display:block;margin:0;width:100%"/></div></p><p>The beauty of this setup is that we can take a principled Bayesian approach toward building systems with a rich internal “mental model” of the observed world, all by training a single, cleverly-designed deep neural network.</p><p>These benefits derive from an enriched understanding of data as merely the tip of the iceberg—the observed result of an underlying causative probabilistic process.</p><p>The power of the resulting model is captured by Feynman’s famous <a href="http://archives-dc.library.caltech.edu/islandora/object/ct1:483">chalkboard quote</a>: “What I cannot create, I do not understand.” When trained on MNIST handwritten digits, our VAE model can parse the information spread thinly over the high-dimensional observed world of pixels, and condense the most meaningful features into a structured distribution over reduced latent dimensions.</p><p>Having recovered the latent manifold and assigned it a coordinate system, it becomes trivial to walk from one point to another along the manifold, creatively generating realistic digits all the while:</p><p><div style="position:relative"><img src="http://fastforwardlabs.github.io/blog-images/miriam/imgs_code/160816_1754_reloaded_latent_784_500_500_50_round_65536_morph_4730816952.gif" alt="Generatively making digits" style="display:block;margin:0;width:100%"/></div></p><p>In this post, we’ll take a look under the hood at the math and technical details that allow us to optimize the VAE model we sketched in <a href="http://fastforwardlabs.github.io/2016/08/12/introducing-variational-autoencoders-in-prose-and.html">Part I</a>.</p><p>Along the way, we’ll show how to implement a VAE in <a href="http://tensorflow.org/">TensorFlow</a>—a library for efficient numerical computation using data flow graphs, with key features like <a href="http://alexey.radul.name/ideas/2013/introduction-to-automatic-differentiation/">automatic differentiation</a> and parallelizability (across clusters, CPUs, GPUs…and <a href="https://cloudplatform.googleblog.com/2016/05/Google-supercharges-machine-learning-tasks-with-custom-chip.html">TPUs</a> if you’re lucky). You can find (and tinker with!) the full implementation <a href="https://github.com/fastforwardlabs/vae-tf/tree/master">here</a>, along with a couple <a href="https://github.com/fastforwardlabs/vae-tf/tree/master/out">pre-trained models</a>.</p><h2>Building the Model</h2><p>Let’s dive into code (Python 3.4), starting with the necessary imports:</p><div><div style="position:relative;margin-bottom:25.784999999999997px;width:602px;margin-left:-20.338749999999997px;margin-right:-20.338749999999997px"><div class="prism-code language-python" style="overflow-x:auto;overflow-y:hidden;background:#f3f3f3"><pre style="float:left;font-size:12.892499999999998px;min-width:100%;position:relative;line-height:1.5;margin:0;padding:12.892499999999998px;overflow:visible"><div class="token-line"><span><span class="token keyword">import</span><span class="token plain"> functools</span></span></div><div class="token-line"><span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain"></span><span class="token keyword">from</span><span class="token plain"> functional </span><span class="token keyword">import</span><span class="token plain"> compose</span><span class="token punctuation">,</span><span class="token plain"> partial</span></span></div><div class="token-line"><span><span class="token plain"></span><span class="token keyword">import</span><span class="token plain"> numpy </span><span class="token keyword">as</span><span class="token plain"> np</span></span></div><div class="token-line"><span><span class="token plain"></span><span class="token keyword">import</span><span class="token plain"> tensorflow </span><span class="token keyword">as</span><span class="token plain"> tf</span></span></div><div class="token-line"><span><span class="token plain"></span></span></div></pre><div style="position:absolute;left:-1px;top:-1px;height:calc(158.7301587301587% + 2px);width:calc(158.7301587301587% + 2px);transform:scale(0.6300000000000001);transform-origin:left top;border:solid 2px black;pointer-events:none"></div></div></div></div><p>One perk of these models is their modularity—VAEs are naturally amenable to swapping in whatever encoder/decoder architecture is most fitting for the task at hand: <a href="https://arxiv.org/abs/1502.04623">recurrent</a> <a href="https://arxiv.org/abs/1511.06349">neural</a> <a href="https://arxiv.org/abs/1412.6581">networks</a>, <a href="https://arxiv.org/abs/1411.5928">convolutional</a> and <a href="https://arxiv.org/abs/1503.03167">deconvolutional</a> networks, etc.</p><p>For our purposes, we will model the relatively simple <a href="http://yann.lecun.com/exdb/mnist/">MNIST</a> dataset using densely-connected layers, wired symmetrically around the hidden code.</p><div><div style="position:relative;margin-bottom:25.784999999999997px;width:602px;margin-left:-20.338749999999997px;margin-right:-20.338749999999997px"><div class="prism-code language-python" style="overflow-x:auto;overflow-y:hidden;background:#f3f3f3"><pre style="float:left;font-size:12.892499999999998px;min-width:100%;position:relative;line-height:1.5;margin:0;padding:12.892499999999998px;overflow:visible"><div class="token-line"><span><span class="token keyword">class</span><span class="token plain"> </span><span class="token class-name">Dense</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">    </span><span class="token triple-quoted-string">&quot;&quot;&quot;Fully-connected layer&quot;&quot;&quot;</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">    </span><span class="token keyword">def</span><span class="token plain"> </span><span class="token function">__init__</span><span class="token punctuation">(</span><span class="token plain">self</span><span class="token punctuation">,</span><span class="token plain"> scope</span><span class="token operator">=</span><span class="token string">&quot;dense_layer&quot;</span><span class="token punctuation">,</span><span class="token plain"> size</span><span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span><span class="token plain"> dropout</span><span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">                 nonlinearity</span><span class="token operator">=</span><span class="token plain">tf</span><span class="token punctuation">.</span><span class="token plain">identity</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">        </span><span class="token comment"># (str, int, (float | tf.Tensor), tf.op)</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">        </span><span class="token keyword">assert</span><span class="token plain"> size</span><span class="token punctuation">,</span><span class="token plain"> </span><span class="token string">&quot;Must specify layer size (num nodes)&quot;</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">        self</span><span class="token punctuation">.</span><span class="token plain">scope </span><span class="token operator">=</span><span class="token plain"> scope</span></span></div><div class="token-line"><span><span class="token plain">        self</span><span class="token punctuation">.</span><span class="token plain">size </span><span class="token operator">=</span><span class="token plain"> size</span></span></div><div class="token-line"><span><span class="token plain">        self</span><span class="token punctuation">.</span><span class="token plain">dropout </span><span class="token operator">=</span><span class="token plain"> dropout </span><span class="token comment"># keep_prob</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">        self</span><span class="token punctuation">.</span><span class="token plain">nonlinearity </span><span class="token operator">=</span><span class="token plain"> nonlinearity</span></span></div><div class="token-line"><span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">    </span><span class="token keyword">def</span><span class="token plain"> </span><span class="token function">__call__</span><span class="token punctuation">(</span><span class="token plain">self</span><span class="token punctuation">,</span><span class="token plain"> x</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">        </span><span class="token triple-quoted-string">&quot;&quot;&quot;Dense layer currying, to apply layer to any input tensor `x`&quot;&quot;&quot;</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">        </span><span class="token comment"># tf.Tensor -&amp;gt; tf.Tensor</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">        </span><span class="token keyword">with</span><span class="token plain"> tf</span><span class="token punctuation">.</span><span class="token plain">name_scope</span><span class="token punctuation">(</span><span class="token plain">self</span><span class="token punctuation">.</span><span class="token plain">scope</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">            </span><span class="token keyword">while</span><span class="token plain"> </span><span class="token boolean">True</span><span class="token punctuation">:</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">                </span><span class="token keyword">try</span><span class="token punctuation">:</span><span class="token plain"> </span><span class="token comment"># reuse weights if already initialized</span><span class="token plain"></span></span></div><div class="token-line" style="background:#ddd;margin-left:-12.892499999999998px;margin-right:-12.892499999999998px"><span><span class="token plain">                    </span><span class="token keyword">return</span><span class="token plain"> self</span><span class="token punctuation">.</span><span class="token plain">nonlinearity</span><span class="token punctuation">(</span><span class="token plain">tf</span><span class="token punctuation">.</span><span class="token plain">matmul</span><span class="token punctuation">(</span><span class="token plain">x</span><span class="token punctuation">,</span><span class="token plain"> self</span><span class="token punctuation">.</span><span class="token plain">w</span><span class="token punctuation">)</span><span class="token plain"> </span><span class="token operator">+</span><span class="token plain"> self</span><span class="token punctuation">.</span><span class="token plain">b</span><span class="token punctuation">)</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">                </span><span class="token keyword">except</span><span class="token punctuation">(</span><span class="token plain">AttributeError</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">                    self</span><span class="token punctuation">.</span><span class="token plain">w</span><span class="token punctuation">,</span><span class="token plain"> self</span><span class="token punctuation">.</span><span class="token plain">b </span><span class="token operator">=</span><span class="token plain"> self</span><span class="token punctuation">.</span><span class="token plain">wbVars</span><span class="token punctuation">(</span><span class="token plain">x</span><span class="token punctuation">.</span><span class="token plain">get_shape</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token plain">value</span><span class="token punctuation">,</span><span class="token plain"> self</span><span class="token punctuation">.</span><span class="token plain">size</span><span class="token punctuation">)</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">                    self</span><span class="token punctuation">.</span><span class="token plain">w </span><span class="token operator">=</span><span class="token plain"> tf</span><span class="token punctuation">.</span><span class="token plain">nn</span><span class="token punctuation">.</span><span class="token plain">dropout</span><span class="token punctuation">(</span><span class="token plain">self</span><span class="token punctuation">.</span><span class="token plain">w</span><span class="token punctuation">,</span><span class="token plain"> self</span><span class="token punctuation">.</span><span class="token plain">dropout</span><span class="token punctuation">)</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">    </span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain"></span></span></div></pre><div style="position:absolute;left:-1px;top:-1px;height:calc(158.7301587301587% + 2px);width:calc(158.7301587301587% + 2px);transform:scale(0.6300000000000001);transform-origin:left top;border:solid 2px black;pointer-events:none"></div></div></div></div><p>We can initialize a <code>Dense</code> layer with our choice of <code>nonlinearity</code> for the layer nodes (i.e. neural network units that apply a nonlinear activation function to a linear combination of their inputs, as per line <code>18</code>).</p><p>We’ll use <a href="https://arxiv.org/abs/1511.07289">ELUs</a> (Exponential Linear Units), a <a href="http://www.picalike.com/blog/2015/11/28/relu-was-yesterday-tomorrow-comes-elu/">recent advance</a> in building nodes that learn quickly by avoiding the problem of vanishing gradients. We wrap up the class with a <a href="https://github.com/fastforwardlabs/vae-tf/blob/master/layers.py#L26-L38">helper function</a> (<code>Dense.wbVars</code>) for compatible random initialization of weights and biases, to further accelerate learning.</p><p>In TensorFlow, neural networks are defined as numerical computation graphs. We will build the graph using partial function composition of sequential layers, which is amenable to an arbitrary number of hidden layers.</p><div><div style="position:relative;margin-bottom:25.784999999999997px;width:602px;margin-left:-20.338749999999997px;margin-right:-20.338749999999997px"><div class="prism-code language-python" style="overflow-x:auto;overflow-y:hidden;background:#f3f3f3"><pre style="float:left;font-size:12.892499999999998px;min-width:100%;position:relative;line-height:1.5;margin:0;padding:12.892499999999998px;overflow:visible"><div class="token-line"><span><span class="token keyword">def</span><span class="token plain"> </span><span class="token function">composeAll</span><span class="token punctuation">(</span><span class="token operator">*</span><span class="token plain">args</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">    </span><span class="token triple-quoted-string">&quot;&quot;&quot;Util for multiple function composition</span></span></div><div class="token-line"><span><span class="token triple-quoted-string"></span></span></div><div class="token-line"><span><span class="token triple-quoted-string">    i.e. composed = composeAll([f, g, h])</span></span></div><div class="token-line"><span><span class="token triple-quoted-string">         composed(x) # == f(g(h(x)))</span></span></div><div class="token-line"><span><span class="token triple-quoted-string">    &quot;&quot;&quot;</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">    </span><span class="token comment"># adapted from https://docs.python.org/3.1/howto/functional.html</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">    </span><span class="token keyword">return</span><span class="token plain"> partial</span><span class="token punctuation">(</span><span class="token plain">functools</span><span class="token punctuation">.</span><span class="token builtin">reduce</span><span class="token punctuation">,</span><span class="token plain"> compose</span><span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token operator">*</span><span class="token plain">args</span><span class="token punctuation">)</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain"></span></span></div></pre><div style="position:absolute;left:-1px;top:-1px;height:calc(158.7301587301587% + 2px);width:calc(158.7301587301587% + 2px);transform:scale(0.6300000000000001);transform-origin:left top;border:solid 2px black;pointer-events:none"></div></div></div></div><p>Now that we’ve defined our model primitives, we can tackle the VAE itself.
Keep in mind: the TensorFlow computational graph is cleanly divorced from the numerical computations themselves. In other words, a <code>tf.Graph</code> wireframes the underlying skeleton of the model, upon which we may hang values only within the context of a <code>tf.Session</code>.
Below, we initialize class <code>VAE</code> and activate a session for future convenience (so we can initialize and evaluate tensors within a single session, e.g. to persist weights and biases across rounds of training).
Here are some relevant snippets, cobbled together from the <a href="https://github.com/fastforwardlabs/vae-tf/blob/master/vae.py">full source code</a>:</p><div><div style="position:relative;margin-bottom:25.784999999999997px;width:602px;margin-left:-20.338749999999997px;margin-right:-20.338749999999997px"><div class="prism-code language-python" style="overflow-x:auto;overflow-y:hidden;background:#f3f3f3"><pre style="float:left;font-size:12.892499999999998px;min-width:100%;position:relative;line-height:1.5;margin:0;padding:12.892499999999998px;overflow:visible"><div class="token-line"><span><span class="token keyword">class</span><span class="token plain"> </span><span class="token class-name">VAE</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">    </span><span class="token triple-quoted-string">&quot;&quot;&quot;Variational Autoencoder</span></span></div><div class="token-line"><span><span class="token triple-quoted-string"></span></span></div><div class="token-line"><span><span class="token triple-quoted-string">    see: Kingma &amp;amp; Welling - Auto-Encoding Variational Bayes</span></span></div><div class="token-line"><span><span class="token triple-quoted-string">    (https://arxiv.org/abs/1312.6114)</span></span></div><div class="token-line"><span><span class="token triple-quoted-string">    &quot;&quot;&quot;</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">    DEFAULTS </span><span class="token operator">=</span><span class="token plain"> </span><span class="token punctuation">{</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">        </span><span class="token string">&quot;batch_size&quot;</span><span class="token punctuation">:</span><span class="token plain"> </span><span class="token number">128</span><span class="token punctuation">,</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">        </span><span class="token string">&quot;learning_rate&quot;</span><span class="token punctuation">:</span><span class="token plain"> </span><span class="token number">1E</span><span class="token operator">-</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">        </span><span class="token string">&quot;dropout&quot;</span><span class="token punctuation">:</span><span class="token plain"> </span><span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span><span class="token plain"> </span><span class="token comment"># keep_prob</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">        </span><span class="token string">&quot;lambda_l2_reg&quot;</span><span class="token punctuation">:</span><span class="token plain"> </span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">        </span><span class="token string">&quot;nonlinearity&quot;</span><span class="token punctuation">:</span><span class="token plain"> tf</span><span class="token punctuation">.</span><span class="token plain">nn</span><span class="token punctuation">.</span><span class="token plain">elu</span><span class="token punctuation">,</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">        </span><span class="token string">&quot;squashing&quot;</span><span class="token punctuation">:</span><span class="token plain"> tf</span><span class="token punctuation">.</span><span class="token plain">nn</span><span class="token punctuation">.</span><span class="token plain">sigmoid</span></span></div><div class="token-line"><span><span class="token plain">    </span><span class="token punctuation">}</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">    RESTORE_KEY </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string">&quot;to_restore&quot;</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">    </span><span class="token keyword">def</span><span class="token plain"> </span><span class="token function">__init__</span><span class="token punctuation">(</span><span class="token plain">self</span><span class="token punctuation">,</span><span class="token plain"> architecture</span><span class="token punctuation">,</span><span class="token plain"> d_hyperparams</span><span class="token operator">=</span><span class="token punctuation">{</span><span class="token punctuation">}</span><span class="token punctuation">,</span><span class="token plain"> meta_graph</span><span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">                 save_graph_def</span><span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span><span class="token plain"> log_dir</span><span class="token operator">=</span><span class="token string">&quot;./log&quot;</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">        </span><span class="token triple-quoted-string">&quot;&quot;&quot;(Re)build a symmetric VAE model with given:</span></span></div><div class="token-line"><span><span class="token triple-quoted-string"></span></span></div><div class="token-line"><span><span class="token triple-quoted-string">         * architecture (list of nodes per encoder layer); e.g.</span></span></div><div class="token-line"><span><span class="token triple-quoted-string">           [1000, 500, 250, 10] specifies a VAE with 1000-D inputs, 10-D latents,</span></span></div><div class="token-line"><span><span class="token triple-quoted-string">           &amp;amp; end-to-end architecture [1000, 500, 250, 10, 250, 500, 1000]</span></span></div><div class="token-line"><span><span class="token triple-quoted-string"></span></span></div><div class="token-line"><span><span class="token triple-quoted-string">         * hyperparameters (optional dictionary of updates to `DEFAULTS`)</span></span></div><div class="token-line"><span><span class="token triple-quoted-string">        &quot;&quot;&quot;</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">        self</span><span class="token punctuation">.</span><span class="token plain">architecture </span><span class="token operator">=</span><span class="token plain"> architecture</span></span></div><div class="token-line"><span><span class="token plain">        self</span><span class="token punctuation">.</span><span class="token plain">__dict__</span><span class="token punctuation">.</span><span class="token plain">update</span><span class="token punctuation">(</span><span class="token plain">VAE</span><span class="token punctuation">.</span><span class="token plain">DEFAULTS</span><span class="token punctuation">,</span><span class="token plain"> </span><span class="token operator">**</span><span class="token plain">d_hyperparams</span><span class="token punctuation">)</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">        self</span><span class="token punctuation">.</span><span class="token plain">sesh </span><span class="token operator">=</span><span class="token plain"> tf</span><span class="token punctuation">.</span><span class="token plain">Session</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">        </span><span class="token keyword">if</span><span class="token plain"> </span><span class="token operator">not</span><span class="token plain"> meta_graph</span><span class="token punctuation">:</span><span class="token plain"> </span><span class="token comment"># new model</span><span class="token plain"></span></span></div><div class="token-line" style="background:#ddd;margin-left:-12.892499999999998px;margin-right:-12.892499999999998px"><span><span class="token plain">            handles </span><span class="token operator">=</span><span class="token plain"> self</span><span class="token punctuation">.</span><span class="token plain">_buildGraph</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">            </span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">            self</span><span class="token punctuation">.</span><span class="token plain">sesh</span><span class="token punctuation">.</span><span class="token plain">run</span><span class="token punctuation">(</span><span class="token plain">tf</span><span class="token punctuation">.</span><span class="token plain">initialize_all_variables</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain"></span></span></div></pre><div style="position:absolute;left:-1px;top:-1px;height:calc(158.7301587301587% + 2px);width:calc(158.7301587301587% + 2px);transform:scale(0.6300000000000001);transform-origin:left top;border:solid 2px black;pointer-events:none"></div></div></div></div><p>Assuming that we are building a model from scratch (rather than restoring a <a href="https://www.tensorflow.org/versions/r0.9/how_tos/meta_graph/index.html">saved</a> <code>meta_graph</code>), the key initialization step is the call to <code>VAE._buildGraph</code> (line <code>32</code>). This internal method constructs nodes representing the placeholders and operations through which the data will flow—<em>before</em> any data is actually piped in.</p><p>Finally, we unpack the iterable <code>handles</code> (populated by <code>_buildGraph</code>) into convenient class attributes—pointers not to numerical values, but rather to nodes in the graph:</p><div><div style="position:relative;margin-bottom:25.784999999999997px;width:602px;margin-left:-20.338749999999997px;margin-right:-20.338749999999997px"><div class="prism-code language-python" style="overflow-x:auto;overflow-y:hidden;background:#f3f3f3"><pre style="float:left;font-size:12.892499999999998px;min-width:100%;position:relative;line-height:1.5;margin:0;padding:12.892499999999998px;overflow:visible"><div class="token-line"><span><span class="token plain">        </span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">        </span><span class="token comment"># unpack handles for tensor ops to feed or fetch</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">        </span><span class="token punctuation">(</span><span class="token plain">self</span><span class="token punctuation">.</span><span class="token plain">x_in</span><span class="token punctuation">,</span><span class="token plain"> self</span><span class="token punctuation">.</span><span class="token plain">dropout_</span><span class="token punctuation">,</span><span class="token plain"> self</span><span class="token punctuation">.</span><span class="token plain">z_mean</span><span class="token punctuation">,</span><span class="token plain"> self</span><span class="token punctuation">.</span><span class="token plain">z_log_sigma</span><span class="token punctuation">,</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">         self</span><span class="token punctuation">.</span><span class="token plain">x_reconstructed</span><span class="token punctuation">,</span><span class="token plain"> self</span><span class="token punctuation">.</span><span class="token plain">z_</span><span class="token punctuation">,</span><span class="token plain"> self</span><span class="token punctuation">.</span><span class="token plain">x_reconstructed_</span><span class="token punctuation">,</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">         self</span><span class="token punctuation">.</span><span class="token plain">cost</span><span class="token punctuation">,</span><span class="token plain"> self</span><span class="token punctuation">.</span><span class="token plain">global_step</span><span class="token punctuation">,</span><span class="token plain"> self</span><span class="token punctuation">.</span><span class="token plain">train_op</span><span class="token punctuation">)</span><span class="token plain"> </span><span class="token operator">=</span><span class="token plain"> handles</span></span></div><div class="token-line"><span><span class="token plain"></span></span></div></pre><div style="position:absolute;left:-1px;top:-1px;height:calc(158.7301587301587% + 2px);width:calc(158.7301587301587% + 2px);transform:scale(0.6300000000000001);transform-origin:left top;border:solid 2px black;pointer-events:none"></div></div></div></div><p>How are these nodes defined? The <code>_buildGraph</code> method encapsulates the core of the VAE model framework—starting with the encoder/inference network:</p><div><div style="position:relative;margin-bottom:25.784999999999997px;width:602px;margin-left:-20.338749999999997px;margin-right:-20.338749999999997px"><div class="prism-code language-python" style="overflow-x:auto;overflow-y:hidden;background:#f3f3f3"><pre style="float:left;font-size:12.892499999999998px;min-width:100%;position:relative;line-height:1.5;margin:0;padding:12.892499999999998px;overflow:visible"><div class="token-line"><span><span class="token plain">    </span><span class="token keyword">def</span><span class="token plain"> </span><span class="token function">_buildGraph</span><span class="token punctuation">(</span><span class="token plain">self</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">        x_in </span><span class="token operator">=</span><span class="token plain"> tf</span><span class="token punctuation">.</span><span class="token plain">placeholder</span><span class="token punctuation">(</span><span class="token plain">tf</span><span class="token punctuation">.</span><span class="token plain">float32</span><span class="token punctuation">,</span><span class="token plain"> shape</span><span class="token operator">=</span><span class="token punctuation">[</span><span class="token boolean">None</span><span class="token punctuation">,</span><span class="token plain"> </span><span class="token comment"># enables variable batch size</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">                                                 self</span><span class="token punctuation">.</span><span class="token plain">architecture</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token plain"> name</span><span class="token operator">=</span><span class="token string">&quot;x&quot;</span><span class="token punctuation">)</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">        dropout </span><span class="token operator">=</span><span class="token plain"> tf</span><span class="token punctuation">.</span><span class="token plain">placeholder_with_default</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span><span class="token plain"> shape</span><span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token plain"> name</span><span class="token operator">=</span><span class="token string">&quot;dropout&quot;</span><span class="token punctuation">)</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">        </span><span class="token comment"># encoding / &quot;recognition&quot;: q(z|x)</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">        encoding </span><span class="token operator">=</span><span class="token plain"> </span><span class="token punctuation">[</span><span class="token plain">Dense</span><span class="token punctuation">(</span><span class="token string">&quot;encoding&quot;</span><span class="token punctuation">,</span><span class="token plain"> hidden_size</span><span class="token punctuation">,</span><span class="token plain"> dropout</span><span class="token punctuation">,</span><span class="token plain"> self</span><span class="token punctuation">.</span><span class="token plain">nonlinearity</span><span class="token punctuation">)</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">                    </span><span class="token comment"># hidden layers reversed for function composition: outer -&amp;gt; inner</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">                    </span><span class="token keyword">for</span><span class="token plain"> hidden_size </span><span class="token keyword">in</span><span class="token plain"> </span><span class="token builtin">reversed</span><span class="token punctuation">(</span><span class="token plain">self</span><span class="token punctuation">.</span><span class="token plain">architecture</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">        h_encoded </span><span class="token operator">=</span><span class="token plain"> composeAll</span><span class="token punctuation">(</span><span class="token plain">encoding</span><span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token plain">x_in</span><span class="token punctuation">)</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">        </span><span class="token comment"># latent distribution parameterized by hidden encoding</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">        </span><span class="token comment"># z ~ N(z_mean, np.exp(z_log_sigma)**2)</span><span class="token plain"></span></span></div><div class="token-line" style="background:#ddd;margin-left:-12.892499999999998px;margin-right:-12.892499999999998px"><span><span class="token plain">        z_mean </span><span class="token operator">=</span><span class="token plain"> Dense</span><span class="token punctuation">(</span><span class="token string">&quot;z_mean&quot;</span><span class="token punctuation">,</span><span class="token plain"> self</span><span class="token punctuation">.</span><span class="token plain">architecture</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token plain"> dropout</span><span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token plain">h_encoded</span><span class="token punctuation">)</span><span class="token plain"></span></span></div><div class="token-line" style="background:#ddd;margin-left:-12.892499999999998px;margin-right:-12.892499999999998px"><span><span class="token plain">        z_log_sigma </span><span class="token operator">=</span><span class="token plain"> Dense</span><span class="token punctuation">(</span><span class="token string">&quot;z_log_sigma&quot;</span><span class="token punctuation">,</span><span class="token plain"> self</span><span class="token punctuation">.</span><span class="token plain">architecture</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token plain"> dropout</span><span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token plain">h_encoded</span><span class="token punctuation">)</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain"></span></span></div></pre><div style="position:absolute;left:-1px;top:-1px;height:calc(158.7301587301587% + 2px);width:calc(158.7301587301587% + 2px);transform:scale(0.6300000000000001);transform-origin:left top;border:solid 2px black;pointer-events:none"></div></div></div></div><p>Here, we build a pipe from <code>x_in</code> (an empty placeholder for input data <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base"><span class="mord mathit">x</span></span></span></span></span>), through the sequential hidden encoding, to the corresponding distribution over latent space—the variational approximate posterior, or hidden representation, <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>z</mi><mo>∼</mo><msub><mi>q</mi><mi>ϕ</mi></msub><mo>(</mo><mi>z</mi><mi mathvariant="normal">∣</mi><mi>x</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">z \sim q_\phi(z|x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1.036108em;vertical-align:-0.286108em;"></span><span class="base"><span class="mord mathit" style="margin-right:0.04398em;">z</span><span class="mord rule" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∼</span><span class="mord rule" style="margin-right:0.2777777777777778em;"></span><span class="mord"><span class="mord mathit" style="margin-right:0.03588em;">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight">ϕ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"></span></span></span></span></span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.04398em;">z</span><span class="mord">∣</span><span class="mord mathit">x</span><span class="mclose">)</span></span></span></span></span>.</p><p>As observed in lines <code>14</code> - <code>15</code>, latent <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base"><span class="mord mathit" style="margin-right:0.04398em;">z</span></span></span></span></span> is distributed as a multivariate <a href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2465539/figure/fig1/">normal</a> with mean <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>μ</mi></mrow><annotation encoding="application/x-tex">\mu</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="base"><span class="mord mathit">μ</span></span></span></span></span> and diagonal covariance values <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>σ</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">\sigma^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8141079999999999em;"></span><span class="strut bottom" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="base"><span class="mord"><span class="mord mathit" style="margin-right:0.03588em;">σ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span> (the square of the “sigma” in <code>z_log_sigma</code>) directly parameterized by the encoder: <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mrow><mi mathvariant="script">N</mi></mrow><mo>(</mo><mi>μ</mi><mo separator="true">,</mo><msup><mi>σ</mi><mn>2</mn></msup><mi>I</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">\mathcal{N}(\mu, \sigma^2I)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8141079999999999em;"></span><span class="strut bottom" style="height:1.064108em;vertical-align:-0.25em;"></span><span class="base"><span class="mord"><span class="mord mathcal" style="margin-right:0.14736em;">N</span></span><span class="mopen">(</span><span class="mord mathit">μ</span><span class="mpunct">,</span><span class="mord rule" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathit" style="margin-right:0.03588em;">σ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mord mathit" style="margin-right:0.07847em;">I</span><span class="mclose">)</span></span></span></span></span>. In other words, we set out to “explain” highly complex observations as the consequence of an unobserved collection of simplified latent variables, i.e. independent Gaussians. (This is dictated by our choice of a conjugate spherical Gaussian prior over <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base"><span class="mord mathit" style="margin-right:0.04398em;">z</span></span></span></span></span>—see <a href="http://blog.fastforwardlabs.com/2016/08/12/introducing-variational-autoencoders-in-prose-and.html">Part I</a>.)</p><p>Next, we sample from this latent distribution (in practice, <a href="https://arxiv.org/abs/1312.6114">one draw is enough</a> given sufficient minibatch size, i.e. &gt;100). This method involves a trick—can you figure out why?—that we will explore in more detail later.</p><div><div style="position:relative;margin-bottom:25.784999999999997px;width:602px;margin-left:-20.338749999999997px;margin-right:-20.338749999999997px"><div class="prism-code language-python" style="overflow-x:auto;overflow-y:hidden;background:#f3f3f3"><pre style="float:left;font-size:12.892499999999998px;min-width:100%;position:relative;line-height:1.5;margin:0;padding:12.892499999999998px;overflow:visible"><div class="token-line"><span><span class="token plain">        z </span><span class="token operator">=</span><span class="token plain"> self</span><span class="token punctuation">.</span><span class="token plain">sampleGaussian</span><span class="token punctuation">(</span><span class="token plain">z_mean</span><span class="token punctuation">,</span><span class="token plain"> z_log_sigma</span><span class="token punctuation">)</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain"></span></span></div></pre><div style="position:absolute;left:-1px;top:-1px;height:calc(158.7301587301587% + 2px);width:calc(158.7301587301587% + 2px);transform:scale(0.6300000000000001);transform-origin:left top;border:solid 2px black;pointer-events:none"></div></div></div></div><p>The sampled <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base"><span class="mord mathit" style="margin-right:0.04398em;">z</span></span></span></span></span> is then passed to the decoder/generative network, which symmetrically builds back out to generate the conditional distribution over input space, reconstruction <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mover accent="true"><mrow><mi>x</mi></mrow><mo>~</mo></mover><mo>∼</mo><msub><mi>p</mi><mi>θ</mi></msub><mo>(</mo><mi>x</mi><mi mathvariant="normal">∣</mi><mi>z</mi><mo>)</mo><mo>)</mo></mrow><annotation encoding="application/x-tex">\tilde{x} \sim p_\theta(x|z))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6678599999999999em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathit">x</span></span></span><span style="top:-3.35em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.22222em;">~</span></span></span></span></span></span><span class="mord rule" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∼</span><span class="mord rule" style="margin-right:0.2777777777777778em;"></span><span class="mord"><span class="mord mathit">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"></span></span></span></span></span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mord">∣</span><span class="mord mathit" style="margin-right:0.04398em;">z</span><span class="mclose">)</span><span class="mclose">)</span></span></span></span></span>.</p><div><div style="position:relative;margin-bottom:25.784999999999997px;width:602px;margin-left:-20.338749999999997px;margin-right:-20.338749999999997px"><div class="prism-code language-python" style="overflow-x:auto;overflow-y:hidden;background:#f3f3f3"><pre style="float:left;font-size:12.892499999999998px;min-width:100%;position:relative;line-height:1.5;margin:0;padding:12.892499999999998px;overflow:visible"><div class="token-line"><span><span class="token plain">        </span><span class="token comment"># decoding / &quot;generative&quot;: p(x|z)</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">        decoding </span><span class="token operator">=</span><span class="token plain"> </span><span class="token punctuation">[</span><span class="token plain">Dense</span><span class="token punctuation">(</span><span class="token string">&quot;decoding&quot;</span><span class="token punctuation">,</span><span class="token plain"> hidden_size</span><span class="token punctuation">,</span><span class="token plain"> dropout</span><span class="token punctuation">,</span><span class="token plain"> self</span><span class="token punctuation">.</span><span class="token plain">nonlinearity</span><span class="token punctuation">)</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">                    </span><span class="token keyword">for</span><span class="token plain"> hidden_size </span><span class="token keyword">in</span><span class="token plain"> self</span><span class="token punctuation">.</span><span class="token plain">architecture</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token plain"> </span><span class="token comment"># assumes symmetry</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">        </span><span class="token comment"># final reconstruction: restore original dims, squash outputs [0, 1]</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">        decoding</span><span class="token punctuation">.</span><span class="token plain">insert</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token plain"> Dense</span><span class="token punctuation">(</span><span class="token plain"> </span><span class="token comment"># prepend as outermost function</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">            </span><span class="token string">&quot;reconstruction&quot;</span><span class="token punctuation">,</span><span class="token plain"> self</span><span class="token punctuation">.</span><span class="token plain">architecture</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token plain"> dropout</span><span class="token punctuation">,</span><span class="token plain"> self</span><span class="token punctuation">.</span><span class="token plain">squashing</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">        x_reconstructed </span><span class="token operator">=</span><span class="token plain"> tf</span><span class="token punctuation">.</span><span class="token plain">identity</span><span class="token punctuation">(</span><span class="token plain">composeAll</span><span class="token punctuation">(</span><span class="token plain">decoding</span><span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token plain">z</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token plain"> name</span><span class="token operator">=</span><span class="token string">&quot;x_reconstructed&quot;</span><span class="token punctuation">)</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain"></span></span></div></pre><div style="position:absolute;left:-1px;top:-1px;height:calc(158.7301587301587% + 2px);width:calc(158.7301587301587% + 2px);transform:scale(0.6300000000000001);transform-origin:left top;border:solid 2px black;pointer-events:none"></div></div></div></div><p>Alternately, we add a placeholder to directly feed arbitrary values of <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base"><span class="mord mathit" style="margin-right:0.04398em;">z</span></span></span></span></span> to the generative network (to fabricate realistic outputs—no input data necessary!):</p><div><div style="position:relative;margin-bottom:25.784999999999997px;width:602px;margin-left:-20.338749999999997px;margin-right:-20.338749999999997px"><div class="prism-code language-python" style="overflow-x:auto;overflow-y:hidden;background:#f3f3f3"><pre style="float:left;font-size:12.892499999999998px;min-width:100%;position:relative;line-height:1.5;margin:0;padding:12.892499999999998px;overflow:visible"><div class="token-line"><span><span class="token plain">        </span><span class="token comment"># ops to directly explore latent space</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">        </span><span class="token comment"># defaults to prior z ~ N(0, I)</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">        z_ </span><span class="token operator">=</span><span class="token plain"> tf</span><span class="token punctuation">.</span><span class="token plain">placeholder_with_default</span><span class="token punctuation">(</span><span class="token plain">tf</span><span class="token punctuation">.</span><span class="token plain">random_normal</span><span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token plain"> self</span><span class="token punctuation">.</span><span class="token plain">architecture</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">                                         shape</span><span class="token operator">=</span><span class="token punctuation">[</span><span class="token boolean">None</span><span class="token punctuation">,</span><span class="token plain"> self</span><span class="token punctuation">.</span><span class="token plain">architecture</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">                                         name</span><span class="token operator">=</span><span class="token string">&quot;latent_in&quot;</span><span class="token punctuation">)</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">        x_reconstructed_ </span><span class="token operator">=</span><span class="token plain"> composeAll</span><span class="token punctuation">(</span><span class="token plain">decoding</span><span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token plain">z_</span><span class="token punctuation">)</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain"></span></span></div></pre><div style="position:absolute;left:-1px;top:-1px;height:calc(158.7301587301587% + 2px);width:calc(158.7301587301587% + 2px);transform:scale(0.6300000000000001);transform-origin:left top;border:solid 2px black;pointer-events:none"></div></div></div></div><p>TensorFlow automatically flows data through the appropriate subgraph, based on the nodes that we fetch and feed with the <code>tf.Session.run</code> method. Defining the <a href="https://github.com/fastforwardlabs/vae-tf/blob/master/vae.py#L190-L196">encoder</a>, <a href="https://github.com/fastforwardlabs/vae-tf/blob/master/vae.py#L198-L209">decoder</a>, and <a href="https://github.com/fastforwardlabs/vae-tf/blob/master/vae.py#L211-L214">end-to-end VAE</a> is then trivial (see linked code).</p><p>We’ll finish the <code>VAE._buildGraph</code> method later in the post, as we walk through the nuances of the model.</p><h2>The Reparameterization Trick</h2><p>In order to estimate the latent representation <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base"><span class="mord mathit" style="margin-right:0.04398em;">z</span></span></span></span></span> for a given observation <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base"><span class="mord mathit">x</span></span></span></span></span>, we want to sample from the approximate posterior <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>(</mo><msub><mi>q</mi><mi>ϕ</mi></msub><mo>(</mo><mi>z</mi><mi mathvariant="normal">∣</mi><mi>x</mi><mo>)</mo><mo>)</mo></mrow><annotation encoding="application/x-tex">(q_\phi(z|x))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1.036108em;vertical-align:-0.286108em;"></span><span class="base"><span class="mopen">(</span><span class="mord"><span class="mord mathit" style="margin-right:0.03588em;">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight">ϕ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"></span></span></span></span></span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.04398em;">z</span><span class="mord">∣</span><span class="mord mathit">x</span><span class="mclose">)</span><span class="mclose">)</span></span></span></span></span> according to the distribution defined by the encoder.</p><p>However, model training by <a href="http://mathworld.wolfram.com/MethodofSteepestDescent.html">gradient descent</a> requires that our model be differentiable with respect to its learned parameters (which is how we propagate the gradients). This presupposes that the model is deterministic—i.e. a given input always returns the same output for a fixed set of parameters, so the only source of stochasticity are the inputs. Incorporating a probabilistic “sampling” node would make the model itself stochastic!</p><p>Instead, we inject randomness into the model by introducing input from an auxiliary random variable: <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>(</mo><mi>ϵ</mi><mo>∼</mo><mi>p</mi><mo>(</mo><mi>ϵ</mi><mo>)</mo><mo>)</mo></mrow><annotation encoding="application/x-tex">(\epsilon \sim p(\epsilon))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base"><span class="mopen">(</span><span class="mord mathit">ϵ</span><span class="mord rule" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∼</span><span class="mord rule" style="margin-right:0.2777777777777778em;"></span><span class="mord mathit">p</span><span class="mopen">(</span><span class="mord mathit">ϵ</span><span class="mclose">)</span><span class="mclose">)</span></span></span></span></span></p><p>For our purposes, rather than sampling <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base"><span class="mord mathit" style="margin-right:0.04398em;">z</span></span></span></span></span> directly from <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>q</mi><mi>ϕ</mi></msub><mo>(</mo><mi>z</mi><mi mathvariant="normal">∣</mi><mi>x</mi><mo>)</mo><mo>∼</mo><mrow><mi mathvariant="script">N</mi></mrow><mo>(</mo><mi>μ</mi><mo separator="true">,</mo><msup><mi>σ</mi><mn>2</mn></msup><mi>I</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">q_\phi(z|x) \sim \mathcal{N}(\mu, \sigma^2I)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.8141079999999999em;"></span><span class="strut bottom" style="height:1.1002159999999999em;vertical-align:-0.286108em;"></span><span class="base"><span class="mord"><span class="mord mathit" style="margin-right:0.03588em;">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight">ϕ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"></span></span></span></span></span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.04398em;">z</span><span class="mord">∣</span><span class="mord mathit">x</span><span class="mclose">)</span><span class="mord rule" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∼</span><span class="mord rule" style="margin-right:0.2777777777777778em;"></span><span class="mord"><span class="mord mathcal" style="margin-right:0.14736em;">N</span></span><span class="mopen">(</span><span class="mord mathit">μ</span><span class="mpunct">,</span><span class="mord rule" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathit" style="margin-right:0.03588em;">σ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mord mathit" style="margin-right:0.07847em;">I</span><span class="mclose">)</span></span></span></span></span>, we generate Gaussian noise <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ϵ</mi><mo>∼</mo><mrow><mi mathvariant="script">N</mi></mrow><mo>(</mo><mn>0</mn><mo separator="true">,</mo><mi>I</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">\epsilon \sim \mathcal{N}(0, I)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base"><span class="mord mathit">ϵ</span><span class="mord rule" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∼</span><span class="mord rule" style="margin-right:0.2777777777777778em;"></span><span class="mord"><span class="mord mathcal" style="margin-right:0.14736em;">N</span></span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mord rule" style="margin-right:0.16666666666666666em;"></span><span class="mord mathit" style="margin-right:0.07847em;">I</span><span class="mclose">)</span></span></span></span></span> and compute <div><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>z</mi><mo>=</mo><mi>μ</mi><mo>+</mo><mi>σ</mi><mo>⊙</mo><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">z = \mu + \sigma \odot \epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.58333em;"></span><span class="strut bottom" style="height:0.7777700000000001em;vertical-align:-0.19444em;"></span><span class="base"><span class="mord mathit" style="margin-right:0.04398em;">z</span><span class="mord rule" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mord rule" style="margin-right:0.2777777777777778em;"></span><span class="mord mathit">μ</span><span class="mord rule" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mord rule" style="margin-right:0.2222222222222222em;"></span><span class="mord mathit" style="margin-right:0.03588em;">σ</span><span class="mord rule" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⊙</span><span class="mord rule" style="margin-right:0.2222222222222222em;"></span><span class="mord mathit">ϵ</span></span></span></span></span></div> (where <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>⊙</mo></mrow><annotation encoding="application/x-tex">\odot</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.58333em;"></span><span class="strut bottom" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="base"><span class="mord">⊙</span></span></span></span></span> is the element-wise product). In code:</p><div><div style="position:relative;margin-bottom:25.784999999999997px;width:602px;margin-left:-20.338749999999997px;margin-right:-20.338749999999997px"><div class="prism-code language-python" style="overflow-x:auto;overflow-y:hidden;background:#f3f3f3"><pre style="float:left;font-size:12.892499999999998px;min-width:100%;position:relative;line-height:1.5;margin:0;padding:12.892499999999998px;overflow:visible"><div class="token-line"><span><span class="token plain">    </span><span class="token keyword">def</span><span class="token plain"> </span><span class="token function">sampleGaussian</span><span class="token punctuation">(</span><span class="token plain">self</span><span class="token punctuation">,</span><span class="token plain"> mu</span><span class="token punctuation">,</span><span class="token plain"> log_sigma</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">        </span><span class="token triple-quoted-string">&quot;&quot;&quot;Draw sample from Gaussian with given shape, subject to random noise epsilon&quot;&quot;&quot;</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">        </span><span class="token keyword">with</span><span class="token plain"> tf</span><span class="token punctuation">.</span><span class="token plain">name_scope</span><span class="token punctuation">(</span><span class="token string">&quot;sample_gaussian&quot;</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">            </span><span class="token comment"># reparameterization trick</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">            epsilon </span><span class="token operator">=</span><span class="token plain"> tf</span><span class="token punctuation">.</span><span class="token plain">random_normal</span><span class="token punctuation">(</span><span class="token plain">tf</span><span class="token punctuation">.</span><span class="token plain">shape</span><span class="token punctuation">(</span><span class="token plain">log_sigma</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token plain"> name</span><span class="token operator">=</span><span class="token string">&quot;epsilon&quot;</span><span class="token punctuation">)</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">            </span><span class="token keyword">return</span><span class="token plain"> mu </span><span class="token operator">+</span><span class="token plain"> epsilon </span><span class="token operator">*</span><span class="token plain"> tf</span><span class="token punctuation">.</span><span class="token plain">exp</span><span class="token punctuation">(</span><span class="token plain">log_sigma</span><span class="token punctuation">)</span><span class="token plain"> </span><span class="token comment"># N(mu, sigma**2)</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain"></span></span></div></pre><div style="position:absolute;left:-1px;top:-1px;height:calc(158.7301587301587% + 2px);width:calc(158.7301587301587% + 2px);transform:scale(0.6300000000000001);transform-origin:left top;border:solid 2px black;pointer-events:none"></div></div></div></div><p>By “reparameterizing” this step, inference and generation become entirely differentiable and hence, learnable.</p><h2>Cost Function</h2><p>Now, in order to optimize the model, we need a metric for how well its parameters capture the true data-generating and latent distributions. That is, how likely is observation <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base"><span class="mord mathit">x</span></span></span></span></span> under the joint distribution <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi><mo>(</mo><mi>x</mi><mo separator="true">,</mo><mi>z</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">p(x, z)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base"><span class="mord mathit">p</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mpunct">,</span><span class="mord rule" style="margin-right:0.16666666666666666em;"></span><span class="mord mathit" style="margin-right:0.04398em;">z</span><span class="mclose">)</span></span></span></span></span>?</p><p>Recall that we represent the global encoder and decoder parameters (i.e. neural network weights and biases) as <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ϕ</mi></mrow><annotation encoding="application/x-tex">\phi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="base"><span class="mord mathit">ϕ</span></span></span></span></span> and <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.69444em;vertical-align:0em;"></span><span class="base"><span class="mord mathit" style="margin-right:0.02778em;">θ</span></span></span></span></span>, respectively.</p><p>In other words, we want to simultaneously tune these complementary parameters such that we maximize <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mo>(</mo><mi>p</mi><mo>(</mo><mi>x</mi><mi mathvariant="normal">∣</mi><mi>ϕ</mi><mo separator="true">,</mo><mi>θ</mi><mo>)</mo><mo>)</mo></mrow><annotation encoding="application/x-tex">log(p(x|\phi, \theta))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base"><span class="mord mathit" style="margin-right:0.01968em;">l</span><span class="mord mathit">o</span><span class="mord mathit" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord mathit">p</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mord">∣</span><span class="mord mathit">ϕ</span><span class="mpunct">,</span><span class="mord rule" style="margin-right:0.16666666666666666em;"></span><span class="mord mathit" style="margin-right:0.02778em;">θ</span><span class="mclose">)</span><span class="mclose">)</span></span></span></span></span>—the log-likelihood across all datapoints <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base"><span class="mord mathit">x</span></span></span></span></span> under the current model settings, after marginalizing out the latent variables <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base"><span class="mord mathit" style="margin-right:0.04398em;">z</span></span></span></span></span>. This term is also known as the model <em>evidence</em>.</p><p>We can express this marginal likelihood as the sum of what we’ll call the <em>variational</em> or <em>evidence lower bound</em> <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mrow><mi mathvariant="script">L</mi></mrow></mrow><annotation encoding="application/x-tex">\mathcal{L}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base"><span class="mord"><span class="mord mathcal">L</span></span></span></span></span></span> and the <em>Kullback-Leibler (KL) divergence</em> <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi mathvariant="script">D</mi><mrow><mi>K</mi><mi>L</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\mathcal{D}_{KL}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="base"><span class="mord"><span class="mord"><span class="mord mathcal" style="margin-right:0.02778em;">D</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight" style="margin-right:0.07153em;">K</span><span class="mord mathit mtight">L</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"></span></span></span></span></span></span></span></span></span> between the approximate and true latent posteriors: <div><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mo>(</mo><mi>p</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>)</mo><mo>=</mo><mrow><mi mathvariant="script">L</mi></mrow><mo>(</mo><mi>ϕ</mi><mo separator="true">,</mo><mi>θ</mi><mo separator="true">;</mo><mi>x</mi><mo>)</mo><mo>+</mo><msub><mi mathvariant="script">D</mi><mrow><mi>K</mi><mi>L</mi></mrow></msub><mo>(</mo><msub><mi>q</mi><mi>ϕ</mi></msub><mo>(</mo><mi>z</mi><mi mathvariant="normal">∣</mi><mi>x</mi><mo>)</mo><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><msub><mi>p</mi><mi>θ</mi></msub><mo>(</mo><mi>z</mi><mi mathvariant="normal">∣</mi><mi>x</mi><mo>)</mo><mo>)</mo></mrow><annotation encoding="application/x-tex"> log(p(x)) = \mathcal{L}(\phi, \theta; x) + \mathcal{D}_{KL}(q_\phi(z|x) || p_\theta(z|x))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1.036108em;vertical-align:-0.286108em;"></span><span class="base"><span class="mord mathit" style="margin-right:0.01968em;">l</span><span class="mord mathit">o</span><span class="mord mathit" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord mathit">p</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span><span class="mclose">)</span><span class="mord rule" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mord rule" style="margin-right:0.2777777777777778em;"></span><span class="mord"><span class="mord mathcal">L</span></span><span class="mopen">(</span><span class="mord mathit">ϕ</span><span class="mpunct">,</span><span class="mord rule" style="margin-right:0.16666666666666666em;"></span><span class="mord mathit" style="margin-right:0.02778em;">θ</span><span class="mpunct">;</span><span class="mord rule" style="margin-right:0.16666666666666666em;"></span><span class="mord mathit">x</span><span class="mclose">)</span><span class="mord rule" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mord rule" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord"><span class="mord mathcal" style="margin-right:0.02778em;">D</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight" style="margin-right:0.07153em;">K</span><span class="mord mathit mtight">L</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathit" style="margin-right:0.03588em;">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight">ϕ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"></span></span></span></span></span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.04398em;">z</span><span class="mord">∣</span><span class="mord mathit">x</span><span class="mclose">)</span><span class="mord">∣</span><span class="mord">∣</span><span class="mord"><span class="mord mathit">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"></span></span></span></span></span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.04398em;">z</span><span class="mord">∣</span><span class="mord mathit">x</span><span class="mclose">)</span><span class="mclose">)</span></span></span></span></span></div></p><p>Here, the KL divergence can be (<a href="http://mathworld.wolfram.com/RelativeEntropy.html">fuzzily</a>!) intuited as a metric for the misfit of the approximate posterior <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>q</mi><mi>ϕ</mi></msub></mrow><annotation encoding="application/x-tex">q_\phi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.716668em;vertical-align:-0.286108em;"></span><span class="base"><span class="mord"><span class="mord mathit" style="margin-right:0.03588em;">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight">ϕ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"></span></span></span></span></span></span></span></span></span>. We’ll delve into this further in a moment, but for now the important thing is that it is non-negative by definition; consequently, the first term acts as a <em>lower bound</em> on the total. So, we maximize the lower bound <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mrow><mi mathvariant="script">L</mi></mrow></mrow><annotation encoding="application/x-tex">\mathcal{L}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base"><span class="mord"><span class="mord mathcal">L</span></span></span></span></span></span> as a (computationally-tractable) proxy for the total marginal likelihood of the data under the model. (And the better our approximate posterior, the tighter the gap between the lower bound and the total model evidence.)</p><p>With some <a href="https://arxiv.org/abs/1312.6114">mathematical wrangling</a>, we can decompose <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mrow><mi mathvariant="script">L</mi></mrow></mrow><annotation encoding="application/x-tex">\mathcal{L}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base"><span class="mord"><span class="mord mathcal">L</span></span></span></span></span></span> into the following objective function: <div><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mrow><mi mathvariant="script">L</mi></mrow><mo>(</mo><mi>ϕ</mi><mo separator="true">,</mo><mi>θ</mi><mo separator="true">;</mo><mi>x</mi><mo>)</mo><mo>=</mo><msub><mi mathvariant="double-struck">E</mi><mrow><mi>z</mi><mo>∼</mo><msub><mi>q</mi><mi>ϕ</mi></msub><mo>(</mo><mi>z</mi><mi mathvariant="normal">∣</mi><mi>x</mi><mo>)</mo></mrow></msub><mo>[</mo><mi>l</mi><mi>o</mi><mi>g</mi><mo>(</mo><msub><mi>p</mi><mi>θ</mi></msub><mo>(</mo><mi>x</mi><mi mathvariant="normal">∣</mi><mi>z</mi><mo>)</mo><mo>)</mo><mo>]</mo><mo>−</mo><msub><mi mathvariant="script">D</mi><mrow><mi>K</mi><mi>L</mi></mrow></msub><mo>(</mo><msub><mi>q</mi><mi>ϕ</mi></msub><mo>(</mo><mi>z</mi><mi mathvariant="normal">∣</mi><mi>x</mi><mo>)</mo><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><msub><mi>p</mi><mi>θ</mi></msub><mo>(</mo><mi>z</mi><mo>)</mo><mo>)</mo></mrow><annotation encoding="application/x-tex">\mathcal{L}(\phi, \theta; x) = \mathbb{E}_{z \sim q_\phi(z|x)}[log(p_\theta(x|z))] - \mathcal{D}_{KL}(q_\phi(z|x) || p_\theta(z))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1.1332799999999998em;vertical-align:-0.38327999999999984em;"></span><span class="base"><span class="mord"><span class="mord mathcal">L</span></span><span class="mopen">(</span><span class="mord mathit">ϕ</span><span class="mpunct">,</span><span class="mord rule" style="margin-right:0.16666666666666666em;"></span><span class="mord mathit" style="margin-right:0.02778em;">θ</span><span class="mpunct">;</span><span class="mord rule" style="margin-right:0.16666666666666666em;"></span><span class="mord mathit">x</span><span class="mclose">)</span><span class="mord rule" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mord rule" style="margin-right:0.2777777777777778em;"></span><span class="mord"><span class="mord"><span class="mord mathbb">E</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.5198em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight" style="margin-right:0.04398em;">z</span><span class="mrel mtight">∼</span><span class="mord mtight"><span class="mord mathit mtight" style="margin-right:0.03588em;">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3487714285714287em;margin-left:-0.03588em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathit mtight">ϕ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.29011428571428566em;"></span></span></span></span></span><span class="mopen mtight">(</span><span class="mord mathit mtight" style="margin-right:0.04398em;">z</span><span class="mord mtight">∣</span><span class="mord mathit mtight">x</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.38327999999999984em;"></span></span></span></span></span><span class="mopen">[</span><span class="mord mathit" style="margin-right:0.01968em;">l</span><span class="mord mathit">o</span><span class="mord mathit" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord"><span class="mord mathit">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"></span></span></span></span></span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mord">∣</span><span class="mord mathit" style="margin-right:0.04398em;">z</span><span class="mclose">)</span><span class="mclose">)</span><span class="mclose">]</span><span class="mord rule" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mord rule" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord"><span class="mord mathcal" style="margin-right:0.02778em;">D</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight" style="margin-right:0.07153em;">K</span><span class="mord mathit mtight">L</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathit" style="margin-right:0.03588em;">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight">ϕ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"></span></span></span></span></span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.04398em;">z</span><span class="mord">∣</span><span class="mord mathit">x</span><span class="mclose">)</span><span class="mord">∣</span><span class="mord">∣</span><span class="mord"><span class="mord mathit">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"></span></span></span></span></span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.04398em;">z</span><span class="mclose">)</span><span class="mclose">)</span></span></span></span></span></div> (Phrased as a cost, we optimize the model by minimizing <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mrow><mo>−</mo><mrow><mi mathvariant="script">L</mi></mrow></mrow></mrow><annotation encoding="application/x-tex">{-\mathcal{L}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="base"><span class="mord"><span class="mord">−</span><span class="mord"><span class="mord mathcal">L</span></span></span></span></span></span></span>.)</p><p>Here, the perhaps unfriendly-looking first term is, in fact, familiar! It’s the probability density of generated output <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mover accent="true"><mrow><mi>x</mi></mrow><mo>~</mo></mover></mrow><annotation encoding="application/x-tex">\tilde{x}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.6678599999999999em;"></span><span class="strut bottom" style="height:0.6678599999999999em;vertical-align:0em;"></span><span class="base"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6678599999999999em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathit">x</span></span></span><span style="top:-3.35em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.22222em;">~</span></span></span></span></span></span></span></span></span></span> given the inferred latent distribution over <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base"><span class="mord mathit" style="margin-right:0.04398em;">z</span></span></span></span></span>—i.e. the (negative) expected <em>reconstruction error</em>. This loss term is intrinsic to perhaps every autoencoder: how accurately does the output replicate the input?</p><p>Choosing an appropriate metric for image resemblance is hard (but that’s another <a href="https://arxiv.org/abs/1512.09300">story</a>). We’ll use the binary <a href="http://neuralnetworksanddeeplearning.com/chap3.html#the_cross-entropy_cost_function">cross-entropy</a>, which is commonly used for data like MNIST that can be modeled as <a href="http://mathworld.wolfram.com/BernoulliDistribution.html">Bernoulli trials</a>. Expressed as a static method of the <code>VAE</code> class:</p><div><div style="position:relative;margin-bottom:25.784999999999997px;width:602px;margin-left:-20.338749999999997px;margin-right:-20.338749999999997px"><div class="prism-code language-python" style="overflow-x:auto;overflow-y:hidden;background:#f3f3f3"><pre style="float:left;font-size:12.892499999999998px;min-width:100%;position:relative;line-height:1.5;margin:0;padding:12.892499999999998px;overflow:visible"><div class="token-line"><span><span class="token plain">    @</span><span class="token builtin">staticmethod</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">    </span><span class="token keyword">def</span><span class="token plain"> </span><span class="token function">crossEntropy</span><span class="token punctuation">(</span><span class="token plain">obs</span><span class="token punctuation">,</span><span class="token plain"> actual</span><span class="token punctuation">,</span><span class="token plain"> offset</span><span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">7</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">        </span><span class="token triple-quoted-string">&quot;&quot;&quot;Binary cross-entropy, per training example&quot;&quot;&quot;</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">        </span><span class="token comment"># (tf.Tensor, tf.Tensor, float) -&gt; tf.Tensor</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">        </span><span class="token keyword">with</span><span class="token plain"> tf</span><span class="token punctuation">.</span><span class="token plain">name_scope</span><span class="token punctuation">(</span><span class="token string">&quot;cross_entropy&quot;</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">            </span><span class="token comment"># bound by clipping to avoid nan</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">            obs_ </span><span class="token operator">=</span><span class="token plain"> tf</span><span class="token punctuation">.</span><span class="token plain">clip_by_value</span><span class="token punctuation">(</span><span class="token plain">obs</span><span class="token punctuation">,</span><span class="token plain"> offset</span><span class="token punctuation">,</span><span class="token plain"> </span><span class="token number">1</span><span class="token plain"> </span><span class="token operator">-</span><span class="token plain"> offset</span><span class="token punctuation">)</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">            </span><span class="token keyword">return</span><span class="token plain"> </span><span class="token operator">-</span><span class="token plain">tf</span><span class="token punctuation">.</span><span class="token plain">reduce_sum</span><span class="token punctuation">(</span><span class="token plain">actual </span><span class="token operator">*</span><span class="token plain"> tf</span><span class="token punctuation">.</span><span class="token plain">log</span><span class="token punctuation">(</span><span class="token plain">obs_</span><span class="token punctuation">)</span><span class="token plain"> </span><span class="token operator">+</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">                                  </span><span class="token punctuation">(</span><span class="token number">1</span><span class="token plain"> </span><span class="token operator">-</span><span class="token plain"> actual</span><span class="token punctuation">)</span><span class="token plain"> </span><span class="token operator">*</span><span class="token plain"> tf</span><span class="token punctuation">.</span><span class="token plain">log</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token plain"> </span><span class="token operator">-</span><span class="token plain"> obs_</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token plain"> </span><span class="token number">1</span><span class="token punctuation">)</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain"></span></span></div></pre><div style="position:absolute;left:-1px;top:-1px;height:calc(158.7301587301587% + 2px);width:calc(158.7301587301587% + 2px);transform:scale(0.6300000000000001);transform-origin:left top;border:solid 2px black;pointer-events:none"></div></div></div></div><p>The second term in the objective is the KL divergence of the prior <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="base"><span class="mord mathit">p</span></span></span></span></span> from the (approximate) posterior <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="base"><span class="mord mathit" style="margin-right:0.03588em;">q</span></span></span></span></span> over the latent space. We’ll approach this conceptually, then mathematically.</p><p>The KL divergence <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi mathvariant="script">D</mi><mrow><mi>K</mi><mi>L</mi></mrow></msub><mo>(</mo><mi>q</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>p</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">\mathcal{D}_{KL}(q||p)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base"><span class="mord"><span class="mord"><span class="mord mathcal" style="margin-right:0.02778em;">D</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight" style="margin-right:0.07153em;">K</span><span class="mord mathit mtight">L</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"></span></span></span></span></span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.03588em;">q</span><span class="mord">∣</span><span class="mord">∣</span><span class="mord mathit">p</span><span class="mclose">)</span></span></span></span></span> is defined as the relative entropy between probability density functions <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="base"><span class="mord mathit" style="margin-right:0.03588em;">q</span></span></span></span></span> and <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="base"><span class="mord mathit">p</span></span></span></span></span>. In information theory, entropy represents information content (measured in nats), so &lt;InlineMath math=&quot;\mathcal{D}_{KL}&quot; /&quot;&gt; quantifies the information gained by revising the candidate prior <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="base"><span class="mord mathit">p</span></span></span></span></span> to match some “ground truth” <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="base"><span class="mord mathit" style="margin-right:0.03588em;">q</span></span></span></span></span>.</p><p>In a related vein, the KL divergence between posterior and prior beliefs (i.e. distributions) can be conceived as a measure of “<a href="http://ilab.usc.edu/surprise">surprise</a>”: the extent to which the model must update its “worldview” (parameters) to accomodate new observations.</p><p>(Note that the formula is asymmetric—i.e. <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi mathvariant="script">D</mi><mrow><mi>K</mi><mi>L</mi></mrow></msub><mo>(</mo><mi>q</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>p</mi><mo>)</mo><mo>≠</mo><msub><mi mathvariant="script">D</mi><mrow><mi>K</mi><mi>L</mi></mrow></msub><mo>(</mo><mi>p</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>q</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">\mathcal{D}_{KL}(q||p) \neq \mathcal{D}_{KL}(p||q)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base"><span class="mord"><span class="mord"><span class="mord mathcal" style="margin-right:0.02778em;">D</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight" style="margin-right:0.07153em;">K</span><span class="mord mathit mtight">L</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"></span></span></span></span></span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.03588em;">q</span><span class="mord">∣</span><span class="mord">∣</span><span class="mord mathit">p</span><span class="mclose">)</span><span class="mord rule" style="margin-right:0.2777777777777778em;"></span><span class="mrel">≠</span><span class="mord rule" style="margin-right:0.2777777777777778em;"></span><span class="mord"><span class="mord"><span class="mord mathcal" style="margin-right:0.02778em;">D</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight" style="margin-right:0.07153em;">K</span><span class="mord mathit mtight">L</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"></span></span></span></span></span><span class="mopen">(</span><span class="mord mathit">p</span><span class="mord">∣</span><span class="mord">∣</span><span class="mord mathit" style="margin-right:0.03588em;">q</span><span class="mclose">)</span></span></span></span></span>—with <a href="http://www.inference.vc/how-to-train-your-generative-models-why-generative-adversarial-networks-work-so-well-2/">implications</a> for its use in generative models. This is also why it is not a true metric.)</p><p>By inducing the learned approximation <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>q</mi><mi>ϕ</mi></msub><mo>(</mo><mi>z</mi><mi mathvariant="normal">∣</mi><mi>x</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">q_\phi(z|x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1.036108em;vertical-align:-0.286108em;"></span><span class="base"><span class="mord"><span class="mord mathit" style="margin-right:0.03588em;">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight">ϕ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"></span></span></span></span></span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.04398em;">z</span><span class="mord">∣</span><span class="mord mathit">x</span><span class="mclose">)</span></span></span></span></span> (the encoder) to match the continuous imposed prior <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base"><span class="mord mathit" style="margin-right:0.04398em;">z</span></span></span></span></span>, the KL term encourages robustness to small perturbations along the latent manifold, enabling smooth interpolation within and between classes (e.g. MNIST digits). This reduces “spottiness” in the latent space that is often observed in autoencoders without such regularization.</p><p>Mathematical bonus: we can strategically choose certain conjugate priors over <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base"><span class="mord mathit" style="margin-right:0.04398em;">z</span></span></span></span></span> that let us analytically integrate the KL divergence, yielding a closed-form equation. This is true of the spherical Gaussian we chose, such that <div><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mrow><mo>−</mo><mrow><mi mathvariant="script">D</mi></mrow></mrow><mrow><mi>K</mi><mi>L</mi></mrow></msub><mo>(</mo><msub><mi>q</mi><mi>ϕ</mi></msub><mo>(</mo><mi>z</mi><mi mathvariant="normal">∣</mi><mi>x</mi><mo>)</mo><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><msub><mi>p</mi><mi>θ</mi></msub><mo>(</mo><mi>z</mi><mo>)</mo><mo>)</mo><mo>=</mo><mfrac><mrow><mn>1</mn></mrow><mn>2</mn></mfrac><mo>∑</mo><mrow><mo>(</mo><mn>1</mn><mo>+</mo><mi>l</mi><mi>o</mi><mi>g</mi><mo>(</mo><msup><mi>σ</mi><mn>2</mn></msup><mo>)</mo><mo>−</mo><msup><mi>μ</mi><mn>2</mn></msup><mo>−</mo><msup><mi>σ</mi><mn>2</mn></msup><mo>)</mo></mrow></mrow><annotation encoding="application/x-tex">{-\mathcal{D}}_{KL}(q_\phi(z|x) || p_\theta(z)) = \frac{1} 2 \sum{(1 + log(\sigma^2) - \mu^2 - \sigma^2)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:1.32144em;"></span><span class="strut bottom" style="height:2.00744em;vertical-align:-0.686em;"></span><span class="base"><span class="mord"><span class="mord"><span class="mord">−</span><span class="mord"><span class="mord mathcal" style="margin-right:0.02778em;">D</span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight" style="margin-right:0.07153em;">K</span><span class="mord mathit mtight">L</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathit" style="margin-right:0.03588em;">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight">ϕ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"></span></span></span></span></span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.04398em;">z</span><span class="mord">∣</span><span class="mord mathit">x</span><span class="mclose">)</span><span class="mord">∣</span><span class="mord">∣</span><span class="mord"><span class="mord mathit">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"></span></span></span></span></span><span class="mopen">(</span><span class="mord mathit" style="margin-right:0.04398em;">z</span><span class="mclose">)</span><span class="mclose">)</span><span class="mord rule" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mord rule" style="margin-right:0.2777777777777778em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.32144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord">2</span></span><span style="top:-3.15em;"><span class="pstrut" style="height:3em;"></span><span class="stretchy" style="height:0.2em;"><svg width='400em' height='0.2em' viewBox='0 0 400000 200' preserveAspectRatio='xMinYMin slice'><path d='M0 80H400000 v40H0z M0 80H400000 v40H0z'/></svg></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mord rule" style="margin-right:0.16666666666666666em;"></span><span class="mop op-symbol large-op" style="position:relative;top:-0.000004999999999977245em;">∑</span><span class="mord rule" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mopen">(</span><span class="mord">1</span><span class="mord rule" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mord rule" style="margin-right:0.2222222222222222em;"></span><span class="mord mathit" style="margin-right:0.01968em;">l</span><span class="mord mathit">o</span><span class="mord mathit" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord"><span class="mord mathit" style="margin-right:0.03588em;">σ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord rule" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mord rule" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord mathit">μ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mord rule" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mord rule" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord mathit" style="margin-right:0.03588em;">σ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></span></div> (summed over the latent dimensions). In TensorFlow, that looks like this:</p><div><div style="position:relative;margin-bottom:25.784999999999997px;width:602px;margin-left:-20.338749999999997px;margin-right:-20.338749999999997px"><div class="prism-code language-python" style="overflow-x:auto;overflow-y:hidden;background:#f3f3f3"><pre style="float:left;font-size:12.892499999999998px;min-width:100%;position:relative;line-height:1.5;margin:0;padding:12.892499999999998px;overflow:visible"><div class="token-line"><span><span class="token plain">    @</span><span class="token builtin">staticmethod</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">    </span><span class="token keyword">def</span><span class="token plain"> </span><span class="token function">kullbackLeibler</span><span class="token punctuation">(</span><span class="token plain">mu</span><span class="token punctuation">,</span><span class="token plain"> log_sigma</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">        </span><span class="token triple-quoted-string">&quot;&quot;&quot;(Gaussian) Kullback-Leibler divergence KL(q||p), per training example&quot;&quot;&quot;</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">        </span><span class="token comment"># (tf.Tensor, tf.Tensor) -&amp;gt; tf.Tensor</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">        </span><span class="token keyword">with</span><span class="token plain"> tf</span><span class="token punctuation">.</span><span class="token plain">name_scope</span><span class="token punctuation">(</span><span class="token string">&quot;KL_divergence&quot;</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">            </span><span class="token comment"># = -0.5 * (1 + log(sigma**2) - mu**2 - sigma**2)</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">            </span><span class="token keyword">return</span><span class="token plain"> </span><span class="token operator">-</span><span class="token number">0.5</span><span class="token plain"> </span><span class="token operator">*</span><span class="token plain"> tf</span><span class="token punctuation">.</span><span class="token plain">reduce_sum</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token plain"> </span><span class="token operator">+</span><span class="token plain"> </span><span class="token number">2</span><span class="token plain"> </span><span class="token operator">*</span><span class="token plain"> log_sigma </span><span class="token operator">-</span><span class="token plain"> mu</span><span class="token operator">**</span><span class="token number">2</span><span class="token plain"> </span><span class="token operator">-</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">                                        tf</span><span class="token punctuation">.</span><span class="token plain">exp</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token plain"> </span><span class="token operator">*</span><span class="token plain"> log_sigma</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token plain"> </span><span class="token number">1</span><span class="token punctuation">)</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain"></span></span></div></pre><div style="position:absolute;left:-1px;top:-1px;height:calc(158.7301587301587% + 2px);width:calc(158.7301587301587% + 2px);transform:scale(0.6300000000000001);transform-origin:left top;border:solid 2px black;pointer-events:none"></div></div></div></div><p><p>Together, these complementary loss terms capture the trade-off between expressivity and concision, between data complexity and simplicity of the prior. Reconstruction loss pushes the model toward perfectionist tendencies, while KL loss (along with the addition of auxiliary noise) encourages it to explore sensibly.</p>
<p>To elaborate (building on the <code>VAE._buildGraph</code> method started above):</p></p><div><div style="position:relative;margin-bottom:25.784999999999997px;width:602px;margin-left:-20.338749999999997px;margin-right:-20.338749999999997px"><div class="prism-code language-python" style="overflow-x:auto;overflow-y:hidden;background:#f3f3f3"><pre style="float:left;font-size:12.892499999999998px;min-width:100%;position:relative;line-height:1.5;margin:0;padding:12.892499999999998px;overflow:visible"><div class="token-line"><span><span class="token plain">        </span><span class="token comment"># reconstruction loss: mismatch b/w x &amp;amp; x_reconstructed</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">        </span><span class="token comment"># binary cross-entropy -- assumes p(x) &amp;amp; p(x|z) are iid Bernoullis</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">        rec_loss </span><span class="token operator">=</span><span class="token plain"> VAE</span><span class="token punctuation">.</span><span class="token plain">crossEntropy</span><span class="token punctuation">(</span><span class="token plain">x_reconstructed</span><span class="token punctuation">,</span><span class="token plain"> x_in</span><span class="token punctuation">)</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">        </span><span class="token comment"># Kullback-Leibler divergence: mismatch b/w approximate posterior &amp;amp; imposed prior</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">        </span><span class="token comment"># KL[q(z|x) || p(z)]</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">        kl_loss </span><span class="token operator">=</span><span class="token plain"> VAE</span><span class="token punctuation">.</span><span class="token plain">kullbackLeibler</span><span class="token punctuation">(</span><span class="token plain">z_mean</span><span class="token punctuation">,</span><span class="token plain"> z_log_sigma</span><span class="token punctuation">)</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">        </span><span class="token comment"># average over minibatch</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">        cost </span><span class="token operator">=</span><span class="token plain"> tf</span><span class="token punctuation">.</span><span class="token plain">reduce_mean</span><span class="token punctuation">(</span><span class="token plain">rec_loss </span><span class="token operator">+</span><span class="token plain"> kl_loss</span><span class="token punctuation">,</span><span class="token plain"> name</span><span class="token operator">=</span><span class="token string">&quot;cost&quot;</span><span class="token punctuation">)</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain"></span></span></div></pre><div style="position:absolute;left:-1px;top:-1px;height:calc(158.7301587301587% + 2px);width:calc(158.7301587301587% + 2px);transform:scale(0.6300000000000001);transform-origin:left top;border:solid 2px black;pointer-events:none"></div></div></div></div><p><p>Beyond its concise elegance and solid grounding in Bayesian theory, the cost function lends itself well to intuitive metaphor:</p>
<p>Information theory-wise, the VAE is a terse game of Telephone, with the aim of finding the <em>minimum description length</em> to convey the input from end to end. Here, reconstruction loss is the information “lost in translation,” while KL loss captures how overly “wordy” the model must be to convey the message through an unpredictable medium (hidden code imperfectly optimized for the input data).</p>
<p>Or, framing the VAE as a lossy compression algorithm, reconstruction loss accounts for the fidelity of (de)compression while KL loss penalizes the model for using a sub-optimal compression scheme.</p>
<h2 id="training">Training</h2>
<p>At last, our VAE cost function in hand (after factoring in optional <a href="http://cs231n.github.io/neural-networks-2/#regularization"><span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi mathvariant="normal">ℓ</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">\ell_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="base"><span class="mord"><span class="mord">ℓ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"></span></span></span></span></span></span></span></span></span>-regularization</a>), we finish <code>VAE._buildGraph</code> with optimization nodes to be evaluated at each step of SGD (with the <a href="https://arxiv.org/abs/1412.6980">Adam</a> optimizer)…</p></p><div><div style="position:relative;margin-bottom:25.784999999999997px;width:602px;margin-left:-20.338749999999997px;margin-right:-20.338749999999997px"><div class="prism-code language-python" style="overflow-x:auto;overflow-y:hidden;background:#f3f3f3"><pre style="float:left;font-size:12.892499999999998px;min-width:100%;position:relative;line-height:1.5;margin:0;padding:12.892499999999998px;overflow:visible"><div class="token-line"><span><span class="token plain">        </span><span class="token comment"># optimization</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">        global_step </span><span class="token operator">=</span><span class="token plain"> tf</span><span class="token punctuation">.</span><span class="token plain">Variable</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token plain"> trainable</span><span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">        </span><span class="token keyword">with</span><span class="token plain"> tf</span><span class="token punctuation">.</span><span class="token plain">name_scope</span><span class="token punctuation">(</span><span class="token string">&quot;Adam_optimizer&quot;</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">            optimizer </span><span class="token operator">=</span><span class="token plain"> tf</span><span class="token punctuation">.</span><span class="token plain">train</span><span class="token punctuation">.</span><span class="token plain">AdamOptimizer</span><span class="token punctuation">(</span><span class="token plain">self</span><span class="token punctuation">.</span><span class="token plain">learning_rate</span><span class="token punctuation">)</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">            tvars </span><span class="token operator">=</span><span class="token plain"> tf</span><span class="token punctuation">.</span><span class="token plain">trainable_variables</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">            grads_and_vars </span><span class="token operator">=</span><span class="token plain"> optimizer</span><span class="token punctuation">.</span><span class="token plain">compute_gradients</span><span class="token punctuation">(</span><span class="token plain">cost</span><span class="token punctuation">,</span><span class="token plain"> tvars</span><span class="token punctuation">)</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">            clipped </span><span class="token operator">=</span><span class="token plain"> </span><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token plain">tf</span><span class="token punctuation">.</span><span class="token plain">clip_by_value</span><span class="token punctuation">(</span><span class="token plain">grad</span><span class="token punctuation">,</span><span class="token plain"> </span><span class="token operator">-</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token plain"> </span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token plain"> tvar</span><span class="token punctuation">)</span><span class="token plain"> </span><span class="token comment"># gradient clipping</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">                    </span><span class="token keyword">for</span><span class="token plain"> grad</span><span class="token punctuation">,</span><span class="token plain"> tvar </span><span class="token keyword">in</span><span class="token plain"> grads_and_vars</span><span class="token punctuation">]</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">            train_op </span><span class="token operator">=</span><span class="token plain"> optimizer</span><span class="token punctuation">.</span><span class="token plain">apply_gradients</span><span class="token punctuation">(</span><span class="token plain">clipped</span><span class="token punctuation">,</span><span class="token plain"> global_step</span><span class="token operator">=</span><span class="token plain">global_step</span><span class="token punctuation">,</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">                                                 name</span><span class="token operator">=</span><span class="token string">&quot;minimize_cost&quot;</span><span class="token punctuation">)</span><span class="token plain"> </span><span class="token comment"># back-prop</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain"></span></span></div></pre><div style="position:absolute;left:-1px;top:-1px;height:calc(158.7301587301587% + 2px);width:calc(158.7301587301587% + 2px);transform:scale(0.6300000000000001);transform-origin:left top;border:solid 2px black;pointer-events:none"></div></div></div></div><p>…and return all of the nodes we want to access in the future to the <code>VAE.__init__</code> method where <code>buildGraph</code> was called.</p><div><div style="position:relative;margin-bottom:25.784999999999997px;width:602px;margin-left:-20.338749999999997px;margin-right:-20.338749999999997px"><div class="prism-code language-python" style="overflow-x:auto;overflow-y:hidden;background:#f3f3f3"><pre style="float:left;font-size:12.892499999999998px;min-width:100%;position:relative;line-height:1.5;margin:0;padding:12.892499999999998px;overflow:visible"><div class="token-line"><span><span class="token plain">        </span><span class="token keyword">return</span><span class="token plain"> </span><span class="token punctuation">(</span><span class="token plain">x_in</span><span class="token punctuation">,</span><span class="token plain"> dropout</span><span class="token punctuation">,</span><span class="token plain"> z_mean</span><span class="token punctuation">,</span><span class="token plain"> z_log_sigma</span><span class="token punctuation">,</span><span class="token plain"> x_reconstructed</span><span class="token punctuation">,</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">                z_</span><span class="token punctuation">,</span><span class="token plain"> x_reconstructed_</span><span class="token punctuation">,</span><span class="token plain"> cost</span><span class="token punctuation">,</span><span class="token plain"> global_step</span><span class="token punctuation">,</span><span class="token plain"> train_op</span><span class="token punctuation">)</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain"></span></span></div></pre><div style="position:absolute;left:-1px;top:-1px;height:calc(158.7301587301587% + 2px);width:calc(158.7301587301587% + 2px);transform:scale(0.6300000000000001);transform-origin:left top;border:solid 2px black;pointer-events:none"></div></div></div></div><p>Using SGD to optimize the function parameters of the inference and generative networks simultaneously is called <em>Stochastic Gradient Variational Bayes</em>.</p><p>This is where TensorFlow really shines: all of the gradient backpropagation and parameter updates are performed via automatic differentation, and abstracted away from the researcher in the <code>train_op</code> (essentially) one-liner on line <code>48</code>.</p><p>Model training (with optional cross-validation) is then as simple as feeding minibatches from dataset <code>X</code> to the <code>x_in</code> placeholder and evaluating (“fetching”) the <code>train_op</code>. Here are some relevant chunks, excerpted from the <a href="https://github.com/fastforwardlabs/vae-tf/blob/master/vae.py#L216-L295">full class method</a>:</p><div><div style="position:relative;margin-bottom:25.784999999999997px;width:602px;margin-left:-20.338749999999997px;margin-right:-20.338749999999997px"><div class="prism-code language-python" style="overflow-x:auto;overflow-y:hidden;background:#f3f3f3"><pre style="float:left;font-size:12.892499999999998px;min-width:100%;position:relative;line-height:1.5;margin:0;padding:12.892499999999998px;overflow:visible"><div class="token-line"><span><span class="token plain">   </span><span class="token keyword">def</span><span class="token plain"> </span><span class="token function">train</span><span class="token punctuation">(</span><span class="token plain">self</span><span class="token punctuation">,</span><span class="token plain"> X</span><span class="token punctuation">,</span><span class="token plain"> max_iter</span><span class="token operator">=</span><span class="token plain">np</span><span class="token punctuation">.</span><span class="token plain">inf</span><span class="token punctuation">,</span><span class="token plain"> max_epochs</span><span class="token operator">=</span><span class="token plain">np</span><span class="token punctuation">.</span><span class="token plain">inf</span><span class="token punctuation">,</span><span class="token plain"> cross_validate</span><span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">             verbose</span><span class="token operator">=</span><span class="token plain">rue</span><span class="token punctuation">,</span><span class="token plain"> save</span><span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span><span class="token plain"> outdir</span><span class="token operator">=</span><span class="token string">&quot;./out&quot;</span><span class="token punctuation">,</span><span class="token plain"> plots_outdir</span><span class="token operator">=</span><span class="token string">&quot;./png&quot;</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">       </span><span class="token keyword">try</span><span class="token punctuation">:</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">           err_train </span><span class="token operator">=</span><span class="token plain"> </span><span class="token number">0</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">           now </span><span class="token operator">=</span><span class="token plain"> datetime</span><span class="token punctuation">.</span><span class="token plain">now</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token plain">isoformat</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">11</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">           </span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;------- Training begin: {} -------\n&quot;</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span><span class="token plain">now</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">           </span><span class="token keyword">while</span><span class="token plain"> </span><span class="token boolean">True</span><span class="token punctuation">:</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">               x</span><span class="token punctuation">,</span><span class="token plain"> _ </span><span class="token operator">=</span><span class="token plain"> X</span><span class="token punctuation">.</span><span class="token plain">train</span><span class="token punctuation">.</span><span class="token plain">next_batch</span><span class="token punctuation">(</span><span class="token plain">self</span><span class="token punctuation">.</span><span class="token plain">batch_size</span><span class="token punctuation">)</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">               feed_dict </span><span class="token operator">=</span><span class="token plain"> </span><span class="token punctuation">{</span><span class="token plain">self</span><span class="token punctuation">.</span><span class="token plain">x_in</span><span class="token punctuation">:</span><span class="token plain"> x</span><span class="token punctuation">,</span><span class="token plain"> self</span><span class="token punctuation">.</span><span class="token plain">dropout_</span><span class="token punctuation">:</span><span class="token plain"> self</span><span class="token punctuation">.</span><span class="token plain">dropout</span><span class="token punctuation">}</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">               fetches </span><span class="token operator">=</span><span class="token plain"> </span><span class="token punctuation">[</span><span class="token plain">self</span><span class="token punctuation">.</span><span class="token plain">x_reconstructed</span><span class="token punctuation">,</span><span class="token plain"> self</span><span class="token punctuation">.</span><span class="token plain">cost</span><span class="token punctuation">,</span><span class="token plain"> self</span><span class="token punctuation">.</span><span class="token plain">global_step</span><span class="token punctuation">,</span><span class="token plain"> self</span><span class="token punctuation">.</span><span class="token plain">train_op</span><span class="token punctuation">]</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">               x_reconstructed</span><span class="token punctuation">,</span><span class="token plain"> cost</span><span class="token punctuation">,</span><span class="token plain"> i</span><span class="token punctuation">,</span><span class="token plain"> _ </span><span class="token operator">=</span><span class="token plain"> self</span><span class="token punctuation">.</span><span class="token plain">sesh</span><span class="token punctuation">.</span><span class="token plain">run</span><span class="token punctuation">(</span><span class="token plain">fetches</span><span class="token punctuation">,</span><span class="token plain"> feed_dict</span><span class="token punctuation">)</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">               err_train </span><span class="token operator">+=</span><span class="token plain"> cost</span></span></div><div class="token-line"><span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">               </span><span class="token keyword">if</span><span class="token plain"> i</span><span class="token operator">%</span><span class="token number">1000</span><span class="token plain"> </span><span class="token operator">==</span><span class="token plain"> </span><span class="token number">0</span><span class="token plain"> </span><span class="token operator">and</span><span class="token plain"> verbose</span><span class="token punctuation">:</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">                   </span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;round {} --&gt; avg cost: &quot;</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span><span class="token plain">i</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token plain"> err_train </span><span class="token operator">/</span><span class="token plain"> i</span><span class="token punctuation">)</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">               </span><span class="token keyword">if</span><span class="token plain"> i </span><span class="token operator">&gt;=</span><span class="token plain"> max_iter </span><span class="token operator">or</span><span class="token plain"> X</span><span class="token punctuation">.</span><span class="token plain">train</span><span class="token punctuation">.</span><span class="token plain">epochs_completed </span><span class="token operator">&amp;</span><span class="token plain">gt</span><span class="token punctuation">;</span><span class="token operator">=</span><span class="token plain"> max_epochs</span><span class="token punctuation">:</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">                   </span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;final avg cost (@ step {} = epoch {}): {}&quot;</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">                       i</span><span class="token punctuation">,</span><span class="token plain"> X</span><span class="token punctuation">.</span><span class="token plain">train</span><span class="token punctuation">.</span><span class="token plain">epochs_completed</span><span class="token punctuation">,</span><span class="token plain"> err_train </span><span class="token operator">/</span><span class="token plain"> i</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">                   now </span><span class="token operator">=</span><span class="token plain"> datetime</span><span class="token punctuation">.</span><span class="token plain">now</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token plain">isoformat</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">11</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">                   </span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;------- Training end: {} -------\n&quot;</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span><span class="token plain">now</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain">                   </span><span class="token keyword">break</span><span class="token plain"></span></span></div><div class="token-line"><span><span class="token plain"></span></span></div></pre><div style="position:absolute;left:-1px;top:-1px;height:calc(158.7301587301587% + 2px);width:calc(158.7301587301587% + 2px);transform:scale(0.6300000000000001);transform-origin:left top;border:solid 2px black;pointer-events:none"></div></div></div></div><p>Helpfully, TensorFlow comes with a built-in <a href="https://www.tensorflow.org/versions/r0.10/how_tos/summaries_and_tensorboard/index.html">visualization dashboard</a>. Here’s the computational graph for an end-to-end VAE with two hidden encoder/decoder layers (that’s what all the <code>tf.name_scope</code>-ing was for):</p><p><div style="position:relative"><img src="http://fastforwardlabs.github.io/blog-images/miriam/imgs_code/tensorboard.png" alt="Tensorboard" style="display:block;margin:0;width:100%"/></div></p><h2>Wrapping Up</h2><p>The future of deep latent models lies in models that can reason about the world—“understanding” complex observations, transforming them into meaningful internal representations, and even leveraging these representations to make decisions—all while coping with scarce data, and in semisupervised or unsupervised settings. VAEs are an important step toward this future, demonstrating the power of new ways of thinking that result from unifying variational Bayesian methods and deep learning.</p><p>We now understand how these fields come together to make the VAE possible, through a theoretically-sound objective function that balances accuracy (reconstruction loss) with variational regularization (KL loss), and efficient optimization of the fully differentiable model thanks to the reparameterization trick.</p><p>We’ll wrap up for now with one more way of visualizing the condensed information encapsulated in VAE latent space.</p><p><a href="http://blog.fastforwardlabs.com/2016/08/12/introducing-variational-autoencoders-in-prose-and.html">Previously</a>, we showed the correspondence between the inference and generative networks by plotting the encoder and decoder perspectives of the latent space in the same 2-D coordinate system. For the decoder perspective, this meant feeding linearly spaced latent coordinates to the generative network and plotting their corresponding outputs.</p><p>To get an undistorted sense of the full latent manifold, we can sample and decode latent space coordinates proportionally to the model’s distribution over latent space. In other words—thanks to variational regularization provided by the KL loss!—we simply sample relative to our chosen prior distribution over <span><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.43056em;vertical-align:0em;"></span><span class="base"><span class="mord mathit" style="margin-right:0.04398em;">z</span></span></span></span></span>. In our case, this means sampling linearly spaced percentiles from the <a href="http://work.thaslwanter.at/Stats/html/statsDistributions.html#other-important-presentations-of-probability-densities">inverse CDF</a> of a spherical Gaussian.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a></p><p><p>Once again, evolving over (logarithmic) time:</p></p><p><div style="position:relative"><img src="http://fastforwardlabs.github.io/blog-images/miriam/imgs_code/160816_1813_latent_784_500_500_2_explore_ppf30_16.0.500x500.slower47.gif" alt="Decoder&#x27;s undistorted view of latent space, over training" style="display:block;margin:0;width:100%"/></div></p><p><p>Interestingly, we can see that the slim tails of the distribution (edges of the frame) are not well-formed. Presumably, this results from few observed inputs being mapped to latent posteriors with significant density in these regions.</p></p><p><p>Here are a few resulting constellations (from a single model):</p></p><p><div style="position:relative"><img src="http://fastforwardlabs.github.io/blog-images/miriam/imgs_code/160805_1646_reloaded_latent_784_500_500_2_round_131072_explore_ppf_30.png" alt="Generative view of latent space: 30x30" style="display:block;margin:0;width:100%"/></div></p><p><div style="position:relative"><img src="http://fastforwardlabs.github.io/blog-images/miriam/imgs_code/160805_1646_reloaded_latent_784_500_500_2_round_131072_explore_ppf_60.png" alt="Generative view of latent space 60x60" style="display:block;margin:0;width:100%"/></div></p><p><div style="position:relative"><img src="http://fastforwardlabs.github.io/blog-images/miriam/imgs_code/160805_1646_reloaded_latent_784_500_500_2_round_131072_explore_ppf_100.png" alt="Generative view of latent space: 100x100" style="display:block;margin:0;width:100%"/></div></p><p><p>Theoretically, we could subdivide the latent space into infinitely many points (limited in practice only by the computer’s floating point precision), and let the generative network dream up infinite constellations of creative variations on MNIST.</p></p><p><p>That’s enough digits for now! Keep your eyes out for the next installment, where we’ll tinker with the vanilla VAE model in the context of a new dataset.</p></p><p><p>– Miriam</p></p><div class="footnotes"><ol><li id="fn1"><p>Thanks Kyle McDonald (<span class="citation">@kcimc</span>) and Tom White (<span class="citation">@dribnet</span>) for noting this! <a href="#fnref1">↩</a></p></li></ol></div><div style="display:none;justify-content:center;padding:12.892499999999998px" class="jsx-1135431938 jsx-2959859206"><img style="height:18.75272727272727px;width:18.75272727272727px;position:relative;display:block" src="/static/images/ff.png" class="jsx-1135431938 jsx-2959859206"/></div></div></div></div><div style="position:relative;display:block;width:612.8924999999999px;grid-template-columns:repeat(2, 1fr);margin-left:-12.892499999999998px;margin-right:-12.892499999999998px"><div style="display:grid"><div style="position:relative;display:grid;grid-template-rows:auto 1fr;margin-bottom:12.892499999999998px"><div style="text-transform:uppercase;font-size:12.892499999999998px;letter-spacing:0.03em;line-height:1.5;padding-bottom:6.446249999999999px;padding-left:25.784999999999997px">Newer</div><div style="position:relative;display:grid"><a class="hover-box no-underline no-hover gray-backer hover-extend" style="display:block;position:relative;width:612.8925px;padding-left:12.892499999999998px;padding-right:12.892499999999998px;margin-left:0;margin-right:0" href="/2016/08/24/next-economics-interview-with-jimi-crawford.html"><div style="display:flex;flex-wrap:wrap"><div style="display:flex;width:587.1075px;font-size:12.892499999999998px;letter-spacing:0.03em;text-transform:uppercase;line-height:1.5;margin-left:0"><div style="width:293.55375px;padding:12.892499999999998px 12.892499999999998px 0px 12.892499999999998px"><div>Aug 04 2016</div></div><div style="width:293.55375px;padding:12.892499999999998px 12.892499999999998px 0px 12.892499999999998px"><div>interview</div></div></div><div style="position:relative;width:587.1075px;padding:12.892499999999998px 0px"><div style="font-size:25.784999999999997px;line-height:1.25;padding:0px 12.892499999999998px;margin-top:-6.446249999999999px"><div style="padding-left:0">Next Economics: Interview with Jimi Crawford</div></div><div style="padding-top:6.446249999999999px;position:relative"><div style="font-size:15.041249999999998px;line-height:1.5em;display:flex;flex-wrap:wrap;padding-left:0"><div style="padding:0px 12.892499999999998px;width:293.55375px"><div style="height:90.24749999999999px;overflow:hidden;display:-webkit-box;-webkit-line-clamp:4;hyphens:auto;-webkit-box-orient:vertical">
Building shadows as proxies for construction rates in Shanghai. Photos courtesy of Orbital Insight/Digital Globe.

It’s no small feat to commercialize new technologies that arise from scientific and academic research. The useful is a small subset of the possible, and the...</div><div style="overflow:hidden;width:100%;text-overflow:ellipsis;white-space:nowrap"><span class="display-link"><span>View<!-- --> <span style="text-transform:lowercase">interview</span></span></span></div></div><div style="position:relative;width:293.55375px"><div style="width:calc(100% - 25.784999999999997px);height:calc(100% - 12.892499999999998px);margin-left:12.892499999999998px;margin-top:6.446249999999999px;background-image:url(http://68.media.tumblr.com/de9fc93bc4bda585c1bb51a785fc7801/tumblr_inline_ocf6npUoHK1ta78fg_540.png);background-size:contain;background-position:center center;background-repeat:no-repeat"></div></div></div></div></div></div></a><div style="position:absolute;left:-1px;top:-1px;height:calc(158.7301587301587% + 2px);width:calc(158.7301587301587% + 2px);transform:scale(0.6300000000000001);transform-origin:left top;border:solid 2px black;pointer-events:none"></div></div></div></div><div style="display:grid"><div style="position:relative;display:grid;grid-template-rows:auto 1fr;margin-bottom:12.892499999999998px"><div style="text-transform:uppercase;font-size:12.892499999999998px;letter-spacing:0.03em;line-height:1.5;padding-bottom:6.446249999999999px;padding-left:25.784999999999997px">Older</div><div style="position:relative;display:grid"><a class="hover-box no-underline no-hover gray-backer hover-extend" style="display:block;position:relative;width:612.8925px;padding-left:12.892499999999998px;padding-right:12.892499999999998px;margin-left:0;margin-right:0" href="/2016/08/18/giving-speech-a-voice-in-the-home.html"><div style="display:flex;flex-wrap:wrap"><div style="display:flex;width:587.1075px;font-size:12.892499999999998px;letter-spacing:0.03em;text-transform:uppercase;line-height:1.5;margin-left:0"><div style="width:293.55375px;padding:12.892499999999998px 12.892499999999998px 0px 12.892499999999998px"><div>Aug 05 2016</div></div><div style="width:293.55375px;padding:12.892499999999998px 12.892499999999998px 0px 12.892499999999998px"><div>Guest Post</div></div></div><div style="position:relative;width:587.1075px;padding:12.892499999999998px 0px"><div style="font-size:25.784999999999997px;line-height:1.25;padding:0px 12.892499999999998px;margin-top:-6.446249999999999px"><div style="padding-left:0">Giving Speech a Voice in the Home</div></div><div style="padding-top:6.446249999999999px;position:relative"><div style="font-size:15.041249999999998px;line-height:1.5em;display:flex;flex-wrap:wrap;padding-left:0"><div style="padding:0px 12.892499999999998px;width:293.55375px"><div style="height:90.24749999999999px;overflow:hidden;display:-webkit-box;-webkit-line-clamp:4;hyphens:auto;-webkit-box-orient:vertical"><span>by <!-- -->Sean Lorenz, SENTER<!-- --> ◦ </span>
This is a guest post by Sean Lorenz, the Founder &amp;amp; CEO of SENTER, a Boston-based startup using sensors and data science to support healthcare in the home. Sean explains how techniques from computational neuroscience can help make the smart home smarter and describes the...</div><div style="overflow:hidden;width:100%;text-overflow:ellipsis;white-space:nowrap"><span class="display-link"><span>View<!-- --> <span style="text-transform:lowercase">Guest Post</span></span></span></div></div><div style="position:relative;width:293.55375px"><div style="width:calc(100% - 25.784999999999997px);height:calc(100% - 12.892499999999998px);margin-left:12.892499999999998px;margin-top:6.446249999999999px;background-image:url(http://68.media.tumblr.com/24baab55961a73824bb7c218d3fb3d52/tumblr_inline_oc4fofyCUx1ta78fg_540.png);background-size:contain;background-position:center center;background-repeat:no-repeat"></div></div></div></div></div></div></a><div style="position:absolute;left:-1px;top:-1px;height:calc(158.7301587301587% + 2px);width:calc(158.7301587301587% + 2px);transform:scale(0.6300000000000001);transform-origin:left top;border:solid 2px black;pointer-events:none"></div></div></div></div></div><div style="padding-bottom:25.784999999999997px;padding-top:25.784999999999997px" class="jsx-1135431938 jsx-2959859206"><div style="font-size:34.379999999999995px;line-height:1.25;padding:0px 12.892499999999998px 0px 12.892499999999998px;margin-bottom:12.892499999999998px;width:587.1075px;margin-left:0" class="jsx-1135431938 jsx-2959859206">About</div><div style="padding:0px 12.892499999999998px;margin-bottom:12.892499999999998px;width:587.1075px;margin-left:0;font-size:17.189999999999998px;line-height:1.5" class="jsx-1135431938 jsx-2959859206">Cloudera Fast Forward Labs is an applied machine learning research group. We help organizations recognize and develop new product and business opportunities through emerging technologies.<!-- --> </div><div style="padding:0px 12.892499999999998px;margin-bottom:12.892499999999998px;width:587.1075px;margin-left:0;font-size:17.189999999999998px;line-height:1.5" class="jsx-1135431938 jsx-2959859206"><a href="https://www.cloudera.com/products/fast-forward-labs-research.html" class="jsx-1135431938 jsx-2959859206">Learn more about working with us.</a></div></div></div><div class="jsx-1135431938 jsx-2959859206"><div style="position:relative" class="jsx-1135431938 jsx-2959859206"><div style="position:absolute;left:0;width:100%;height:2px;transform:scaleY(0.60165);transform-origin:center center;background:black;top:-1px"></div><div style="padding:12.892499999999998px;display:flex;justify-content:space-between;flex-wrap:wrap;font-size:17.189999999999998px;line-height:1.5" class="jsx-1135431938 jsx-2959859206"><div class="jsx-1135431938 jsx-2959859206"><a href="/" class="jsx-1135431938 jsx-2959859206">Blog</a></div><div style="display:flex;flex-wrap:wrap" class="jsx-1135431938 jsx-2959859206"><div style="margin-right:12.892499999999998px" class="jsx-1135431938 jsx-2959859206"><a href="https://www.cloudera.com/products/fast-forward-labs-research.html" class="jsx-1135431938 jsx-2959859206">Cloudera</a></div><div style="margin-right:12.892499999999998px" class="jsx-1135431938 jsx-2959859206"><a href="https://blog.fastforwardlabs.com/" class="jsx-1135431938 jsx-2959859206">AI Experiments</a></div><div class="jsx-1135431938 jsx-2959859206"><a href="https://twitter.com/fastforwardlabs" class="jsx-1135431938 jsx-2959859206">Twitter</a></div></div></div></div></div></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{}},"page":"/posts/2016-08-22-under-the-hood-of-the-variational-autoencoder-in","query":{},"buildId":"EnU~FBvqy_55fWO5RkIyH","nextExport":true}</script><script async="" id="__NEXT_PAGE__/posts/2016-08-22-under-the-hood-of-the-variational-autoencoder-in" src="/_next/static/EnU~FBvqy_55fWO5RkIyH/pages/posts/2016-08-22-under-the-hood-of-the-variational-autoencoder-in.js"></script><script async="" id="__NEXT_PAGE__/_app" src="/_next/static/EnU~FBvqy_55fWO5RkIyH/pages/_app.js"></script><script src="/_next/static/runtime/webpack-fdce77a122c11e06ae50.js" async=""></script><script src="/_next/static/chunks/commons.f77b50de979bfbbb280b.js" async=""></script><script src="/_next/static/runtime/main-5ab107747766f1b4e0bf.js" async=""></script></body></html>