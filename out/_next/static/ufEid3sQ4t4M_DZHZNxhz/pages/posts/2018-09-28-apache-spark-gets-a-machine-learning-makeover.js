(window.webpackJsonp=window.webpackJsonp||[]).push([["a1f7"],{"3+vP":function(e,a,t){(window.__NEXT_P=window.__NEXT_P||[]).push(["/posts/2018-09-28-apache-spark-gets-a-machine-learning-makeover",function(){var e=t("l2pp");return{page:e.default||e}}])},l2pp:function(e,a,t){"use strict";t.r(a),t.d(a,"frontMatter",function(){return s}),t.d(a,"default",function(){return c});var n=t("kOwS"),i=t("qNsG"),r=(t("q1tI"),t("E/Ix")),s={title:"Apache Spark gets a machine learning makeover",date:"2018-09-28 17:10 -0400",preview_image:"/images/editor_uploads/2018-09-19-194511-XOnSpark.png",feature:!1,published:!0,author:"Seth",author_link:"https://twitter.com/shendrickson16",post_type:"newsletter"},o={frontMatter:s},l="wrapper";function c(e){var a=e.components,t=Object(i.a)(e,["components"]);return Object(r.b)(l,Object(n.a)({},o,t,{components:a,mdxType:"MDXLayout"}),Object(r.b)("h3",null,"Machine learning on Spark: an abridged history"),Object(r.b)("p",null,Object(r.b)("a",Object(n.a)({parentName:"p"},{href:"https://spark.apache.org/"}),"Apache Spark")," - the cluster computing framework\nthat's been throwing shade at MapReduce since 2011 - has always been a bit\nremarkable, because it bridged the divide between data engineering and data\nscience. One of the great promises of Spark was that it would be easy,\ntrivial almost, to scale machine learning and data science to arbitrarily\nlarge data. Seven years later, Spark has made its place in data science, but\nperhaps not in the way we originally hoped."),Object(r.b)("p",null,"Spark's big contribution was that it delivered a very elegant API for dealing\nwith distributed collections of data, termed ",Object(r.b)("em",{parentName:"p"},"Resilient Distributed Datasets"),"\n(RDDs). Compared to alternatives at the time, it was simple to use that API\nto write certain machine learning algorithms, and since those algorithms were\nbuilt on RDDs; you got fault tolerance and scale for free. It wasn't long\nuntil a machine learning library built on RDDs was born: ",Object(r.b)("a",Object(n.a)({parentName:"p"},{href:"https://spark.apache.org/docs/latest/ml-guide.html"}),"MLlib"),". "),Object(r.b)("p",null,"Implementing performant, scalable machine learning algorithms in MLlib wasn't\n",Object(r.b)("em",{parentName:"p"},"quite")," as easy as just expressing the logic using RDD transformations, but in\nsome cases it worked quite well. Spark, and by extension MLlib, work well\nwhen algorithms can be expressed in parallel, independent tasks that each\nwork on independent chunks of data. Accordingly, MLlib has seen success and\nadoption with linear models, K-means clustering, decision trees, and some\nothers. But some algorithms, most notably deep learning, are difficult to\nexpress using Spark. "),Object(r.b)("p",null,"In comparison to linear models, optimizing deep learning algorithms over\ndistributed collections requires frequent communication between tasks.\nFurther, deep learning is ",Object(r.b)("em",{parentName:"p"},"slow"),", if you don't use a framework that has been\nheavily optimized for that exact use case. Tensorflow, PyTorch, MXNet, etc.\nall leverage accelerated hardware and heavily optimized C/C++ code to achieve\nreasonable efficiency. All this is to say that Spark and deep learning aren't\na very good match.  So why are we talking about it?"),Object(r.b)("p",null,'Deep learning needs data (big data!) and that data often needs to be accessed\nthrough or pre-processed by Spark. That data is also messy and is probably\nstored across many datasets in many different storage platforms. Spark makes\nreading, aggregating, and joining these datasets less awful. So even if Spark\nisn\'t heavily optimized for machine learning, the data that feeds these\nalgorithms often goes through Spark first. This reality led many developers\nto ponder, "what if we could combine the heavily optimized ML/DL frameworks\ninto Spark?" And with that, the family of XOnSpark libraries came to be.'),Object(r.b)("p",null,Object(r.b)("img",Object(n.a)({parentName:"p"},{src:"/images/editor_uploads/2018-09-19-194511-XOnSpark.png",alt:null}))),Object(r.b)("p",null,"But Spark hasn't made it very easy to combine these other libraries, most of\nwhich are written in a combination of Python and C++, with Spark. There are\nthree main shortcomings:"),Object(r.b)("ul",null,Object(r.b)("li",{parentName:"ul"},"Moving data between Spark processes (JVM) and Python processes is inefficient"),Object(r.b)("li",{parentName:"ul"},"Spark task scheduling doesn't take GPUs into account"),Object(r.b)("li",{parentName:"ul"},"Deep learning tasks need to constantly communicate gradient/weight updates between them, which is a Spark anti-pattern")),Object(r.b)("h3",null,"Project Hydrogen makes Spark play nice with other ML frameworks"),Object(r.b)("p",null,"To address each of these issues, the open source community for Spark is\nundertaking a new initiative, dubbed ",Object(r.b)("em",{parentName:"p"},"Project Hydrogen"),", which is\nbroken up into three main chunks, each designed to solve one of these\nshortcomings:"),Object(r.b)("ul",null,Object(r.b)("li",{parentName:"ul"},Object(r.b)("a",Object(n.a)({parentName:"li"},{href:"https://issues.apache.org/jira/browse/SPARK-24374"}),"Barrier scheduling")),Object(r.b)("li",{parentName:"ul"},Object(r.b)("a",Object(n.a)({parentName:"li"},{href:"https://issues.apache.org/jira/browse/SPARK-24615"}),"Accelerator aware scheduling")),Object(r.b)("li",{parentName:"ul"},Object(r.b)("a",Object(n.a)({parentName:"li"},{href:"https://issues.apache.org/jira/browse/SPARK-24579"}),"Data interchange using Apache Arrow"))),Object(r.b)("p",null,"The goal of Project Hydrogen is to make it easy and efficient to build deep learning workflows that can run end to end in Spark. This is exciting!"),Object(r.b)("p",null,"Spark and deep learning can't ignore each other, and that probably won't\nchange any time soon. Because of the current complexities, it's best to avoid\ndistributing deep learning training when possible. But we're excited to\nsee investment into scaling deep learning with Spark. There are so many great\nlibraries for doing heavily optimized machine learning - PyTorch, Tensorflow,\nXGBoost, LightGBM - that it's hugely beneficial to be able to scale these\nup with Spark.\n"))}c.isMDXComponent=!0}},[["3+vP","5d41","9da1"]]]);