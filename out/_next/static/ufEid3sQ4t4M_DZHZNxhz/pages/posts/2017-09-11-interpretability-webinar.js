(window.webpackJsonp=window.webpackJsonp||[]).push([["3373"],{FUxV:function(e,t,a){(window.__NEXT_P=window.__NEXT_P||[]).push(["/posts/2017-09-11-interpretability-webinar",function(){var e=a("SWMd");return{page:e.default||e}}])},SWMd:function(e,t,a){"use strict";a.r(t),a.d(t,"frontMatter",function(){return o}),a.d(t,"default",function(){return c});var n=a("kOwS"),i=a("qNsG"),r=(a("q1tI"),a("E/Ix")),o={layout:"post",title:"Interpretability in conversation with Patrick Hall and Sameer Singh",date:new Date("2017-09-11T00:00:00.000Z"),preview_image:"/images/2017/08/ff06-logo.png",author:"Mike",author_link:"http://twitter.com/mikepqr",feature:!0,published:!0},s={frontMatter:o},l="wrapper";function c(e){var t=e.components,a=Object(i.a)(e,["components"]);return Object(r.b)(l,Object(n.a)({},s,a,{components:t,mdxType:"MDXLayout"}),Object(r.b)("div",{class:"html-video-holder"},Object(r.b)("iframe",{width:"560",height:"315",src:"https://www.youtube.com/embed/NxYCY8-Qfx0",frameborder:"0",allowfullscreen:!0})),Object(r.b)("p",null,"We're pleased to share the recording of our recent webinar on machine learning\ninterpretability and accompanying resources."),Object(r.b)("p",null,"We were joined by guests Patrick Hall (Senior Director for Data Science\nProducts at H2o.ai, co-author of ",Object(r.b)("a",Object(n.a)({parentName:"p"},{href:"https://www.oreilly.com/ideas/ideas-on-interpreting-machine-learning"}),"Ideas on Interpreting Machine\nLearning"),")\nand Sameer Singh (Assistant Professor of Computer Science at UC Irvine,\nco-creator of ",Object(r.b)("a",Object(n.a)({parentName:"p"},{href:"https://github.com/marcotcr/lime"}),"LIME"),")."),Object(r.b)("p",null,"We spoke for an hour and got lots of fantastic questions during that time. We\ndidn't have time to answer them all, so Patrick and Sameer have been kind\nenough to ",Object(r.b)("a",Object(n.a)({parentName:"p"},{href:"#questions"}),"answer many of them below"),"."),Object(r.b)("p",null,"We're also glad to share contact information for all the participants and links\nto code and further reading. Please get in touch with any of us if you're\ninterested in working together."),Object(r.b)("h3",null,"Contact"),Object(r.b)("ul",null,Object(r.b)("li",{parentName:"ul"},"Fast Forward Labs, ",Object(r.b)("a",Object(n.a)({parentName:"li"},{href:"mailto:contact@fastforwardlabs.com"}),"contact@fastforwardlabs.com"),",\n",Object(r.b)("a",Object(n.a)({parentName:"li"},{href:"https://twitter.com/FastForwardLabs"}),"@fastforwardlabs")),Object(r.b)("li",{parentName:"ul"},"Mike Lee Williams, ",Object(r.b)("a",Object(n.a)({parentName:"li"},{href:"https://twitter.com/mikepqr"}),"@mikepqr")),Object(r.b)("li",{parentName:"ul"},"Patrick Hall, ",Object(r.b)("a",Object(n.a)({parentName:"li"},{href:"https://twitter.com/jpatrickhall"}),"@jpatrickhall"),",\n",Object(r.b)("a",Object(n.a)({parentName:"li"},{href:"mailto:phall@h2o.ai"}),"phall@h2o.ai")),Object(r.b)("li",{parentName:"ul"},"Sameer Singh, ",Object(r.b)("a",Object(n.a)({parentName:"li"},{href:"http://sameersingh.org/"}),"sameersingh.org")," or\n",Object(r.b)("a",Object(n.a)({parentName:"li"},{href:"https://twitter.com/sameer_"}),"@sameer","_"),", ",Object(r.b)("a",Object(n.a)({parentName:"li"},{href:"mailto:sameer@uci.edu"}),"sameer@uci.edu"))),Object(r.b)("h3",null,"Code, demos and applications"),Object(r.b)("ul",null,Object(r.b)("li",{parentName:"ul"},Object(r.b)("a",Object(n.a)({parentName:"li"},{href:"https://github.com/marcotcr/lime"}),"Open source LIME implementation")),Object(r.b)("li",{parentName:"ul"},Object(r.b)("a",Object(n.a)({parentName:"li"},{href:"https://youtu.be/3_gm00kBwEw"}),"Machine Learning Interpretability with H2O.ai’s Driverless\nAI")),Object(r.b)("li",{parentName:"ul"},Object(r.b)("a",Object(n.a)({parentName:"li"},{href:"https://github.com/jphall663/GWU_data_mining/blob/master/10_model_interpretability/10_model_interpretability.md"}),"Practical Model\nInterpretability"),"\n(Patrick’s teaching resources)"),Object(r.b)("li",{parentName:"ul"},Object(r.b)("a",Object(n.a)({parentName:"li"},{href:"http://blog.fastforwardlabs.com/2017/09/01/LIME-for-couples.html"}),"Why your relationship is likely to last (or not): using\nLIME")," by\nFast Forward Labs")),Object(r.b)("h3",null,"Reading"),Object(r.b)("ul",null,Object(r.b)("li",{parentName:"ul"},Object(r.b)("a",Object(n.a)({parentName:"li"},{href:"https://www.oreilly.com/ideas/ideas-on-interpreting-machine-learning"}),"Ideas on Machine\nLearning"),"\nby Patrick Hall"),Object(r.b)("li",{parentName:"ul"},Object(r.b)("a",Object(n.a)({parentName:"li"},{href:"https://arxiv.org/abs/1602.04938"}),'"Why Should I Trust You?": Explaining the Predictions of Any\nClassifier')," (or ",Object(r.b)("a",Object(n.a)({parentName:"li"},{href:"https://www.oreilly.com/learning/introduction-to-local-interpretable-model-agnostic-explanations-lime"}),"O’Reilly version of\npaper"),")\nby Marco Tulio Ribero, Sameer Singh and Carlos Guestrin"),Object(r.b)("li",{parentName:"ul"},Object(r.b)("a",Object(n.a)({parentName:"li"},{href:"http://people.dbmi.columbia.edu/noemie/papers/15kdd.pdf"}),"Intelligible Models for HealthCare: Predicting Pneumonia Risk and Hospital\n30-day Readmission"),"\nby Rich Caruana et al. (the source for the pneumonia/asthma story Mike told)"),Object(r.b)("li",{parentName:"ul"},Object(r.b)("a",Object(n.a)({parentName:"li"},{href:"http://blog.fastforwardlabs.com/2017/08/02/business-interpretability.html"}),"The business case for\ninterpretability"),"\nby Fast Forward Labs"),Object(r.b)("li",{parentName:"ul"},Object(r.b)("a",Object(n.a)({parentName:"li"},{href:"https://arxiv.org/abs/1606.05386"}),"A Case for Model-Agnostic\nInterpretability")," by Marco Tulio Ribero,\nSameer Singh and Carlos Guestrin"),Object(r.b)("li",{parentName:"ul"},Object(r.b)("a",Object(n.a)({parentName:"li"},{href:"http://www.fatml.org/"}),"Fairness, Accountability and Transparency in Machine\nLearning")," and ",Object(r.b)("a",Object(n.a)({parentName:"li"},{href:"https://fatconference.org/"}),"FAT\nconference")," (NYC, February 2018)"),Object(r.b)("li",{parentName:"ul"},Object(r.b)("a",Object(n.a)({parentName:"li"},{href:"https://logicmag.io/01-intelligence/"}),"Logic Magazine Issue 1"),", which\nfeatures the interview with an anonymous data scientist")),Object(r.b)("h3",null,"Talks"),Object(r.b)("ul",null,Object(r.b)("li",{parentName:"ul"},Object(r.b)("a",Object(n.a)({parentName:"li"},{href:"https://www.youtube.com/watch?v=LAm4QmVaf0E"}),"Explaining Black-box Machine Learning\nPredictions"),", talk by Sameer on\nLIME and related ideas at the Orange County ACM chapter."),Object(r.b)("li",{parentName:"ul"},Object(r.b)("a",Object(n.a)({parentName:"li"},{href:"https://conferences.oreilly.com/strata/strata-ny/public/schedule/detail/59747"}),"Interpretable AI: not just for\nregulators"),"\na forthcoming talk by SriSatish Ambati and Patrick Hall at Strata NYC, Sep\n27 2017")),Object(r.b)("h2",null,"Audience questions we didn't address during the webinar"),Object(r.b)("p",null,Object(r.b)("strong",{parentName:"p"},"Is there a standard way to measure model complexity?")),Object(r.b)("p",null,"Patrick: Not that I am aware of, but we use and have ",Object(r.b)("a",Object(n.a)({parentName:"p"},{href:"https://www.oreilly.com/ideas/ideas-on-interpreting-machine-learning"}),"put forward this simple\nheuristic"),":"),Object(r.b)("ul",null,Object(r.b)("li",{parentName:"ul"},"Linear, monotonic models - easily interpretable"),Object(r.b)("li",{parentName:"ul"},"Nonlinear, monotonic models - interpretable"),Object(r.b)("li",{parentName:"ul"},"Nonlinear, non-monotonic models - difficult to interpret")),Object(r.b)("p",null,'Mike: one option, when comparing like with like, is simply the number of\nparameters. This is a common metric in the deep learning community. It glosses\nover some of what we really mean when we say "complex", but it gets at\nsomething.'),Object(r.b)("p",null,"Sameer: Complexity is very subjective, and in different contexts, different\ndefinitions are useful. I also agree that number of parameters are often quite\na useful proxy to complexity. One other metric I like is running time or energy\nconsumed for each prediction. Of course, there is some theoretical work on this\nas well, such as ",Object(r.b)("a",Object(n.a)({parentName:"p"},{href:"https://en.wikipedia.org/wiki/VC_dimension"}),"VC\ndimensionality")," or even ",Object(r.b)("a",Object(n.a)({parentName:"p"},{href:"https://en.wikipedia.org/wiki/Kolmogorov_complexity"}),"Kolmogorov\ncomplexity"),". The open\nquestion is which of these measures of complexity correlates with a user’s\ncapacity to interpret it."),Object(r.b)("p",null,Object(r.b)("strong",{parentName:"p"},"Is there really a trade-off in call cases between interpretability and accuracy? There are certainly cases where there isn't, e.g Rich Caruana's pneumonia model. Can you characterize where this trade-off exists and doesn't?")),Object(r.b)("p",null,"Patrick: I think we are making an assumption that greater accuracy requires\ngreater complexity -- which it often does for predictive modeling. So, maybe\nit’s more accurate to say there is a trade-off between interpretability and\ncomplexity. Humans cannot, in general, understand models with thousands, let\nalone millions, of rules or parameters -- this level of complexity is common in\nmachine learning -- and this level of complexity is often required to model\nreal-life, nonlinear phenomena. For a linear model, I probably agree with the\nquestioner that the trade-off may not be as impactful, as long as the number of\nparameters in the linear model is relatively small."),Object(r.b)("p",null,"Mike: This may be stating the obvious, but I’d also add that, in situations\nwhere you can get high enough accuracy for your use case with a model so simple\nit’s interpretable by inspection (which does happen!), there is of course no\ntrade-off. You can have it all!"),Object(r.b)("p",null,Object(r.b)("strong",{parentName:"p"},'Is the black box aspect of machine learning programming only an early AI development issue? Will it eventually be possible to program in "check points" where programmed models will reveal key points or factors that appear within levels of neural network calculations?')),Object(r.b)("p",null,'Patrick: I don’t think this is an early AI issue. In my opinion, it’s about the\nfundamental complexity of the generated models. Again, the sheer volume of\ninformation is not interpretable to humans -- not even touching on more subtle\ncomplications. I don’t mean big data either -- even though that often doesn’t\nhelp make things any clearer -- I mean that I don’t think anyone can understand\na mathematical formula that requires 500 MB just to store it’s rules and\nparameters. (Which is not uncommon in practice.) I do like the idea of training\ncheckpoints, but what if at the checkpoint, the model says: "these are the\n10,000 most important extracted features which represent 100+ degree\ncombinations of the original model inputs"? So perhaps the combination of\ntraining checkpoints plus constraints on complexity could be very useful.'),Object(r.b)("p",null,Object(r.b)("strong",{parentName:"p"},"Conversations in data science center around the latest/greatest models, not interpretability. Do you have any recommendations for building a company culture that values interpretability.")),Object(r.b)("p",null,"Mike: send your colleagues our blog post ",Object(r.b)("a",Object(n.a)({parentName:"p"},{href:"http://blog.fastforwardlabs.com/2017/08/02/business-interpretability.html"}),"The Business Case for Machine\nLearning\nInterpretability"),"!\nInterpretability models are profitable, safer and more intellectually\nrewarding. Hopefully every one of your colleagues is interested in at least one\nof those things."),Object(r.b)("p",null,'Patrick: In my opinion, I’d also say this is part of customer-focus in an\nanalytics tool provider’s culture. It’s usually us data-nerds who want to use\nour new toys. I almost never hear a customer say, "give me a model using the\nlatest and greatest algorithm, oh, and it’s fine if it’s super complex and not\ninterpretable."'),Object(r.b)("p",null,"Sameer: Partly, it comes from the fact that accuracy provides a single number,\nwhich appeals to the human strive for competition and for sports, and for\nengineering things that beat other things. Interpretability is, almost by\ndefinition, much more fuzzier to define and evaluate, making us a little\nnervous as empiricists, I think."),Object(r.b)("p",null,Object(r.b)("strong",{parentName:"p"},"How does interpretability varies across industry, e.g. aviation v media v financial services?")),Object(r.b)("p",null,"Patrick: I can only say that the regulations for predictive models are probably\nmost mature in credit lending ",Object(r.b)("em",{parentName:"p"},"in the U.S."),", and that I see machine learning\nbeing used more prominently in verticals outside credit lending, i.e.\ne-commerce, marketing, anti-fraud, anti-money-laundering."),Object(r.b)("p",null,"Mike: I’d say that, of the particular three mentioned, the need for\ninterpretability is most acute in aviation. In fact, it goes beyond\ninterpretability into ",Object(r.b)("a",Object(n.a)({parentName:"p"},{href:"http://composition.al/blog/2017/05/30/proving-that-safety-critical-neural-networks-do-what-theyre-supposed-to-where-we-are-where-were-going-part-1-of-2/"}),"formal verifiability of the properties of an\nalgorithm"),",\nwhich is a whole different ball of wax. The acknowledged need is least in\nmedia, because there’s relatively little regulation. Which is not to say it\nwouldn’t be profitable to apply these techniques in this or any other industry\nwhere it’s important to understand your customers. Financial services is\ninteresting. The need for interpretability is well-understood (and hopefully\nwell-enforced) there. There’s no question, however, that more accurate models\nwould make more money. People are ",Object(r.b)("a",Object(n.a)({parentName:"p"},{href:"https://insight.equifax.com/approve-business-customers/"}),"starting to build neural network-based\ntrading and lending\nmodels")," that satisfy\napplicable regulations, e.g. ",Object(r.b)("a",Object(n.a)({parentName:"p"},{href:"https://www.federalreserve.gov/supervisionreg/srletters/sr1107.htm"}),"SR\n11-7"),", and\nthe ",Object(r.b)("a",Object(n.a)({parentName:"p"},{href:"https://en.wikipedia.org/wiki/Fair_Credit_Reporting_Act"}),"Fair Credit Reporting\nAct"),". There's a huge\nfirst-to-market advantage in deploying these more accurate models."),Object(r.b)("p",null,Object(r.b)("strong",{parentName:"p"},"Model governance and model reviews are standard for financial models as are stress tests. Do you see something similar in the future of industry ML models?")),Object(r.b)("p",null,"Patrick: I don’t know why so few machine learning practitioners stress-test\ntheir models. It’s easy to do with simple sensitivity analysis, and the\nfinancial risks of out-of-range predictions on new data are staggering! I do\nalso hope that machine learning models that make serious impacts on people’s\nlives will be better regulated in the future, and the EU is taking steps toward\nthis with the GDPR. In the meantime, you can keep up with research in this area\nat ",Object(r.b)("a",Object(n.a)({parentName:"p"},{href:"http://www.fatml.org/"}),"FATML"),"."),Object(r.b)("p",null,"Mike: I also recommend ",Object(r.b)("a",Object(n.a)({parentName:"p"},{href:"https://research.google.com/pubs/pub45742.html"}),"What’s your ML test score? A rubric for ML production\nsystems"),", which mentions a\nbunch of really basic stuff that far too few of us do."),Object(r.b)("p",null,Object(r.b)("strong",{parentName:"p"},"What effect will interpretability have on feature selection?")),Object(r.b)("p",null,"Mike: Anecdotally, we spotted a bunch of problems with our model of customer\nchurn using LIME. In particular, as non-experts in the domain, we’d left in\nfeatures that were leaking the target variable. These lit up like Christmas\ntrees in our web interface thanks to LIME."),Object(r.b)("p",null,"Patrick: I think it will prevent people from using overly-complicated,\ndifficult to explain features in their models as well. It’s no good to say\n",Object(r.b)("inlineCode",{parentName:"p"},"CV_SVD23_Cluster99_INCOME_RANGE")," is the most important variable in the\nprediction if you can’t also say directly what that variable is exactly and how\nit was derived."),Object(r.b)("p",null,Object(r.b)("strong",{parentName:"p"},"I'm a graduate DS student who just sent some ML research to a group of people in industry who I thought would be interested. In response I got the question \"will your research replace my job\". What are some ways to overcome the fear of ML and convince people that AI won't replace the creativity in decision making of humans.")),Object(r.b)("p",null,'Patrick: Well it might one day -- and we all need to be realistic about that.\nBut for today, and likely for many years, most of us can rest easy. Today,\nmachine learning is only good at specific tasks: tasks where there is a large\namount of labeled, easy-to-use "clean" data that has also been labeled.'),Object(r.b)("p",null,'Sameer: For now, you can use the explanations almost as a way to show that\nmachine learning is not a magical black-box. Without an explanation, a natural\nreaction is to say "how could it predict this? It must be super-intelligent!",\nbut with an explanation and demystifies this, even if it is doing the right\nthing for right reasons, the perception of machine learning will not be of an\nadversary.'),Object(r.b)("p",null,Object(r.b)("strong",{parentName:"p"},"Why is it that some models are seen as interpretable and others aren't? There are large tomes on the theory of linear models, yet they're seen as interpretable. Could part of this be due to how long they've been taught?")),Object(r.b)("p",null,"Mike: this is a great point. I don’t think it’s simply due to our relative\nfamiliarity with linear models. It’s that a trained linear model really is\nsimple to describe (and interpret). Trained neural networks are, in a relative\nsense, not even simple to describe. The big linear modeling textbooks are about the long textbooks' deep domain-specific implications, difficulties like\ncausality, and the ",Object(r.b)("a",Object(n.a)({parentName:"p"},{href:"https://arxiv.org/abs/1008.4686"}),"real numerical/engineering\nsubtleties"),"."),Object(r.b)("p",null,"Patrick: I 100% agree with the questioner's sentiment. Essentially linear model\ninterpretations are exact and stable, which is great, ",Object(r.b)("em",{parentName:"p"},"but")," the models are\napproximate. Machine learning explanations take a different mindset: machine\nlearning explanations are less stable and exact, but the model itself is\nusually much less approximate. So, do you prefer an exact explanation for an\napproximate model? Or an approximate explanation for an exact model? In my\nopinion, both are useful."),Object(r.b)("p",null,"Sameer: Interpretability is relative. I don’t think we should hold linear\nmodels as the ideal in interpretability, because it is not, especially with\nlarge number of variables. One of the known problems with linear models is\ncorrelated features, i.e. the importance of a feature can get distributed to\ncorrelated features, making features that are less important, but uncorrelated,\nhave a higher weight. We tried to get around this somewhat in LIME by\nrestricting the number of features chosen as an explanation (L1 regularization\nor Lasso), and normalizing the regression variables over our samples (to reduce\nthe effect of the bias)."),Object(r.b)("p",null,Object(r.b)("strong",{parentName:"p"},"Once we identify biases, how do we address them?")),Object(r.b)("p",null,"Patrick: Problematic features -- such as those correlated to race, gender,\nmarital status, disability status, etc. -- can be removed from the input data\nand the model can be retrained. Or features can be intentionally corrupted to\nremove problematic information with techniques like differential privacy during\nmodel training. Another method I’m personally interested is determining the\nlocal contribution of problematic features using something like LOCO or LIME\nand subtracting out the different contributions of problematic features\nrow-by-row when predictions are made."),Object(r.b)("p",null,Object(r.b)("strong",{parentName:"p"},"Aren't we reducing interpretability to visual analytics of sensitivity?")),Object(r.b)("p",null,"Patrick: In some cases yes, but I would argue this is a good thing. In my\nopinion, explanations themselves have to be simple. However, I’m more\ninterested in fostering the understanding of someone who was just denied parole\nor a credit card (both of which are happening today) based on the decision of a\npredictive model. For the mass-consumer audience, it’s not an effective\nstrategy to provide explanations that are just as mathematically complex as the\noriginal model."),Object(r.b)("p",null,Object(r.b)("strong",{parentName:"p"},"How is LIME different than variable importance, which we get from different algorithms (e.g. RFs?)")),Object(r.b)("p",null,"Patrick: The key is locality. LIME essentially provides local variable\nimportance, meaning that you often get a different variable importance value\nfor each input variable for each row of the data set. This opens up the\npossibility of describing why a machine learning model made the prediction it\nmade for each customer, patient, transaction, etc. in the data set."),Object(r.b)("p",null,"Sameer: To add to that, I would say the difference between global and local\ndependence can sometimes be quite important. Aggregations used to compute\nglobal dependence, like variable importance, can sometimes drown signals. For\nexample, if race is being used to make a decision for a really small number of\nindividuals, it might not show up in the global aggregations. Similarly, local\nexplanations are also useful in showing the sign of the dependence in context,\ni.e. age might be important overall, but for some individuals age might act as\na negative factor, and for a positive, and global explanations will not be able\nto capture that. That said, it’s much easier to look at only the big picture,\ninstead of many small pictures."),Object(r.b)("p",null,Object(r.b)("strong",{parentName:"p"},"Which bootstrapping algorithm is used by LIME generate the perturbed samples")),Object(r.b)("p",null,"Sameer: This is often domain dependent, and you can plug in your own. We tried\nto stick with pretty simple techniques for each domain, such as removing tokens\nin text, patches in images, etc. More details are in the paper/code."),Object(r.b)("p",null,Object(r.b)("strong",{parentName:"p"},"In the case of adversarial attacks, can LIME detect what causes the deviation from correct detection.")),Object(r.b)("p",null,'Sameer: (excerpt from an email thread with Adam) This is quite an interesting\nidea, but unfortunately, I believe LIME will get quite stumped in this case,\nespecially for images, either proposing the whole image as the explanation\n(assuming the adversarial noise is spread out, as it often is), or find a "low\nconfidence" explanation, i.e. it\'ll find the subset of the image that is most\nadversarial, but with sufficient uncertainty to say "don\'t take this\nexplanation too seriously".'),Object(r.b)("p",null,Object(r.b)("strong",{parentName:"p"},"Can you explain the significance of the clusters in the H2O interpretability interface?")),Object(r.b)("p",null,"Patrick: We chose to use clusters in the training data, instead of bootstrapped\nor simulated samples around a row of data, to construct local regions on which\nto build explanatory linear models. This has two primary advantages:"),Object(r.b)("ul",null,Object(r.b)("li",{parentName:"ul"},"We don’t need a new/different sample for every point we want to explain"),Object(r.b)("li",{parentName:"ul"},"It allows us to present the (hopefully helpful) diagnostic plot of the\ntraining data, complex model, and explanatory model that you saw in the\nwebinar.")),Object(r.b)("p",null,"The main drawback is that sometimes clusters are large and the fit of the\nexplanatory model can degrade in this case. If you’re curious, we choose the\nnumber clusters by maximizing the R-squared between all the linear model\npredictions and the complex model’s predictions."),Object(r.b)("p",null,Object(r.b)("strong",{parentName:"p"},"LIME makes accurate models more interpretable. Also mentioned was the related idea of making interpretable models more accurate. Which is more promising?")),Object(r.b)("p",null,"Patrick: Most research I see is towards making accurate models more\ninterpretable. One nice practical approach for going the other direction --\nmaking interpretable models more accurate -- are the monotonicity constraints\nin XGBoost."),Object(r.b)("p",null,"Sameer: Personally, I like the former, since I do believe an inaccurate model\nis not a useful model. I also don’t want to restrict the architecture or the\nalgorithms that people want to use, nor do I want to constrain them to certain\ntypes of interpretations that an interpretable model provides."),Object(r.b)("h3",null,"Mailing list"),Object(r.b)("p",null,"Our public mailing list is a great way of getting a taste of what Fast Forward\nLabs is interested in and working on right now. ",Object(r.b)("a",Object(n.a)({parentName:"p"},{href:"https://fastforwardlabs.us8.list-manage.com/subscribe/post?u=bdb368b9a389b010c19dbcd54&id=1d8c97a0bd"}),"We hope you'll sign up"),"!"))}c.isMDXComponent=!0}},[["FUxV","5d41","9da1"]]]);