(window.webpackJsonp=window.webpackJsonp||[]).push([["40b5"],{jfrC:function(e,n,t){(window.__NEXT_P=window.__NEXT_P||[]).push(["/posts/2018-04-10-pytorch-for-recommenders-101",function(){var e=t("kjca");return{page:e.default||e}}])},kjca:function(e,n,t){"use strict";t.r(n),t.d(n,"frontMatter",function(){return i}),t.d(n,"default",function(){return m});var r=t("kOwS"),a=t("qNsG"),s=(t("q1tI"),t("E/Ix")),i={title:"PyTorch for Recommenders 101",date:"2018-04-10 13:04 -0400",preview_image:"/images/editor_uploads/2018-04-11-175944-02_07.png",author:"Shioulin",author_link:"https://twitter.com/shioulin_sam"},o={frontMatter:i},d="wrapper";function m(e){var n=e.components,t=Object(a.a)(e,["components"]);return Object(s.b)(d,Object(r.a)({},o,t,{components:n,mdxType:"MDXLayout"}),Object(s.b)("p",null,"Recommenders, generally associated with e-commerce, sift though a huge inventory\nof available items to find and recommend ones that a user will like. Different\nfrom search, recommenders rely on historical data to tease out user\npreference. How does a recommender accomplish this? In this post we explore\nbuilding simple recommendation systems in ",Object(s.b)("a",Object(r.a)({parentName:"p"},{href:"http://pytorch.org/"}),"PyTorch")," using\nthe ",Object(s.b)("a",Object(r.a)({parentName:"p"},{href:"https://grouplens.org/datasets/movielens/100k/"}),"Movielens 100K data"),", which\nhas 100,000 ratings (1-5) that 943 users provided on 1682 movies. "),Object(s.b)("h2",null,"Matrix Factorization"),Object(s.b)("p",null,Object(s.b)("img",Object(r.a)({parentName:"p"},{src:"/images/editor_uploads/2018-04-11-175944-02_07.png",alt:"Diagram of a simple recommendation system. It moves from Step 1: User, to Step 2: Connected Factors, to Step 3: Recommended items based on the connected factors."}))),Object(s.b)("p",null,"We first build a traditional recommendation system based on ",Object(s.b)("a",Object(r.a)({parentName:"p"},{href:"https://datajobs.com/data-science-repo/Recommender-Systems-%5BNetflix%5D.pdf"}),"matrix\nfactorization"),". The\ninput data is an interaction matrix where each row represents a user and each\ncolumn represents an item. The rating assigned by a user for a particular item\nis found in the corresponding row and column of the interaction matrix. This\nmatrix is generally large but sparse; there are many items and users but a\nsingle user would only have interacted with a small subset of items. Matrix\nfactorization decomposes this larger matrix into two smaller matrices - the\nfirst one maps users into a set of factors and the second maps items into the\nsame set of factors. Multiplying these two smaller matrices together gives an\napproximation to the original matrix, with values for empty elements\ninferred. To predict a rating for a user-item pair, we simply multiply the row\nrepresenting the user from the first matrix with the column representing the\nitem from the second matrix."),Object(s.b)("p",null,'In PyTorch we can implement a version of matrix factorization by using the\nembedding layer to "map" users into a set of factors. The number of factors\ndetermine the size of the embedding vector. Similarly we map items into their\nown embedding layer. Both user and item embeddings have the same size. To\npredict a user-item rating, we multiply the user embeddings with item embeddings\nand sum to obtain one number. The following code draws from ',Object(s.b)("a",Object(r.a)({parentName:"p"},{href:"https://github.com/EthanRosenthal/torchmf/blob/master/torchmf.py"}),"Ethan Rosenthal's\nwork on matrix factorization in\nPyTorch"),"."),Object(s.b)("pre",null,Object(s.b)("code",Object(r.a)({parentName:"pre"},{className:"language-python"}),"import torch\nfrom torch.autograd import Variable\n\nclass MatrixFactorization(torch.nn.Module):\n    def __init__(self, n_users, n_items, n_factors=20):\n        super().__init__()\n    # create user embeddings\n        self.user_factors = torch.nn.Embedding(n_users, n_factors,\n                                               sparse=True)\n    # create item embeddings\n        self.item_factors = torch.nn.Embedding(n_items, n_factors,\n                                               sparse=True)\n\n    def forward(self, user, item):\n        # matrix multiplication\n        return (self.user_factors(user)*self.item_factors(item)).sum(1)\n\n    def predict(self, user, item):\n        return self.forward(user, item)\n")),Object(s.b)("p",null,"To fit the matrix factorization model we need to pick a loss function and an\noptimizer. In this example we use the average squared distance between the\nprediction and the actual value as a loss function, this is known as\nmean-squared error. We then try to minimize this loss by using stochastic\ngradient descent. The code below shows how the model is fitted in four steps: i)\npass in a user-item pair, ii) forward pass to compute the predicted rating, iii)\ncompute the loss, and iv) backpropagate to compute gradients and update the\nweights. "),Object(s.b)("pre",null,Object(s.b)("code",Object(r.a)({parentName:"pre"},{className:"language-python"}),"model = MatrixFactorization(n_users, n_items, n_factors=20)\nloss_fn = torch.nn.MSELoss() \noptimizer = torch.optim.SGD(model.parameters(),\n                            lr=1e-6)\n\nfor user, item in zip(users, items):\n    # get user, item and rating data\n    rating = Variable(torch.FloatTensor([ratings[user, item]]))\n    user = Variable(torch.LongTensor([int(user)]))\n    item = Variable(torch.LongTensor([int(item)]))\n\n    # predict\n    prediction = model(user, item)\n    loss = loss_fn(prediction, rating)\n\n    # backpropagate\n    loss.backward()\n\n    # update weights\n    optimizer.step()\n")),Object(s.b)("p",null,"We train this model on the Movielens dataset with ratings scaled between ","[0, 1]","\nto help with convergence. Applied on the test set, we obtain a root mean-squared\nerror(RMSE) of 0.66. This means that on average, the difference between our\nprediction and the actual value is 0.66!"),Object(s.b)("h2",null,"Dense Feedforward Neural Network"),Object(s.b)("p",null,"Given the underwhelming performance of our matrix factorization model, we try a\nsimple feedforward recommendation system instead. The input to this neural\nnetwork is a pair of user and item represented by their IDs. Both user and item\nIDs first pass through an embedding layer. The output of the embedding layer,\nwhich are two embedding vectors, are then concatenated into one and passed into\na linear network. The output of the linear network is one dimensional -\nrepresenting the rating for the user-item pair. The model is fit the same way as\nthe matrix factorization model and uses the standard PyTorch approach of forward\npassing, computing the loss, backpropagating and updating weights."),Object(s.b)("pre",null,Object(s.b)("code",Object(r.a)({parentName:"pre"},{className:"language-python"}),'import torch\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torch import nn\n\nclass DenseNet(nn.Module):\n\n    def __init__(self, n_users, n_items, n_factors, H1, D_out):\n        """\n        Simple Feedforward with Embeddings\n        """\n        super().__init__()\n    # user and item embedding layers\n        self.user_factors = torch.nn.Embedding(n_users, n_factors,\n                                               sparse=True)\n        self.item_factors = torch.nn.Embedding(n_items, n_factors,\n                                               sparse=True)\n    # linear layers\n        self.linear1 = torch.nn.Linear(n_factors*2, H1)\n        self.linear2 = torch.nn.Linear(H1, D_out)\n\n    def forward(self, users, items):\n        users_embedding = self.user_factors(users)\n        items_embedding = self.item_factors(items)\n    # concatenate user and item embeddings to form input\n        x = torch.cat([users_embedding, items_embedding], 1)\n        h1_relu = F.relu(self.linear1(x))\n        output_scores = self.linear2(h1_relu)\n        return output_scores\n\n    def predict(self, users, items):\n        # return the score\n        output_scores = self.forward(users, items)\n        return output_scores\n')),Object(s.b)("p",null,"Once again, we train this model on the Movielens dataset with ratings scaled\nbetween ","[0, 1]"," to help with convergence. Applied on the test set, we obtain a\nroot mean-squared error(RMSE) of 0.28, a substantial improvement!"),Object(s.b)("h2",null,"Sequence based Recommendation System"),Object(s.b)("p",null,"Finally we build a recommendation system that takes into account the sequence of\nitem interactions. The heart of this is a Long Short-Term Memory (LSTM) cell, a\nvariant of Recurrent Neural Networks (RNN) with faster convergence and better\nlong term memory. The input to this system is a history of item interactions and\ntheir corresponding ratings. In the following example of an input, we show a\nsequence of item interaction of length 10 (arbitrarily chosen) and the\ncorresponding rating. Elements in the first array correspond to items(movies),\nand elements in the second array correspond to ratings. We see that, for\nexample, movie 209 has a rating of 4, and movie 32 has a rating of 5. Sequences\nshorter than 10 are padded with zeros."),Object(s.b)("pre",null,Object(s.b)("code",Object(r.a)({parentName:"pre"},{className:"language-python"}),"In [2]: training_data[0]\nOut[2]: \n(array([209,  32, 189, 242, 171, 111, 256,   5,  74, 102], dtype=int32),\n array([4, 5, 3, 5, 5, 5, 4, 3, 1, 2], dtype=int32))\n")),Object(s.b)("p",null,"Items are passed through an embedding layer before going into the LSTM. The\noutput of the LSTM is then fed into a linear layer with an output dimension of\none. The LSTM has 2 hidden states, one for short term memory and one for long\nterm. Both states need to be initialized."),Object(s.b)("p",null,"PyTorch expects LSTM inputs to be a three dimensional tensor. The first\ndimension is the length of the sequence itself, the second represents the number\nof instances in a mini-batch, the third is the size of the actual input into the\nLSTM. Using our training data example with sequence of length 10 and embedding\ndimension of 20, input to the LSTM is a tensor of size 10x1x20 when we do not\nuse mini batches. For a mini-batch size of 2, each forward pass will have two\nsequences, and the input to the LSTM needs to have a dimension of 10x2x20. LSTMs\ntake variable input sequence lengths but for batch training purposes the input\ndata is generally processed(with padding if necessary) to have a fixed length."),Object(s.b)("pre",null,Object(s.b)("code",Object(r.a)({parentName:"pre"},{className:"language-python"}),"import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\nclass LSTMRating(nn.Module):\n\n    def __init__(self, embedding_dim, hidden_dim, num_items, num_output):\n        super().__init__()\n        self.hidden_dim = hidden_dim\n        self.item_embeddings = nn.Embedding(num_items, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n        self.linear = nn.Linear(hidden_dim, num_output)\n        self.hidden = self.init_hidden()\n\n    def init_hidden(self):\n        # initialize both hidden layers\n        return (Variable(torch.zeros(1, 1, self.hidden_dim)),\n                Variable(torch.zeros(1, 1, self.hidden_dim)))\n\n    def forward(self, sequence):\n        embeddings = self.item_embeddings(sequence)\n        output, self.hidden = self.lstm(embeddings.view(len(sequence), 1, -1),\n                                        self.hidden)\n        rating_scores = self.linear(output.view(len(sequence), -1))\n        return rating_scores\n\n    def predict(self, sequence):\n        rating_scores = self.forward(sequence)\n        return rating_scores\n")),Object(s.b)("p",null,"Once the neural network is defined, we fit the training data using stochastic\ngradient descent with a mean squared error loss function. "),Object(s.b)("pre",null,Object(s.b)("code",Object(r.a)({parentName:"pre"},{className:"language-python"}),"embedding_dim = 64\nhidden_dim = 128\nn_output = 1\n\n# add one to represent padding when there is not enough history\nmodel = LSTMRating(embedding_dim, hidden_dim, n_items+1, n_output)\nloss_fn = nn.MSELoss()\noptimizer = optim.SGD(model.parameters(), lr=0.1)\n\nfor sequence, target_ratings in training_data:\n    model.zero_grad()\n    # initialize hidden layers\n    model.hidden = model.init_hidden()\n    # convert sequence to PyTorch variables\n    sequence_var = Variable(torch.LongTensor(sequence.astype('int64')))\n    # forward pass\n    ratings_scores = model(sequence_var)\n    target_ratings_var = Variable(torch.FloatTensor(target_ratings.astype('float32')))\n    # compute loss\n    loss = loss_fn(ratings_scores, target_ratings_var)\n    # backpropagate\n    loss.backward()\n    # update weights\n    optimizer.step()\n")),Object(s.b)("p",null,"Similar to other models, we train the LSTM-based model on the Movielens dataset\nwith ratings scaled between ","[0, 1]"," to help with convergence. Applied on the test\nset, we obtain a root mean-squared error(RMSE) of 0.43 - the LSTM model\nunderperforms the dense feed forward network."),Object(s.b)("h2",null,"Post-amble"),Object(s.b)("p",null,"The models discussed in this post are basic building blocks for a recommendation\nsystem in PyTorch. There are no bells and whistles and we did not attempt to\nfine tune any hyperparameters. Our first pass result suggests that the dense\nnetwork performs best, followed by the LSTM network and finally the matrix\nfactorization model. The root mean-squared error (RMSE) are 0.28, 0.43 and 0.66\nrespectively on the Movielens 100K dataset with ratings scaled between ","[0,\n1]",". We thought PyTorch was fun to use; models can be built and swapped out\nrelatively easily. When we did encounter errors, most of them were triggered by\nincorrect data types."),Object(s.b)("p",null,"For more on recommendations, please see our ",Object(s.b)("a",Object(r.a)({parentName:"p"},{href:"https://www.cloudera.com/more/services-and-support/fast-forward-labs.html"}),"Semantic Recommendations report"),"\nwhere we focus on how machines can better understand content!"))}m.isMDXComponent=!0}},[["jfrC","5d41","9da1"]]]);