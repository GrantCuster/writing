(window.webpackJsonp=window.webpackJsonp||[]).push([["e1d7"],{FdQQ:function(e,t,i){"use strict";i.r(t),i.d(t,"frontMatter",function(){return r}),i.d(t,"default",function(){return c});var n=i("kOwS"),a=i("qNsG"),o=(i("q1tI"),i("E/Ix")),r={title:"Realistic Video Generation",date:"2018-09-28 17:10 -0400",preview_image:"/images/2018/09/gan-1536758013486.png",feature:!1,published:!0,author:"Micha",author_link:"http://micha.codes/",post_type:"newsletter"},s={frontMatter:r},l="wrapper";function c(e){var t=e.components,i=Object(a.a)(e,["components"]);return Object(o.b)(l,Object(n.a)({},s,i,{components:t,mdxType:"MDXLayout"}),Object(o.b)("p",null,Object(o.b)("a",Object(n.a)({parentName:"p"},{href:"https://arxiv.org/abs/1406.2661"}),"Generative Adversarial Networks")," (GANs) wowed the world in 2014 with their\nability to generate what we considered to be ",Object(o.b)("a",Object(n.a)({parentName:"p"},{href:"https://arxiv.org/abs/1701.07875"}),"realistic images"),". While these\nimages were quite low resolution, researchers kept working on how to perfect\nthese methods in order to increase the quality of the images and even to apply\nthe algorithm on other types of data like text and sound."),Object(o.b)("p",null,Object(o.b)("img",Object(n.a)({parentName:"p"},{src:"/images/2018/09/gan-1536758013486.png",alt:null}))),Object(o.b)("p",null,"However, until recently there has been little success in making realistic\nvideos. The main problem with making videos is temporal consistency: while\npeople can be forgiving in one frame and find some interpretation for\nunrealistic regions, we are adept at seeing inconsistencies with how videos\nprogress."),Object(o.b)("p",null,Object(o.b)("img",Object(n.a)({parentName:"p"},{src:"/images/2018/09/pix2pixhd-1536757958811.gif",alt:null}))),Object(o.b)("p",null,"For example, we can accept some strange looking texture in the background of an\nimage as simply some strange looking background. However, if that background is\nrandomly changing from frame to frame in a video, we immediately discount the\nvideo. It is exactly this temporal consistency which has plagued researchers\ntrying to apply GANs to videos -- while each frame seemed realistic taken on\nits own, when assembled into a video, there were considerable inconsistencies\nwhich ruined any illusion of realism. This restricted the ability to reuse\nmodels that showed success at generating individual images, and forced\nresearchers to come up with new methods to deal with the temporal nature of\nvideos."),Object(o.b)("p",null,Object(o.b)("img",Object(n.a)({parentName:"p"},{src:"/images/2018/09/maxresdefault-1536763922861.jpg",alt:null}))),Object(o.b)("p",null,"Recently, researchers at NVIDIA and MIT have come up with a new type of\nGAN, ",Object(o.b)("a",Object(n.a)({parentName:"p"},{href:"https://tcwang0509.github.io/vid2vid/"}),"vid2vid"),", which primarily addresses this problem by explicitly\nincorporating how things seem to be moving within the video, in order to continue\nthis motion in future frames. (In addition, they follow previous work, which uses\na multi-resolution approach for generating high resolution images). This is done\nby calculating the ",Object(o.b)("a",Object(n.a)({parentName:"p"},{href:"https://en.wikipedia.org/wiki/Optical_flow"}),"optical flow")," of the image, which is a classic computer\nvision method that simply has not been incorporated into such a model until now."),Object(o.b)("p",null,Object(o.b)("img",Object(n.a)({parentName:"p"},{src:"/images/2018/09/citysmall-1536759505454.gif",alt:null}))),Object(o.b)("p",null,"The results are quite staggering (we highly recommend watching their ",Object(o.b)("a",Object(n.a)({parentName:"p"},{href:"https://www.youtube.com/watch?v=GrP_aOSXt5U&feature=youtu.be"}),"release\nvideo"),"). With the model you can create dashboard camera footage from the initial\nsegmentation frame (allowing you to change the type and shape of objects in the\nframe by simply drawing in the corresponding color); it's even possible to create realistic looking\ndance videos from pose information. It's interesting to see this new method as\n",Object(o.b)("a",Object(n.a)({parentName:"p"},{href:"https://www.youtube.com/watch?v=A7g4mLD1E1E"}),"compared with previous methods"),", to really get a sense of how important this\nadditional temporal information is for making realistic results."),Object(o.b)("p",null,Object(o.b)("img",Object(n.a)({parentName:"p"},{src:"/images/2018/09/pose_to_body_vid2vi2-1536757918891.gif",alt:null}))),Object(o.b)("p",null,"These high quality results are quite exciting and are groundbreaking work in\nthe field of video generation. From applications in generating synthetic\ntraining data to use in creative projects, the vid2vid model itself is instantly\napplicable."),Object(o.b)("p",null,"Even more interesting is how the field as a whole will learn from this\nresearch and start finding ways to incorporate other classic algorithms into\nneural networks. Just as conv-nets explicitly encoded the two dimensional\nunderstanding we have for images into models so that they can more quickly and\naccurately learn how to work with that data, this method explicitly encodes our\nunderstanding of how frames of a video flow from one to another (albeit this was\nmuch trickier to do than the conv-net example!). We're interested in seeing what\nother algorithms will be incorporated into neural networks like this and what\ncapabilities these models will have."))}c.isMDXComponent=!0},gjVP:function(e,t,i){(window.__NEXT_P=window.__NEXT_P||[]).push(["/posts/2018-09-28-realistic-video-generation",function(){var e=i("FdQQ");return{page:e.default||e}}])}},[["gjVP","5d41","9da1"]]]);