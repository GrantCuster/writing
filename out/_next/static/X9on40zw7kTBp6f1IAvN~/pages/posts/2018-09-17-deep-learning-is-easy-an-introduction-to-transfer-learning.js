(window.webpackJsonp=window.webpackJsonp||[]).push([["9a55"],{"/YMA":function(e,n,t){"use strict";t.r(n),t.d(n,"frontMatter",function(){return s}),t.d(n,"default",function(){return c});var a=t("kOwS"),r=t("qNsG"),i=(t("q1tI"),t("E/Ix")),s={title:"Deep learning made easier with transfer learning",date:"2018-09-17 09:09 -0700",preview_image:"/images/editor_uploads/2018-09-18-170232-robot_share_1.png",feature:!0,published:!0},o={frontMatter:s},l="wrapper";function c(e){var n=e.components,t=Object(r.a)(e,["components"]);return Object(i.b)(l,Object(a.a)({},o,t,{components:n,mdxType:"MDXLayout"}),Object(i.b)("p",null,"Deep learning has provided extraordinary advances in problem spaces that\nare poorly solved by other approaches. This success is due to several key\ndepartures from traditional machine learning that allow it to excel when\napplied to unstructured data. Today, deep learning models can play games,\ndetect cancer, talk to humans, and drive cars."),Object(i.b)("p",null,"But the differences that make deep learning powerful also make it costly.\nYou may have heard that deep learning success requires massive data, expensive\nhardware, and even more expensive elite engineering talent."),Object(i.b)("p",null,"At Cloudera Fast Forward Labs, we're particularly excited about innovations\nthat lessen these challenges. Our ",Object(i.b)("a",Object(a.a)({parentName:"p"},{href:"/2018/07/24/ff08-launch.html"}),"latest research report goes in depth on\nmulti-task learning"),", an approach that allows\nmachine learning models to learn from multiple tasks at once. Among its many\nbenefits, this approach can reduce training data requirements."),Object(i.b)("p",null,"In this article, we're going to look at transfer learning, a related technique\nthat enables the transfer of knowledge from one task to another. Rather than\ndeveloping an entirely customized solution to your problem, transfer learning\nallows you to transfer knowledge from related problems to help solve your\ncustom problem more easily. By transferring that knowledge, you are taking\nadvantage of the expensive resources that were used to acquire it - training data,\nhardware, researchers - without the incurring the cost. Let's see how\nand when this approach is effective."),Object(i.b)("h2",null,"Why deep learning is different"),Object(i.b)("p",null,"Transfer learning is not a new technique, nor is it specific to deep learning,\nbut it is ",Object(i.b)("em",{parentName:"p"},"newly exciting")," in light of the recent progress in deep learning. So\nfirst, it's important to spell out the ways in which deep learning is different\nthan traditional machine learning."),Object(i.b)("h3",null,"Deep learning operates at a lower level of abstraction"),Object(i.b)("p",null,"Machine learning is a way for machines to automatically learn functions which\nassign predictions or labels to numerical inputs, i.e. data."),Object(i.b)("p",null,Object(i.b)("img",Object(a.a)({parentName:"p"},{src:"/images/editor_uploads/2018-09-18-165643-ml_function_1.png",alt:null}))),Object(i.b)("p",null,"The difficult part here is determining exactly how the function produces the\noutput from the provided input. Without any restrictions on the function, the\npossibilities (and complexities) are endless. In order to simplify this task,\nwe usually impose some type of structure on the function - based on the type of\nproblem we’re solving, domain expertise, or simply trial and error. That\nstructure defines a type of machine learning model."),Object(i.b)("p",null,Object(i.b)("img",Object(a.a)({parentName:"p"},{src:"/images/editor_uploads/2018-09-18-165658-ml_function_2.png",alt:null}))),Object(i.b)("p",null,"In theory, there are an infinity of possible structures, but in practice most\nmachine learning use cases can be solved by applying one of only a handful of\nstructures: linear models, ensembles of trees, and support vector machines make\nup a solid core. The data scientist’s job is then to choose the correct\nstructure from this small set of possible structures."),Object(i.b)("p",null,Object(i.b)("img",Object(a.a)({parentName:"p"},{src:"/images/editor_uploads/2018-09-18-170135-ml_function_3.png",alt:null}))),Object(i.b)("p",null,"These models are available as black box objects from a variety of mature\nmachine learning libraries, and can be trained in just a few lines of code. For\nexample, you can train a random forest model using Python's\n",Object(i.b)("a",Object(a.a)({parentName:"p"},{href:"http://scikit-learn.org/"}),"scikit-learn")," like this:"),Object(i.b)("pre",null,Object(i.b)("code",Object(a.a)({parentName:"pre"},{className:"language-python"}),"clf = RandomForestClassifier()\nclf.fit(past_data, labels)\npredictions = clf.predict(future_data)\n")),Object(i.b)("p",null,"Or a linear regression model in R:"),Object(i.b)("pre",null,Object(i.b)("code",Object(a.a)({parentName:"pre"},{className:"language-r"}),"linearModel <- lm(y ~ X, data=pastData)\npredictions <- predict(linearModel, futureData)\n")),Object(i.b)("p",null,"Deep learning, however, operates at a lower level. Rather than choosing among a\nsmall, finite set of model structures, deep learning allows practitioners to\ncompose arbitrary structures. The building blocks are modules or layers that\ncan be thought of as basic, fundamental data transformations. This means that\nwe need to open up the black box when applying deep learning, instead of\ntreating it as fixed by the algorithm."),Object(i.b)("p",null,Object(i.b)("img",Object(a.a)({parentName:"p"},{src:"/images/editor_uploads/2018-09-18-170159-ml_function_4.png",alt:null}))),Object(i.b)("p",null,"This allows more powerful models to be built, but it also adds an entirely new\ndimension to the model building process. Composing these transformations\neffectively can be a difficult process, despite the volume of published deep\nlearning research, ",Object(i.b)("a",Object(a.a)({parentName:"p"},{href:"https://developers.google.com/machine-learning/guides/text-classification/"}),"practical\nguidelines"),"\nand folk wisdom."),Object(i.b)("p",null,"Consider an extremely simple Convolutional Neural Network image classifier,\ndefined here in the popular deep learning library PyTorch."),Object(i.b)("pre",null,Object(i.b)("code",Object(a.a)({parentName:"pre"},{className:"language-python"}),"class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.conv2_drop = nn.Dropout2d()\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, 10)\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n        x = x.view(-1, 320)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)\n")),Object(i.b)("h5",null,"Source: ",Object(i.b)("a",Object(a.a)({parentName:"h5"},{href:"https://github.com/pytorch/examples/blob/master/mnist/main.py"}),"PyTorch")),Object(i.b)("p",null,"Because we are working with low level building blocks, we can change a\nsingle component of the model (e.g. ",Object(i.b)("inlineCode",{parentName:"p"},"F.relu")," to ",Object(i.b)("inlineCode",{parentName:"p"},"F.sigmoid"),", for instance).\nThis gives us an entirely new model architecture that may yield dramatically\ndifferent results. And the possibilities are literally endless."),Object(i.b)("h3",null,"Deep learning is not yet well-understood"),Object(i.b)("p",null,"Even given a fixed neural network architecture, training is notoriously\ndifficult. First, deep learning loss functions are not in general convex, which\nmeans that training does not necessarily yield the best possible solution.\nSecond, deep learning is still very new and many of its components are not well-understood. For example, ",Object(i.b)("a",Object(a.a)({parentName:"p"},{href:"https://arxiv.org/abs/1502.03167"}),"batch\nnormalization")," has received attention\nrecently because its inclusion in some models seems to be critical for good\nresults, but ",Object(i.b)("a",Object(a.a)({parentName:"p"},{href:"https://arxiv.org/abs/1805.11604v2"}),"experts cannot agree")," on why.\nResearcher Ali Rahimi caused some controversy when he went so far as to ",Object(i.b)("a",Object(a.a)({parentName:"p"},{href:"http://www.argmin.net/2017/12/05/kitchen-sinks/"}),"liken\ndeep learning to alchemy")," at a\nrecent machine learning conference."),Object(i.b)("h3",null,"Automatic Feature Engineering"),Object(i.b)("p",null,"The added complexity in deep learning enables a technique called ",Object(i.b)("a",Object(a.a)({parentName:"p"},{href:"https://en.wikipedia.org/wiki/Feature_learning"}),"representation\nlearning"),', which is why it\'s\noften stated that neural networks do "automatic feature engineering." In short,\ninstead of having a human hand-engineer helpful features from a dataset, we\nbuild models in such a way that they can learn whatever features are necessary\nand helpful for the task at hand. Offloading feature engineering onto the model\nis immensely powerful, but comes with the cost of models that require massive amounts\nof data and, consequently, massive computing power.'),Object(i.b)("h2",null,"What you can do about it"),Object(i.b)("p",null,"Deep learning is so complex in comparison to other machine learning methods,\nthat it can seem too overwhelming to incorporate into your business. For\nresource-constrained organizations, this feeling is magnified."),Object(i.b)("p",null,"For organizations that truly need to operate on the bleeding edge, it may\nindeed be necessary to hire experts and purchase specialized hardware. But this\nis not necessary in many cases. There are ways to apply it effectively without\nmaking enormous investments. This is where transfer learning comes in."),Object(i.b)("p",null,"Transfer learning enables the transfer of knowledge from one machine learning\nmodel to another. These models may be the result of years of research into\nmodel structure, trained on colossal datasets, and optimized over years of\ncompute time. With transfer learning you can get much of the benefit of this\nwork for none of the cost!"),Object(i.b)("h2",null,"What is transfer learning?"),Object(i.b)("p",null,"Most machine learning tasks start with zero knowledge, meaning that the\nstructure and parameters of the model begins as random guesses. This is what we\nmean when we say a model is learned ",Object(i.b)("em",{parentName:"p"},"from scratch"),"."),Object(i.b)("p",null,Object(i.b)("img",Object(a.a)({parentName:"p"},{src:"/images/editor_uploads/2018-09-18-170232-robot_share_1.png",alt:null}))),Object(i.b)("h5",null,"A cat detector model trained from scratch starts by guessing. It gradually learns what a cat is by aggregating common patterns across the many different cats it has seen."),Object(i.b)("p",null,"In this situation, everything the model learns comes from the data that you\nshow it. But is this the only way of solving a problem? In some cases, it might\nseem like it."),Object(i.b)("p",null,Object(i.b)("img",Object(a.a)({parentName:"p"},{src:"/images/editor_uploads/2018-09-18-170302-robot_share_2.png",alt:null}))),Object(i.b)("h5",null,"A cat detector model is likely useless in unrelated applications, like fraud detection. It only knows how to make sense of cat pictures, not credit card transactions."),Object(i.b)("p",null,"But in other cases, it seems like we should be able to share information between\ntasks."),Object(i.b)("p",null,Object(i.b)("img",Object(a.a)({parentName:"p"},{src:"/images/editor_uploads/2018-09-18-170328-robot_share_3.png",alt:null}))),Object(i.b)("h5",null,"A cat detector ",Object(i.b)("em",{parentName:"h5"},"is helpful")," in related tasks, like cat facial location. The detector should already know how to detect whiskers, noses, and eyes - all things that are useful in locating the cat's face."),Object(i.b)("p",null,"This is the essence of transfer learning: taking a model that has learned how\nto do one task very well and transferring some (or all) of that knowledge to a\nrelated task."),Object(i.b)("p",null,"This makes sense when we examine our own learning experiences; we regularly\ntransfer skills learned in the past to more quickly learn new skills. For\nexample, someone who has learned to throw a baseball does not need to\ncompletely re-learn the mechanics of throwing a projectile to learn how to\nthrow a football. These things are inherently related, and the ability to do\none of them well naturally translates into the ability to do the other."),Object(i.b)("p",null,"In the machine learning world, there is perhaps no better example than the\nfield of computer vision over the last five years. It is now exceedingly rare\nto train an image model from scratch. Instead, we start with a pretrained model\nthat already knows how to classify simple objects such as cats and dogs and\numbrellas. Models that learn to classify images do so by\nfirst learning to ",Object(i.b)("a",Object(a.a)({parentName:"p"},{href:"https://arxiv.org/abs/1311.2901"}),"detect general image\nfeatures"),", such as edges, shapes, text, and\nfaces. The pretrained model has these fundamental skills (as well as more\nspecific skills, such as distinguishing between dogs and cats)."),Object(i.b)("p",null,"The pretrained classification model can then be ",Object(i.b)("em",{parentName:"p"},"slightly")," modified by adding\nlayers or retraining on a new dataset, to carry over those expensively acquired\nfundamental skills to a new specialization. This is transfer learning."),Object(i.b)("p",null,"The benefits of this approach are far-reaching."),Object(i.b)("h3",null,"Transfer learning needs less training data"),Object(i.b)("p",null,'When you re-use your favorite cat detection model in a new, cat-related task,\nyour model already has "the wisdom of one million cats," which means that you\ndon’t need to use nearly as many cat pictures to train the new task. Reducing\nthe size of training data can enable you to train in settings where there is\nvery little data available and where more data may be expensive or impossible to\nobtain, and can also allow you to train models faster on cheaper hardware.'),Object(i.b)("h3",null,"Models learned by transfer learning generalize better"),Object(i.b)("p",null,"Transfer learning improves generalization, or the ability of the model to\nperform well on data that it wasn't trained on. This is because pre-trained\nmodels are purposefully trained on tasks that force the model to learn generic\nfeatures that are useful in related contexts. When the model is transferred to\na new task, it will be difficult to overfit to the new training data, since the\nmodel will only learn incrementally from a very general knowledge base.\nBuilding a model that generalizes well is one of the ",Object(i.b)("a",Object(a.a)({parentName:"p"},{href:"https://twitter.com/mat_kelcey/status/1037812291092111360"}),"hardest and most\nimportant parts of machine\nlearning"),"."),Object(i.b)("h3",null,"The transfer learning training process is less brittle"),Object(i.b)("p",null,"Starting with a pre-trained model also helps overcome the frustrating, brittle,\nand confusing process of training a complex model with millions of parameters.\nTransfer learning reduces the number of trainable parameters by as much as\n100%, making training more stable and easier to debug."),Object(i.b)("h3",null,"Transfer learning makes deep learning easier"),Object(i.b)("p",null,"Finally, transfer learning helps make deep learning more accessible, since you\ndon’t need to be an expert yourself to obtain expert level results. Consider\nthe popular image classification model\n",Object(i.b)("a",Object(a.a)({parentName:"p"},{href:"https://arxiv.org/abs/1512.03385"}),"Resnet-50"),"."),Object(i.b)("p",null,Object(i.b)("img",Object(a.a)({parentName:"p"},{src:"/images/editor_uploads/2018-09-18-181622-resnet50.png",alt:null}))),Object(i.b)("p",null,"How was that particular architecture chosen? It is the result of years of\nresearch and experimentation from various deep learning experts. Within this\ncomplicated structure there are 25 million weights, and optimizing these\nweights from scratch can be near impossible without extensive knowledge of each\nof the model’s components. Fortunately, with transfer learning you can re-use\nboth the complicated structure and optimized weights, significantly lowering\nthe barrier to entry for deep learning."),Object(i.b)("h2",null,"What about multi-task learning?"),Object(i.b)("p",null,"Transfer learning is one of a family of knowledge sharing techniques for\ntraining machine learning models that has proven to be extremely effective.\nCurrently, the two most interesting of these techniques are transfer and\nmulti-task learning. In transfer learning, a model is trained on a single task\nand then used as a starting point for a related task. In learning the related\ntask, the original transferred model will learn to specialize in the new task,\nwithout concern of how that affects its performance on the original task. In\nmulti-task learning, a single model learns to do multiple tasks at once, and\nthe evaluation of its performance depends on how well it learns to do all\nthose tasks. For a more detailed discussion on the benefits of multi-task\nlearning and when it may be useful, see our latest ",Object(i.b)("a",Object(a.a)({parentName:"p"},{href:"https://blog.fastforwardlabs.com/2018/07/24/ff08-launch.html"}),"research on multi-task\nlearning"),"."),Object(i.b)("h2",null,"Conclusion"),Object(i.b)("p",null,"Transfer learning is a knowledge-sharing technique that reduces the amount of\ntraining data, computing power, and engineering talent needed to build deep\nlearning models. And since deep learning can provide significant improvements\nover its counterparts from traditional machine learning, transfer learning is\nan essential tool."))}c.isMDXComponent=!0},"U+05":function(e,n,t){(window.__NEXT_P=window.__NEXT_P||[]).push(["/posts/2018-09-17-deep-learning-is-easy-an-introduction-to-transfer-learning",function(){var e=t("/YMA");return{page:e.default||e}}])}},[["U+05","5d41","9da1"]]]);