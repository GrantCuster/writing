(window.webpackJsonp=window.webpackJsonp||[]).push([["cd47"],{PgHM:function(e,t,a){(window.__NEXT_P=window.__NEXT_P||[]).push(["/posts/2018-07-31-progress-in-machine-learning-interpretability",function(){var e=a("bx7N");return{page:e.default||e}}])},bx7N:function(e,t,a){"use strict";a.r(t),a.d(t,"frontMatter",function(){return s}),a.d(t,"default",function(){return c});var n=a("kOwS"),o=a("qNsG"),i=(a("q1tI"),a("E/Ix")),s={title:"Progress in machine learning interpretability",date:"2018-07-31 12:08 -0400",preview_image:"/images/2018/07/lime-1530894622923.png",feature:!1,published:!0,post_type:"Newsletter"},r={frontMatter:s},l="wrapper";function c(e){var t=e.components,a=Object(o.a)(e,["components"]);return Object(i.b)(l,Object(n.a)({},r,a,{components:t,mdxType:"MDXLayout"}),Object(i.b)("p",null,"Our goal when we do research is to address capabilities and technologies that\nwe expect to become production-ready in one to two years. That focus on\nfast-moving areas means that new algorithmic ideas sometimes come along that\nallow our clients to extend or improve upon the work in our reports."),Object(i.b)("p",null,"We published our report on machine learning interpretability last year. The technical focus of our report was LIME, a tool that computes locally\ncorrect explanations of a model's behaviour. If a model is good, LIME's\nexplanations can offer completely new insights. (We saw this in our prototype,\nwhich models customer churn using traditional machine learning techniques, but\nthen uses LIME to say precisely what it is about a customer that makes them a\nchurn risk.) And if a model is bad, LIME can help you understand why."),Object(i.b)("p",null,"This all sounds great, but we had to leave three issues unresolved in our\nreport. Progress since last year has begun to address those concerns."),Object(i.b)("p",null,Object(i.b)("img",Object(n.a)({parentName:"p"},{src:"/images/2018/07/lime-1530894622923.png",alt:null}))),Object(i.b)("h5",null,'LIME explanations of sentiment classification. "Not" is a positive word in one example, but not in another. Image credit: ',Object(i.b)("a",Object(n.a)({parentName:"h5"},{href:"https://homes.cs.washington.edu/~marcotcr/aaai18.pdf"}),"Anchors"),"."),Object(i.b)("p",null,'Firstly, LIME\'s explanations are local. For example, a LIME explanation may\n(correctly) tell you that "This movie is not bad" has positive sentiment\nbecause it contains the word "not." But because LIME\'s explanations are local,\na user is not generally entitled to conclude from this that the word "not"\nalways indicates positive sentiment. This makes sense: the presence of "not" in\n"this movie is not very good" does not tell you its sentiment is positive! But\nhow local is "local"? How similar to the original sentence does a new sentence\nneed to be for LIME\'s explanation to apply?'),Object(i.b)("p",null,Object(i.b)("img",Object(n.a)({parentName:"p"},{src:"/images/2018/07/anchor-1530894675267.png",alt:null}))),Object(i.b)("h5",null,'Anchors explanations of sentiment classification. "Not" is a positive word in combination with "bad." Image credit: ',Object(i.b)("a",Object(n.a)({parentName:"h5"},{href:"https://homes.cs.washington.edu/~marcotcr/aaai18.pdf"}),"Anchors"),"."),Object(i.b)("p",null,"The creators of LIME offer an answer to this question in the form of ",Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"https://homes.cs.washington.edu/~marcotcr/aaai18.pdf"}),Object(i.b)("em",{parentName:"a"},"Anchors:\nHigh-Precision Model-Agnostic Explanations"),"(PDF,\n2.7MB)"),'." Anchors works\nlike LIME in that it probes the behaviour of the black-box model by perturbing\nthe original example. But it takes a very different approach to constructing a\nhuman-friendly explanation. Rather than fit a locally correct linear model\n(which raises the question: how local?), it constructs a set of rules. For the\n"this movie is not bad" example above, the rule might be "sentence contains\n\'not\' ',Object(i.b)("em",{parentName:"p"},"and"),' \'bad\'". Such black and white rules are easier for many people to\nunderstand than quantitative weights. And they implicitly define locality: if\nthe sentence doesn\'t contain "not" or "bad," the rule (and the explanation)\ndoesn\'t apply. The ',Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"https://github.com/marcotcr/anchor"}),"Anchors code is publicly\navailable"),"."),Object(i.b)("p",null,Object(i.b)("img",Object(n.a)({parentName:"p"},{src:"https://raw.githubusercontent.com/slundberg/shap/master/docs/artwork/boston_instance.png",alt:null}))),Object(i.b)("h5",null,Object(i.b)("a",Object(n.a)({parentName:"h5"},{href:"https://github.com/slundberg/shap"}),"SHAP")," explanation of a prediction for a model of the Boston house price dataset."),Object(i.b)("p",null,"Secondly, LIME's choice of perturbation strategy and its local linear model are\nheuristics -- which is to say they feel a little arbitrary, and it's reasonable\nto wonder whether they are optimal in practice. In ",Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf"}),"A Unified Approach to\nInterpreting Model\nPredictions"),'\nLundberg and Lee carefully define what we mean by optimal, and show that LIME\nis a specific example of a more general class of explanation tools they call\n"additive feature attribution methods." This class includes the classical\n"Shapley" feature importance measure familiar to economists, and\n',Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"https://github.com/kundajelab/deeplift"}),"DeepLIFT"),", a neural network\ninterpretability tool. They unify this class in a provably optimal way they\ncall ",Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"https://github.com/slundberg/shap"}),"SHAP"),". The code is public, and is\n",Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"https://arxiv.org/abs/1802.03888"}),"highly optimized")," for the particular case of\ntree-based methods such as XGboost. One thing we really like about SHAP is that\nthe built-in visualization tools are very nice! This seemingly minor point is\nsurprisingly important to the adoption of new tools, and we're glad to see\nthese authors spend time on this aspect of their code."),Object(i.b)("p",null,"Finally, how do we ",Object(i.b)("em",{parentName:"p"},"test")," explanations? How do we know whether an explanation\nis evidence of a problem with the model or a surprising insight? Patrick Hall\nand colleagues at ",Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"https://www.h2o.ai/"}),"H2O.ai")," sum up the current situation very well in a new\narticle for O'Reilly ",Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"https://www.oreilly.com/ideas/testing-machine-learning-interpretability-techniques"}),"Testing machine learning interpretability\ntechniques"),'.\nThe conclusion is: "use more than one type of tool to explain your machine\nlearning models, and look for consistent results across different explanatory\nmethods." We agree, and we\'re glad to see new options such as Anchors and SHAP\nthat make this easy!'),Object(i.b)("p",null,"So, a year after our report, machine learning interpretability remains not only\na very useful business capability, but a vibrant area of research.\n"))}c.isMDXComponent=!0}},[["PgHM","5d41","9da1"]]]);