(window.webpackJsonp=window.webpackJsonp||[]).push([["b241"],{x9EI:function(e,a,t){"use strict";t.r(a),t.d(a,"frontMatter",function(){return s}),t.d(a,"default",function(){return p});var n=t("kOwS"),r=t("qNsG"),i=(t("q1tI"),t("E/Ix")),s={title:"Learning to learn in a model-agnostic way",date:"2018-10-29 11:11 -0400",preview_image:"/images/2018/10/meta_learning-1538505489954.png",feature:!1,published:!0,author:"Nisha",author_link:"https://twitter.com/NishaMuktewar",post_type:"newsletter"},o={frontMatter:s},l="wrapper";function p(e){var a=e.components,t=Object(r.a)(e,["components"]);return Object(i.b)(l,Object(n.a)({},o,t,{components:a,mdxType:"MDXLayout"}),Object(i.b)("p",null,"As humans, we can quickly adapt our actions in new situations, be it recognizing objects from a few examples, or learning new skills and\napplying them in a matter of just a few minutes. But when it comes to deep learning techniques, an\nunderstandably large amount of time and data is required. So the challenge is to help our deep models do the same thing we can - to learn and quickly\nadapt from only a few examples, and to continue to adapt as more data becomes available. This approach of learning to learn is\ncalled ",Object(i.b)("strong",{parentName:"p"},"meta-learning"),", and being a hot topic, has seen a flurry of research papers using techniques like\n",Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"https://arxiv.org/abs/1606.04080"}),"matching networks"),", ",Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"https://arxiv.org/abs/1605.06065"}),"memory-augmented networks"),",\n",Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"https://arxiv.org/abs/1603.05106"}),"sequence generative models"),", ",Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"https://arxiv.org/abs/1611.05763"}),"fast reinforcement learning"),"\nand many others. "),Object(i.b)("p",null,"Meta-learning methods differ from many standard machine learning techniques, which involve training on a\nsingle task and testing on held-out examples from that task. These systems are trained by exposing them to a large number of\ntasks and are then tested in their ability to learn new tasks; an example of a task might be classifying a new image within 5\npossible classes, given one example of each class."),Object(i.b)("p",null,Object(i.b)("img",Object(n.a)({parentName:"p"},{src:"/images/2018/10/meta_learning-1538505489954.png",alt:null}))),Object(i.b)("h5",null,"A meta-learning set-up for few-shot image classification from a paper on ",Object(i.b)("a",Object(n.a)({parentName:"h5"},{href:"https://openreview.net/forum?id=rJY0-Kcll"}),"Optimization as a Model for Few-Shot Learning")),Object(i.b)("p",null,"During this process, the model is trained to learn tasks in the meta-training set. There are two optimizations at play – the\nlearner, which learns new tasks, and the meta-learner, which trains the learner. "),Object(i.b)("p",null,"One of the recent benchmark papers in this area is ",Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"https://arxiv.org/abs/1703.03400"}),"MAML: Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks"),".\nLike other meta-learning methods, MAML trains over a wide range of tasks. It trains for a representation that can be quickly\nadapted to a new task, via a few gradient steps. The meta-learner seeks to find an initialization that is not only useful for\nadapting to various problems, but also can be adapted quickly (in a small number of steps) and efficiently (using only a few\nexamples). Suppose we are seeking to find a set of parameters θ that are highly adaptable. During the course of meta-learning (the bold line), MAML optimizes for a set of parameters such that when a gradient step is taken with respect to a particular task i (the gray lines), the parameters are close to the optimal parameters θ",Object(i.b)("sup",null,"∗"),Object(i.b)("sub",null,"i")," for task i."),Object(i.b)("p",null,Object(i.b)("img",Object(n.a)({parentName:"p"},{src:"/images/2018/10/MAML-1538505613205.png",alt:null}))),Object(i.b)("h5",null,Object(i.b)("a",Object(n.a)({parentName:"h5"},{href:"https://arxiv.org/pdf/1703.03400.pdf"}),"(image source)")),Object(i.b)("p",null,"This approach is quite simple and has some distinctive advantages:   "),Object(i.b)("ul",null,Object(i.b)("li",{parentName:"ul"},"It does not make any assumptions on the form of the model, in the sense that it can be applied to any learning problem and\nmodel trained with gradient descent procedure."),Object(i.b)("li",{parentName:"ul"},"It is ",Object(i.b)("strong",{parentName:"li"},"parameter efficient")," - there are no additional parameters introduced for meta-learning and the learner’s strategy uses a known optimization process (gradient descent), rather than having to come up with one from scratch."),Object(i.b)("li",{parentName:"ul"},"And, unlike other approaches, it can be readily applied to classification, regression  and reinforcement learning tasks.")))}p.isMDXComponent=!0},y86L:function(e,a,t){(window.__NEXT_P=window.__NEXT_P||[]).push(["/posts/2018-10-29-learning-to-learn-in-a-modelagnostic-way",function(){var e=t("x9EI");return{page:e.default||e}}])}},[["y86L","5d41","9da1"]]]);